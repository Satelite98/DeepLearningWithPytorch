{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7d0d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "#这个是一组输入温度计温度和表测值之间的数据，想找到表测值和实际温度的函数关系\n",
    "#注意： 先假设线性函数关系 t_c = w*t_u + b\n",
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0] \n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] \n",
    "t_c = torch.tensor(t_c) \n",
    "t_u = torch.tensor(t_u) \n",
    "print(t_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f787ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
      "        48.4000, 60.4000, 68.4000])\n",
      "tensor(1763.8848)\n"
     ]
    }
   ],
   "source": [
    "# 1. 定义模型\n",
    "def model(t_u,w,b):\n",
    "    return w *t_u + b\n",
    "\n",
    "#2. 定义损失函数，这里调用mean 是为了计算出来保证是标量\n",
    "def loss_fn(t_p,t_c):\n",
    "    squared_diffs  = (t_p - t_c )**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "t_p = model(t_u, w, b) \n",
    "print(t_p)\n",
    "loss = loss_fn(t_p,t_c)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b5dfdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-44.1730) tensor(-0.8260)\n"
     ]
    }
   ],
   "source": [
    "#3. 计算损失函数分别对w ,b 的倒数，并且更新w ,b\n",
    "delta = 0.1\n",
    "loss_rate_w = (loss_fn(model(t_u, w+delta, b),t_c) - loss_fn(model(t_u, w-delta, b),t_c))/(2.0 * delta )\n",
    "loss_rate_b = (loss_fn(model(t_u, w, b+delta),t_c) - loss_fn(model(t_u, w, b-delta),t_c))/(2.0 * delta )\n",
    "\n",
    "learning_rate = 1e-2\n",
    "w = w - learning_rate*loss_rate_w\n",
    "b = b - learning_rate*loss_rate_b\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b518b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. 采用链式法则来计算倒数 dloss/dw = dloss/t_p* t_p/dw\n",
    "def dloss_fn(t_p, t_c):\n",
    "    dsq_diff = 2 * (t_p - t_c )/t_p.size(0)\n",
    "    #print(\"t_p.size(0) :\",t_p.size(0)) -> 打印出来这个值是11 \n",
    "    return dsq_diff\n",
    "#   t_c = w*t_u + b\n",
    "def dmodel_dw(t_u, w, b): \n",
    "    return t_u \n",
    "\n",
    "def dmodel_db(t_u, w, b): \n",
    "    return 1.0 \n",
    "\n",
    "def grad_fn(t_u, t_c, t_p, w, b): \n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3cd4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 定义一次训练的流程\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c): \n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        w, b = params \n",
    "        t_p = model(t_u, w, b) #正向传播\n",
    "        loss  = loss_fn(t_p,t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)# 反向梯度计算\n",
    "        \n",
    "        params = params - learning_rate*grad\n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss))) \n",
    "        print(\"  params : \" ,params)\n",
    "        print(\"  grad : \" ,grad)\n",
    "\n",
    "    return params \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87922e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "  params :  tensor([ 0.5483, -0.0083])\n",
      "  grad :  tensor([4517.2964,   82.6000])\n",
      "Epoch 2, Loss 323.090515\n",
      "  params :  tensor([ 0.3623, -0.0118])\n",
      "  grad :  tensor([1859.5493,   35.7843])\n",
      "Epoch 3, Loss 78.929634\n",
      "  params :  tensor([ 0.2858, -0.0135])\n",
      "  grad :  tensor([765.4666,  16.5122])\n",
      "Epoch 4, Loss 37.552845\n",
      "  params :  tensor([ 0.2543, -0.0143])\n",
      "  grad :  tensor([315.0790,   8.5787])\n",
      "Epoch 5, Loss 30.540283\n",
      "  params :  tensor([ 0.2413, -0.0149])\n",
      "  grad :  tensor([129.6733,   5.3127])\n",
      "Epoch 6, Loss 29.351154\n",
      "  params :  tensor([ 0.2360, -0.0153])\n",
      "  grad :  tensor([53.3495,  3.9682])\n",
      "Epoch 7, Loss 29.148884\n",
      "  params :  tensor([ 0.2338, -0.0156])\n",
      "  grad :  tensor([21.9304,  3.4148])\n",
      "Epoch 8, Loss 29.113848\n",
      "  params :  tensor([ 0.2329, -0.0159])\n",
      "  grad :  tensor([8.9964, 3.1869])\n",
      "Epoch 9, Loss 29.107145\n",
      "  params :  tensor([ 0.2325, -0.0162])\n",
      "  grad :  tensor([3.6721, 3.0930])\n",
      "Epoch 10, Loss 29.105247\n",
      "  params :  tensor([ 0.2324, -0.0166])\n",
      "  grad :  tensor([1.4803, 3.0544])\n",
      "Epoch 11, Loss 29.104168\n",
      "  params :  tensor([ 0.2323, -0.0169])\n",
      "  grad :  tensor([0.5781, 3.0384])\n",
      "Epoch 12, Loss 29.103222\n",
      "  params :  tensor([ 0.2323, -0.0172])\n",
      "  grad :  tensor([0.2066, 3.0318])\n",
      "Epoch 13, Loss 29.102295\n",
      "  params :  tensor([ 0.2323, -0.0175])\n",
      "  grad :  tensor([0.0537, 3.0291])\n",
      "Epoch 14, Loss 29.101379\n",
      "  params :  tensor([ 0.2323, -0.0178])\n",
      "  grad :  tensor([-0.0093,  3.0279])\n",
      "Epoch 15, Loss 29.100466\n",
      "  params :  tensor([ 0.2323, -0.0181])\n",
      "  grad :  tensor([-0.0353,  3.0274])\n",
      "Epoch 16, Loss 29.099548\n",
      "  params :  tensor([ 0.2323, -0.0184])\n",
      "  grad :  tensor([-0.0459,  3.0272])\n",
      "Epoch 17, Loss 29.098631\n",
      "  params :  tensor([ 0.2323, -0.0187])\n",
      "  grad :  tensor([-0.0502,  3.0270])\n",
      "Epoch 18, Loss 29.097717\n",
      "  params :  tensor([ 0.2323, -0.0190])\n",
      "  grad :  tensor([-0.0520,  3.0270])\n",
      "Epoch 19, Loss 29.096796\n",
      "  params :  tensor([ 0.2323, -0.0193])\n",
      "  grad :  tensor([-0.0528,  3.0269])\n",
      "Epoch 20, Loss 29.095881\n",
      "  params :  tensor([ 0.2323, -0.0196])\n",
      "  grad :  tensor([-0.0531,  3.0268])\n",
      "Epoch 21, Loss 29.094959\n",
      "  params :  tensor([ 0.2323, -0.0199])\n",
      "  grad :  tensor([-0.0533,  3.0268])\n",
      "Epoch 22, Loss 29.094049\n",
      "  params :  tensor([ 0.2323, -0.0202])\n",
      "  grad :  tensor([-0.0533,  3.0267])\n",
      "Epoch 23, Loss 29.093134\n",
      "  params :  tensor([ 0.2323, -0.0205])\n",
      "  grad :  tensor([-0.0533,  3.0267])\n",
      "Epoch 24, Loss 29.092216\n",
      "  params :  tensor([ 0.2323, -0.0208])\n",
      "  grad :  tensor([-0.0533,  3.0266])\n",
      "Epoch 25, Loss 29.091301\n",
      "  params :  tensor([ 0.2323, -0.0211])\n",
      "  grad :  tensor([-0.0533,  3.0266])\n",
      "Epoch 26, Loss 29.090385\n",
      "  params :  tensor([ 0.2323, -0.0214])\n",
      "  grad :  tensor([-0.0533,  3.0265])\n",
      "Epoch 27, Loss 29.089464\n",
      "  params :  tensor([ 0.2323, -0.0217])\n",
      "  grad :  tensor([-0.0533,  3.0265])\n",
      "Epoch 28, Loss 29.088551\n",
      "  params :  tensor([ 0.2323, -0.0220])\n",
      "  grad :  tensor([-0.0532,  3.0264])\n",
      "Epoch 29, Loss 29.087635\n",
      "  params :  tensor([ 0.2323, -0.0223])\n",
      "  grad :  tensor([-0.0533,  3.0264])\n",
      "Epoch 30, Loss 29.086714\n",
      "  params :  tensor([ 0.2323, -0.0226])\n",
      "  grad :  tensor([-0.0533,  3.0263])\n",
      "Epoch 31, Loss 29.085804\n",
      "  params :  tensor([ 0.2324, -0.0229])\n",
      "  grad :  tensor([-0.0532,  3.0262])\n",
      "Epoch 32, Loss 29.084888\n",
      "  params :  tensor([ 0.2324, -0.0232])\n",
      "  grad :  tensor([-0.0533,  3.0262])\n",
      "Epoch 33, Loss 29.083967\n",
      "  params :  tensor([ 0.2324, -0.0235])\n",
      "  grad :  tensor([-0.0533,  3.0261])\n",
      "Epoch 34, Loss 29.083057\n",
      "  params :  tensor([ 0.2324, -0.0238])\n",
      "  grad :  tensor([-0.0533,  3.0261])\n",
      "Epoch 35, Loss 29.082142\n",
      "  params :  tensor([ 0.2324, -0.0241])\n",
      "  grad :  tensor([-0.0532,  3.0260])\n",
      "Epoch 36, Loss 29.081221\n",
      "  params :  tensor([ 0.2324, -0.0244])\n",
      "  grad :  tensor([-0.0533,  3.0260])\n",
      "Epoch 37, Loss 29.080309\n",
      "  params :  tensor([ 0.2324, -0.0247])\n",
      "  grad :  tensor([-0.0533,  3.0259])\n",
      "Epoch 38, Loss 29.079390\n",
      "  params :  tensor([ 0.2324, -0.0250])\n",
      "  grad :  tensor([-0.0532,  3.0259])\n",
      "Epoch 39, Loss 29.078474\n",
      "  params :  tensor([ 0.2324, -0.0253])\n",
      "  grad :  tensor([-0.0533,  3.0258])\n",
      "Epoch 40, Loss 29.077562\n",
      "  params :  tensor([ 0.2324, -0.0256])\n",
      "  grad :  tensor([-0.0533,  3.0258])\n",
      "Epoch 41, Loss 29.076649\n",
      "  params :  tensor([ 0.2324, -0.0259])\n",
      "  grad :  tensor([-0.0533,  3.0257])\n",
      "Epoch 42, Loss 29.075731\n",
      "  params :  tensor([ 0.2324, -0.0262])\n",
      "  grad :  tensor([-0.0532,  3.0257])\n",
      "Epoch 43, Loss 29.074812\n",
      "  params :  tensor([ 0.2324, -0.0265])\n",
      "  grad :  tensor([-0.0533,  3.0256])\n",
      "Epoch 44, Loss 29.073895\n",
      "  params :  tensor([ 0.2324, -0.0268])\n",
      "  grad :  tensor([-0.0533,  3.0256])\n",
      "Epoch 45, Loss 29.072981\n",
      "  params :  tensor([ 0.2324, -0.0271])\n",
      "  grad :  tensor([-0.0533,  3.0255])\n",
      "Epoch 46, Loss 29.072069\n",
      "  params :  tensor([ 0.2324, -0.0274])\n",
      "  grad :  tensor([-0.0533,  3.0254])\n",
      "Epoch 47, Loss 29.071148\n",
      "  params :  tensor([ 0.2324, -0.0277])\n",
      "  grad :  tensor([-0.0533,  3.0254])\n",
      "Epoch 48, Loss 29.070234\n",
      "  params :  tensor([ 0.2324, -0.0281])\n",
      "  grad :  tensor([-0.0533,  3.0253])\n",
      "Epoch 49, Loss 29.069323\n",
      "  params :  tensor([ 0.2325, -0.0284])\n",
      "  grad :  tensor([-0.0533,  3.0253])\n",
      "Epoch 50, Loss 29.068401\n",
      "  params :  tensor([ 0.2325, -0.0287])\n",
      "  grad :  tensor([-0.0532,  3.0252])\n",
      "Epoch 51, Loss 29.067486\n",
      "  params :  tensor([ 0.2325, -0.0290])\n",
      "  grad :  tensor([-0.0533,  3.0252])\n",
      "Epoch 52, Loss 29.066566\n",
      "  params :  tensor([ 0.2325, -0.0293])\n",
      "  grad :  tensor([-0.0533,  3.0251])\n",
      "Epoch 53, Loss 29.065657\n",
      "  params :  tensor([ 0.2325, -0.0296])\n",
      "  grad :  tensor([-0.0533,  3.0251])\n",
      "Epoch 54, Loss 29.064741\n",
      "  params :  tensor([ 0.2325, -0.0299])\n",
      "  grad :  tensor([-0.0533,  3.0250])\n",
      "Epoch 55, Loss 29.063826\n",
      "  params :  tensor([ 0.2325, -0.0302])\n",
      "  grad :  tensor([-0.0532,  3.0250])\n",
      "Epoch 56, Loss 29.062910\n",
      "  params :  tensor([ 0.2325, -0.0305])\n",
      "  grad :  tensor([-0.0533,  3.0249])\n",
      "Epoch 57, Loss 29.061995\n",
      "  params :  tensor([ 0.2325, -0.0308])\n",
      "  grad :  tensor([-0.0532,  3.0249])\n",
      "Epoch 58, Loss 29.061079\n",
      "  params :  tensor([ 0.2325, -0.0311])\n",
      "  grad :  tensor([-0.0533,  3.0248])\n",
      "Epoch 59, Loss 29.060169\n",
      "  params :  tensor([ 0.2325, -0.0314])\n",
      "  grad :  tensor([-0.0533,  3.0248])\n",
      "Epoch 60, Loss 29.059248\n",
      "  params :  tensor([ 0.2325, -0.0317])\n",
      "  grad :  tensor([-0.0533,  3.0247])\n",
      "Epoch 61, Loss 29.058336\n",
      "  params :  tensor([ 0.2325, -0.0320])\n",
      "  grad :  tensor([-0.0533,  3.0247])\n",
      "Epoch 62, Loss 29.057415\n",
      "  params :  tensor([ 0.2325, -0.0323])\n",
      "  grad :  tensor([-0.0534,  3.0246])\n",
      "Epoch 63, Loss 29.056507\n",
      "  params :  tensor([ 0.2325, -0.0326])\n",
      "  grad :  tensor([-0.0533,  3.0245])\n",
      "Epoch 64, Loss 29.055586\n",
      "  params :  tensor([ 0.2325, -0.0329])\n",
      "  grad :  tensor([-0.0532,  3.0245])\n",
      "Epoch 65, Loss 29.054674\n",
      "  params :  tensor([ 0.2325, -0.0332])\n",
      "  grad :  tensor([-0.0533,  3.0244])\n",
      "Epoch 66, Loss 29.053761\n",
      "  params :  tensor([ 0.2325, -0.0335])\n",
      "  grad :  tensor([-0.0533,  3.0244])\n",
      "Epoch 67, Loss 29.052843\n",
      "  params :  tensor([ 0.2325, -0.0338])\n",
      "  grad :  tensor([-0.0533,  3.0243])\n",
      "Epoch 68, Loss 29.051929\n",
      "  params :  tensor([ 0.2326, -0.0341])\n",
      "  grad :  tensor([-0.0532,  3.0243])\n",
      "Epoch 69, Loss 29.051012\n",
      "  params :  tensor([ 0.2326, -0.0344])\n",
      "  grad :  tensor([-0.0533,  3.0242])\n",
      "Epoch 70, Loss 29.050098\n",
      "  params :  tensor([ 0.2326, -0.0347])\n",
      "  grad :  tensor([-0.0532,  3.0242])\n",
      "Epoch 71, Loss 29.049183\n",
      "  params :  tensor([ 0.2326, -0.0350])\n",
      "  grad :  tensor([-0.0533,  3.0241])\n",
      "Epoch 72, Loss 29.048273\n",
      "  params :  tensor([ 0.2326, -0.0353])\n",
      "  grad :  tensor([-0.0533,  3.0241])\n",
      "Epoch 73, Loss 29.047350\n",
      "  params :  tensor([ 0.2326, -0.0356])\n",
      "  grad :  tensor([-0.0532,  3.0240])\n",
      "Epoch 74, Loss 29.046442\n",
      "  params :  tensor([ 0.2326, -0.0359])\n",
      "  grad :  tensor([-0.0533,  3.0240])\n",
      "Epoch 75, Loss 29.045530\n",
      "  params :  tensor([ 0.2326, -0.0362])\n",
      "  grad :  tensor([-0.0532,  3.0239])\n",
      "Epoch 76, Loss 29.044611\n",
      "  params :  tensor([ 0.2326, -0.0365])\n",
      "  grad :  tensor([-0.0533,  3.0239])\n",
      "Epoch 77, Loss 29.043699\n",
      "  params :  tensor([ 0.2326, -0.0368])\n",
      "  grad :  tensor([-0.0533,  3.0238])\n",
      "Epoch 78, Loss 29.042784\n",
      "  params :  tensor([ 0.2326, -0.0371])\n",
      "  grad :  tensor([-0.0533,  3.0238])\n",
      "Epoch 79, Loss 29.041870\n",
      "  params :  tensor([ 0.2326, -0.0374])\n",
      "  grad :  tensor([-0.0533,  3.0237])\n",
      "Epoch 80, Loss 29.040955\n",
      "  params :  tensor([ 0.2326, -0.0377])\n",
      "  grad :  tensor([-0.0532,  3.0236])\n",
      "Epoch 81, Loss 29.040039\n",
      "  params :  tensor([ 0.2326, -0.0380])\n",
      "  grad :  tensor([-0.0534,  3.0236])\n",
      "Epoch 82, Loss 29.039122\n",
      "  params :  tensor([ 0.2326, -0.0383])\n",
      "  grad :  tensor([-0.0533,  3.0235])\n",
      "Epoch 83, Loss 29.038210\n",
      "  params :  tensor([ 0.2326, -0.0386])\n",
      "  grad :  tensor([-0.0532,  3.0235])\n",
      "Epoch 84, Loss 29.037294\n",
      "  params :  tensor([ 0.2326, -0.0389])\n",
      "  grad :  tensor([-0.0533,  3.0234])\n",
      "Epoch 85, Loss 29.036379\n",
      "  params :  tensor([ 0.2326, -0.0392])\n",
      "  grad :  tensor([-0.0533,  3.0234])\n",
      "Epoch 86, Loss 29.035463\n",
      "  params :  tensor([ 0.2326, -0.0395])\n",
      "  grad :  tensor([-0.0532,  3.0233])\n",
      "Epoch 87, Loss 29.034554\n",
      "  params :  tensor([ 0.2327, -0.0398])\n",
      "  grad :  tensor([-0.0533,  3.0233])\n",
      "Epoch 88, Loss 29.033636\n",
      "  params :  tensor([ 0.2327, -0.0401])\n",
      "  grad :  tensor([-0.0532,  3.0232])\n",
      "Epoch 89, Loss 29.032722\n",
      "  params :  tensor([ 0.2327, -0.0405])\n",
      "  grad :  tensor([-0.0533,  3.0232])\n",
      "Epoch 90, Loss 29.031811\n",
      "  params :  tensor([ 0.2327, -0.0408])\n",
      "  grad :  tensor([-0.0533,  3.0231])\n",
      "Epoch 91, Loss 29.030895\n",
      "  params :  tensor([ 0.2327, -0.0411])\n",
      "  grad :  tensor([-0.0532,  3.0231])\n",
      "Epoch 92, Loss 29.029976\n",
      "  params :  tensor([ 0.2327, -0.0414])\n",
      "  grad :  tensor([-0.0532,  3.0230])\n",
      "Epoch 93, Loss 29.029066\n",
      "  params :  tensor([ 0.2327, -0.0417])\n",
      "  grad :  tensor([-0.0533,  3.0230])\n",
      "Epoch 94, Loss 29.028151\n",
      "  params :  tensor([ 0.2327, -0.0420])\n",
      "  grad :  tensor([-0.0532,  3.0229])\n",
      "Epoch 95, Loss 29.027235\n",
      "  params :  tensor([ 0.2327, -0.0423])\n",
      "  grad :  tensor([-0.0533,  3.0229])\n",
      "Epoch 96, Loss 29.026323\n",
      "  params :  tensor([ 0.2327, -0.0426])\n",
      "  grad :  tensor([-0.0533,  3.0228])\n",
      "Epoch 97, Loss 29.025410\n",
      "  params :  tensor([ 0.2327, -0.0429])\n",
      "  grad :  tensor([-0.0532,  3.0227])\n",
      "Epoch 98, Loss 29.024492\n",
      "  params :  tensor([ 0.2327, -0.0432])\n",
      "  grad :  tensor([-0.0532,  3.0227])\n",
      "Epoch 99, Loss 29.023582\n",
      "  params :  tensor([ 0.2327, -0.0435])\n",
      "  grad :  tensor([-0.0533,  3.0226])\n",
      "Epoch 100, Loss 29.022667\n",
      "  params :  tensor([ 0.2327, -0.0438])\n",
      "  grad :  tensor([-0.0532,  3.0226])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 开始训练。 \n",
    "    \n",
    "training_loop( \n",
    "    n_epochs = 100, \n",
    "    learning_rate = 1e-4, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_u, \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d38f46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "  params :  tensor([1.7761, 0.1064])\n",
      "  grad :  tensor([-77.6140, -10.6400])\n",
      "Epoch 2, Loss 37.574913\n",
      "  params :  tensor([2.0848, 0.1303])\n",
      "  grad :  tensor([-30.8623,  -2.3864])\n",
      "Epoch 3, Loss 30.871077\n",
      "  params :  tensor([2.2094, 0.1217])\n",
      "  grad :  tensor([-12.4631,   0.8587])\n",
      "Epoch 4, Loss 29.756193\n",
      "  params :  tensor([2.2616, 0.1004])\n",
      "  grad :  tensor([-5.2218,  2.1327])\n",
      "Epoch 5, Loss 29.507153\n",
      "  params :  tensor([2.2853, 0.0740])\n",
      "  grad :  tensor([-2.3715,  2.6310])\n",
      "Epoch 6, Loss 29.392456\n",
      "  params :  tensor([2.2978, 0.0458])\n",
      "  grad :  tensor([-1.2492,  2.8241])\n",
      "Epoch 7, Loss 29.298828\n",
      "  params :  tensor([2.3059, 0.0168])\n",
      "  grad :  tensor([-0.8071,  2.8970])\n",
      "Epoch 8, Loss 29.208717\n",
      "  params :  tensor([ 2.3122, -0.0124])\n",
      "  grad :  tensor([-0.6325,  2.9227])\n",
      "Epoch 9, Loss 29.119415\n",
      "  params :  tensor([ 2.3178, -0.0417])\n",
      "  grad :  tensor([-0.5633,  2.9298])\n",
      "Epoch 10, Loss 29.030489\n",
      "  params :  tensor([ 2.3232, -0.0710])\n",
      "  grad :  tensor([-0.5355,  2.9295])\n",
      "Epoch 11, Loss 28.941877\n",
      "  params :  tensor([ 2.3284, -0.1003])\n",
      "  grad :  tensor([-0.5240,  2.9264])\n",
      "Epoch 12, Loss 28.853565\n",
      "  params :  tensor([ 2.3336, -0.1295])\n",
      "  grad :  tensor([-0.5190,  2.9222])\n",
      "Epoch 13, Loss 28.765553\n",
      "  params :  tensor([ 2.3388, -0.1587])\n",
      "  grad :  tensor([-0.5165,  2.9175])\n",
      "Epoch 14, Loss 28.677851\n",
      "  params :  tensor([ 2.3439, -0.1878])\n",
      "  grad :  tensor([-0.5150,  2.9126])\n",
      "Epoch 15, Loss 28.590431\n",
      "  params :  tensor([ 2.3491, -0.2169])\n",
      "  grad :  tensor([-0.5138,  2.9077])\n",
      "Epoch 16, Loss 28.503319\n",
      "  params :  tensor([ 2.3542, -0.2459])\n",
      "  grad :  tensor([-0.5129,  2.9028])\n",
      "Epoch 17, Loss 28.416498\n",
      "  params :  tensor([ 2.3593, -0.2749])\n",
      "  grad :  tensor([-0.5120,  2.8979])\n",
      "Epoch 18, Loss 28.329973\n",
      "  params :  tensor([ 2.3644, -0.3038])\n",
      "  grad :  tensor([-0.5111,  2.8930])\n",
      "Epoch 19, Loss 28.243742\n",
      "  params :  tensor([ 2.3695, -0.3327])\n",
      "  grad :  tensor([-0.5102,  2.8881])\n",
      "Epoch 20, Loss 28.157804\n",
      "  params :  tensor([ 2.3746, -0.3615])\n",
      "  grad :  tensor([-0.5093,  2.8832])\n",
      "Epoch 21, Loss 28.072151\n",
      "  params :  tensor([ 2.3797, -0.3903])\n",
      "  grad :  tensor([-0.5084,  2.8783])\n",
      "Epoch 22, Loss 27.986797\n",
      "  params :  tensor([ 2.3848, -0.4190])\n",
      "  grad :  tensor([-0.5076,  2.8734])\n",
      "Epoch 23, Loss 27.901728\n",
      "  params :  tensor([ 2.3899, -0.4477])\n",
      "  grad :  tensor([-0.5067,  2.8685])\n",
      "Epoch 24, Loss 27.816950\n",
      "  params :  tensor([ 2.3949, -0.4763])\n",
      "  grad :  tensor([-0.5059,  2.8636])\n",
      "Epoch 25, Loss 27.732464\n",
      "  params :  tensor([ 2.4000, -0.5049])\n",
      "  grad :  tensor([-0.5050,  2.8588])\n",
      "Epoch 26, Loss 27.648256\n",
      "  params :  tensor([ 2.4050, -0.5335])\n",
      "  grad :  tensor([-0.5042,  2.8539])\n",
      "Epoch 27, Loss 27.564344\n",
      "  params :  tensor([ 2.4101, -0.5620])\n",
      "  grad :  tensor([-0.5033,  2.8490])\n",
      "Epoch 28, Loss 27.480707\n",
      "  params :  tensor([ 2.4151, -0.5904])\n",
      "  grad :  tensor([-0.5024,  2.8442])\n",
      "Epoch 29, Loss 27.397362\n",
      "  params :  tensor([ 2.4201, -0.6188])\n",
      "  grad :  tensor([-0.5016,  2.8394])\n",
      "Epoch 30, Loss 27.314295\n",
      "  params :  tensor([ 2.4251, -0.6471])\n",
      "  grad :  tensor([-0.5007,  2.8346])\n",
      "Epoch 31, Loss 27.231512\n",
      "  params :  tensor([ 2.4301, -0.6754])\n",
      "  grad :  tensor([-0.4999,  2.8297])\n",
      "Epoch 32, Loss 27.149010\n",
      "  params :  tensor([ 2.4351, -0.7037])\n",
      "  grad :  tensor([-0.4990,  2.8249])\n",
      "Epoch 33, Loss 27.066790\n",
      "  params :  tensor([ 2.4401, -0.7319])\n",
      "  grad :  tensor([-0.4982,  2.8201])\n",
      "Epoch 34, Loss 26.984844\n",
      "  params :  tensor([ 2.4450, -0.7600])\n",
      "  grad :  tensor([-0.4973,  2.8153])\n",
      "Epoch 35, Loss 26.903175\n",
      "  params :  tensor([ 2.4500, -0.7881])\n",
      "  grad :  tensor([-0.4965,  2.8106])\n",
      "Epoch 36, Loss 26.821791\n",
      "  params :  tensor([ 2.4550, -0.8162])\n",
      "  grad :  tensor([-0.4957,  2.8058])\n",
      "Epoch 37, Loss 26.740679\n",
      "  params :  tensor([ 2.4599, -0.8442])\n",
      "  grad :  tensor([-0.4948,  2.8010])\n",
      "Epoch 38, Loss 26.659838\n",
      "  params :  tensor([ 2.4649, -0.8722])\n",
      "  grad :  tensor([-0.4940,  2.7963])\n",
      "Epoch 39, Loss 26.579279\n",
      "  params :  tensor([ 2.4698, -0.9001])\n",
      "  grad :  tensor([-0.4931,  2.7915])\n",
      "Epoch 40, Loss 26.498987\n",
      "  params :  tensor([ 2.4747, -0.9280])\n",
      "  grad :  tensor([-0.4923,  2.7868])\n",
      "Epoch 41, Loss 26.418974\n",
      "  params :  tensor([ 2.4796, -0.9558])\n",
      "  grad :  tensor([-0.4915,  2.7820])\n",
      "Epoch 42, Loss 26.339228\n",
      "  params :  tensor([ 2.4845, -0.9836])\n",
      "  grad :  tensor([-0.4906,  2.7773])\n",
      "Epoch 43, Loss 26.259754\n",
      "  params :  tensor([ 2.4894, -1.0113])\n",
      "  grad :  tensor([-0.4898,  2.7726])\n",
      "Epoch 44, Loss 26.180548\n",
      "  params :  tensor([ 2.4943, -1.0390])\n",
      "  grad :  tensor([-0.4890,  2.7679])\n",
      "Epoch 45, Loss 26.101616\n",
      "  params :  tensor([ 2.4992, -1.0666])\n",
      "  grad :  tensor([-0.4881,  2.7632])\n",
      "Epoch 46, Loss 26.022947\n",
      "  params :  tensor([ 2.5041, -1.0942])\n",
      "  grad :  tensor([-0.4873,  2.7585])\n",
      "Epoch 47, Loss 25.944544\n",
      "  params :  tensor([ 2.5089, -1.1217])\n",
      "  grad :  tensor([-0.4865,  2.7538])\n",
      "Epoch 48, Loss 25.866417\n",
      "  params :  tensor([ 2.5138, -1.1492])\n",
      "  grad :  tensor([-0.4856,  2.7491])\n",
      "Epoch 49, Loss 25.788549\n",
      "  params :  tensor([ 2.5186, -1.1766])\n",
      "  grad :  tensor([-0.4848,  2.7444])\n",
      "Epoch 50, Loss 25.710938\n",
      "  params :  tensor([ 2.5235, -1.2040])\n",
      "  grad :  tensor([-0.4840,  2.7398])\n",
      "Epoch 51, Loss 25.633600\n",
      "  params :  tensor([ 2.5283, -1.2314])\n",
      "  grad :  tensor([-0.4832,  2.7351])\n",
      "Epoch 52, Loss 25.556524\n",
      "  params :  tensor([ 2.5331, -1.2587])\n",
      "  grad :  tensor([-0.4823,  2.7305])\n",
      "Epoch 53, Loss 25.479700\n",
      "  params :  tensor([ 2.5379, -1.2860])\n",
      "  grad :  tensor([-0.4815,  2.7258])\n",
      "Epoch 54, Loss 25.403149\n",
      "  params :  tensor([ 2.5428, -1.3132])\n",
      "  grad :  tensor([-0.4807,  2.7212])\n",
      "Epoch 55, Loss 25.326851\n",
      "  params :  tensor([ 2.5476, -1.3403])\n",
      "  grad :  tensor([-0.4799,  2.7166])\n",
      "Epoch 56, Loss 25.250811\n",
      "  params :  tensor([ 2.5523, -1.3675])\n",
      "  grad :  tensor([-0.4791,  2.7120])\n",
      "Epoch 57, Loss 25.175035\n",
      "  params :  tensor([ 2.5571, -1.3945])\n",
      "  grad :  tensor([-0.4783,  2.7074])\n",
      "Epoch 58, Loss 25.099512\n",
      "  params :  tensor([ 2.5619, -1.4216])\n",
      "  grad :  tensor([-0.4775,  2.7028])\n",
      "Epoch 59, Loss 25.024248\n",
      "  params :  tensor([ 2.5667, -1.4485])\n",
      "  grad :  tensor([-0.4766,  2.6982])\n",
      "Epoch 60, Loss 24.949236\n",
      "  params :  tensor([ 2.5714, -1.4755])\n",
      "  grad :  tensor([-0.4758,  2.6936])\n",
      "Epoch 61, Loss 24.874483\n",
      "  params :  tensor([ 2.5762, -1.5024])\n",
      "  grad :  tensor([-0.4750,  2.6890])\n",
      "Epoch 62, Loss 24.799976\n",
      "  params :  tensor([ 2.5809, -1.5292])\n",
      "  grad :  tensor([-0.4742,  2.6845])\n",
      "Epoch 63, Loss 24.725737\n",
      "  params :  tensor([ 2.5857, -1.5560])\n",
      "  grad :  tensor([-0.4734,  2.6799])\n",
      "Epoch 64, Loss 24.651739\n",
      "  params :  tensor([ 2.5904, -1.5828])\n",
      "  grad :  tensor([-0.4726,  2.6753])\n",
      "Epoch 65, Loss 24.577986\n",
      "  params :  tensor([ 2.5951, -1.6095])\n",
      "  grad :  tensor([-0.4718,  2.6708])\n",
      "Epoch 66, Loss 24.504494\n",
      "  params :  tensor([ 2.5998, -1.6361])\n",
      "  grad :  tensor([-0.4710,  2.6663])\n",
      "Epoch 67, Loss 24.431252\n",
      "  params :  tensor([ 2.6045, -1.6628])\n",
      "  grad :  tensor([-0.4702,  2.6617])\n",
      "Epoch 68, Loss 24.358257\n",
      "  params :  tensor([ 2.6092, -1.6893])\n",
      "  grad :  tensor([-0.4694,  2.6572])\n",
      "Epoch 69, Loss 24.285505\n",
      "  params :  tensor([ 2.6139, -1.7159])\n",
      "  grad :  tensor([-0.4686,  2.6527])\n",
      "Epoch 70, Loss 24.212999\n",
      "  params :  tensor([ 2.6186, -1.7423])\n",
      "  grad :  tensor([-0.4678,  2.6482])\n",
      "Epoch 71, Loss 24.140741\n",
      "  params :  tensor([ 2.6232, -1.7688])\n",
      "  grad :  tensor([-0.4670,  2.6437])\n",
      "Epoch 72, Loss 24.068733\n",
      "  params :  tensor([ 2.6279, -1.7952])\n",
      "  grad :  tensor([-0.4662,  2.6392])\n",
      "Epoch 73, Loss 23.996971\n",
      "  params :  tensor([ 2.6326, -1.8215])\n",
      "  grad :  tensor([-0.4654,  2.6347])\n",
      "Epoch 74, Loss 23.925446\n",
      "  params :  tensor([ 2.6372, -1.8478])\n",
      "  grad :  tensor([-0.4646,  2.6302])\n",
      "Epoch 75, Loss 23.854168\n",
      "  params :  tensor([ 2.6418, -1.8741])\n",
      "  grad :  tensor([-0.4638,  2.6258])\n",
      "Epoch 76, Loss 23.783125\n",
      "  params :  tensor([ 2.6465, -1.9003])\n",
      "  grad :  tensor([-0.4631,  2.6213])\n",
      "Epoch 77, Loss 23.712328\n",
      "  params :  tensor([ 2.6511, -1.9265])\n",
      "  grad :  tensor([-0.4623,  2.6169])\n",
      "Epoch 78, Loss 23.641773\n",
      "  params :  tensor([ 2.6557, -1.9526])\n",
      "  grad :  tensor([-0.4615,  2.6124])\n",
      "Epoch 79, Loss 23.571455\n",
      "  params :  tensor([ 2.6603, -1.9787])\n",
      "  grad :  tensor([-0.4607,  2.6080])\n",
      "Epoch 80, Loss 23.501379\n",
      "  params :  tensor([ 2.6649, -2.0047])\n",
      "  grad :  tensor([-0.4599,  2.6035])\n",
      "Epoch 81, Loss 23.431538\n",
      "  params :  tensor([ 2.6695, -2.0307])\n",
      "  grad :  tensor([-0.4591,  2.5991])\n",
      "Epoch 82, Loss 23.361937\n",
      "  params :  tensor([ 2.6741, -2.0566])\n",
      "  grad :  tensor([-0.4584,  2.5947])\n",
      "Epoch 83, Loss 23.292570\n",
      "  params :  tensor([ 2.6787, -2.0825])\n",
      "  grad :  tensor([-0.4576,  2.5903])\n",
      "Epoch 84, Loss 23.223436\n",
      "  params :  tensor([ 2.6832, -2.1084])\n",
      "  grad :  tensor([-0.4568,  2.5859])\n",
      "Epoch 85, Loss 23.154541\n",
      "  params :  tensor([ 2.6878, -2.1342])\n",
      "  grad :  tensor([-0.4560,  2.5815])\n",
      "Epoch 86, Loss 23.085882\n",
      "  params :  tensor([ 2.6923, -2.1600])\n",
      "  grad :  tensor([-0.4553,  2.5771])\n",
      "Epoch 87, Loss 23.017447\n",
      "  params :  tensor([ 2.6969, -2.1857])\n",
      "  grad :  tensor([-0.4545,  2.5727])\n",
      "Epoch 88, Loss 22.949251\n",
      "  params :  tensor([ 2.7014, -2.2114])\n",
      "  grad :  tensor([-0.4537,  2.5684])\n",
      "Epoch 89, Loss 22.881283\n",
      "  params :  tensor([ 2.7060, -2.2370])\n",
      "  grad :  tensor([-0.4529,  2.5640])\n",
      "Epoch 90, Loss 22.813549\n",
      "  params :  tensor([ 2.7105, -2.2626])\n",
      "  grad :  tensor([-0.4522,  2.5597])\n",
      "Epoch 91, Loss 22.746044\n",
      "  params :  tensor([ 2.7150, -2.2882])\n",
      "  grad :  tensor([-0.4514,  2.5553])\n",
      "Epoch 92, Loss 22.678766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([ 2.7195, -2.3137])\n",
      "  grad :  tensor([-0.4506,  2.5510])\n",
      "Epoch 93, Loss 22.611717\n",
      "  params :  tensor([ 2.7240, -2.3392])\n",
      "  grad :  tensor([-0.4499,  2.5466])\n",
      "Epoch 94, Loss 22.544899\n",
      "  params :  tensor([ 2.7285, -2.3646])\n",
      "  grad :  tensor([-0.4491,  2.5423])\n",
      "Epoch 95, Loss 22.478306\n",
      "  params :  tensor([ 2.7330, -2.3900])\n",
      "  grad :  tensor([-0.4483,  2.5380])\n",
      "Epoch 96, Loss 22.411934\n",
      "  params :  tensor([ 2.7374, -2.4153])\n",
      "  grad :  tensor([-0.4476,  2.5337])\n",
      "Epoch 97, Loss 22.345793\n",
      "  params :  tensor([ 2.7419, -2.4406])\n",
      "  grad :  tensor([-0.4468,  2.5294])\n",
      "Epoch 98, Loss 22.279875\n",
      "  params :  tensor([ 2.7464, -2.4658])\n",
      "  grad :  tensor([-0.4461,  2.5251])\n",
      "Epoch 99, Loss 22.214186\n",
      "  params :  tensor([ 2.7508, -2.4910])\n",
      "  grad :  tensor([-0.4453,  2.5208])\n",
      "Epoch 100, Loss 22.148710\n",
      "  params :  tensor([ 2.7553, -2.5162])\n",
      "  grad :  tensor([-0.4446,  2.5165])\n",
      "Epoch 101, Loss 22.083464\n",
      "  params :  tensor([ 2.7597, -2.5413])\n",
      "  grad :  tensor([-0.4438,  2.5122])\n",
      "Epoch 102, Loss 22.018436\n",
      "  params :  tensor([ 2.7641, -2.5664])\n",
      "  grad :  tensor([-0.4430,  2.5080])\n",
      "Epoch 103, Loss 21.953632\n",
      "  params :  tensor([ 2.7686, -2.5914])\n",
      "  grad :  tensor([-0.4423,  2.5037])\n",
      "Epoch 104, Loss 21.889046\n",
      "  params :  tensor([ 2.7730, -2.6164])\n",
      "  grad :  tensor([-0.4415,  2.4994])\n",
      "Epoch 105, Loss 21.824677\n",
      "  params :  tensor([ 2.7774, -2.6414])\n",
      "  grad :  tensor([-0.4408,  2.4952])\n",
      "Epoch 106, Loss 21.760529\n",
      "  params :  tensor([ 2.7818, -2.6663])\n",
      "  grad :  tensor([-0.4400,  2.4910])\n",
      "Epoch 107, Loss 21.696600\n",
      "  params :  tensor([ 2.7862, -2.6912])\n",
      "  grad :  tensor([-0.4393,  2.4867])\n",
      "Epoch 108, Loss 21.632883\n",
      "  params :  tensor([ 2.7906, -2.7160])\n",
      "  grad :  tensor([-0.4385,  2.4825])\n",
      "Epoch 109, Loss 21.569389\n",
      "  params :  tensor([ 2.7949, -2.7408])\n",
      "  grad :  tensor([-0.4378,  2.4783])\n",
      "Epoch 110, Loss 21.506102\n",
      "  params :  tensor([ 2.7993, -2.7655])\n",
      "  grad :  tensor([-0.4370,  2.4741])\n",
      "Epoch 111, Loss 21.443037\n",
      "  params :  tensor([ 2.8037, -2.7902])\n",
      "  grad :  tensor([-0.4363,  2.4699])\n",
      "Epoch 112, Loss 21.380186\n",
      "  params :  tensor([ 2.8080, -2.8149])\n",
      "  grad :  tensor([-0.4356,  2.4657])\n",
      "Epoch 113, Loss 21.317549\n",
      "  params :  tensor([ 2.8124, -2.8395])\n",
      "  grad :  tensor([-0.4348,  2.4615])\n",
      "Epoch 114, Loss 21.255117\n",
      "  params :  tensor([ 2.8167, -2.8641])\n",
      "  grad :  tensor([-0.4341,  2.4573])\n",
      "Epoch 115, Loss 21.192907\n",
      "  params :  tensor([ 2.8211, -2.8886])\n",
      "  grad :  tensor([-0.4334,  2.4531])\n",
      "Epoch 116, Loss 21.130898\n",
      "  params :  tensor([ 2.8254, -2.9131])\n",
      "  grad :  tensor([-0.4326,  2.4490])\n",
      "Epoch 117, Loss 21.069105\n",
      "  params :  tensor([ 2.8297, -2.9375])\n",
      "  grad :  tensor([-0.4319,  2.4448])\n",
      "Epoch 118, Loss 21.007526\n",
      "  params :  tensor([ 2.8340, -2.9619])\n",
      "  grad :  tensor([-0.4311,  2.4407])\n",
      "Epoch 119, Loss 20.946150\n",
      "  params :  tensor([ 2.8383, -2.9863])\n",
      "  grad :  tensor([-0.4304,  2.4365])\n",
      "Epoch 120, Loss 20.884981\n",
      "  params :  tensor([ 2.8426, -3.0106])\n",
      "  grad :  tensor([-0.4297,  2.4324])\n",
      "Epoch 121, Loss 20.824024\n",
      "  params :  tensor([ 2.8469, -3.0349])\n",
      "  grad :  tensor([-0.4290,  2.4282])\n",
      "Epoch 122, Loss 20.763273\n",
      "  params :  tensor([ 2.8512, -3.0592])\n",
      "  grad :  tensor([-0.4282,  2.4241])\n",
      "Epoch 123, Loss 20.702728\n",
      "  params :  tensor([ 2.8555, -3.0834])\n",
      "  grad :  tensor([-0.4275,  2.4200])\n",
      "Epoch 124, Loss 20.642384\n",
      "  params :  tensor([ 2.8597, -3.1075])\n",
      "  grad :  tensor([-0.4268,  2.4159])\n",
      "Epoch 125, Loss 20.582249\n",
      "  params :  tensor([ 2.8640, -3.1316])\n",
      "  grad :  tensor([-0.4260,  2.4118])\n",
      "Epoch 126, Loss 20.522322\n",
      "  params :  tensor([ 2.8682, -3.1557])\n",
      "  grad :  tensor([-0.4253,  2.4077])\n",
      "Epoch 127, Loss 20.462593\n",
      "  params :  tensor([ 2.8725, -3.1797])\n",
      "  grad :  tensor([-0.4246,  2.4036])\n",
      "Epoch 128, Loss 20.403069\n",
      "  params :  tensor([ 2.8767, -3.2037])\n",
      "  grad :  tensor([-0.4239,  2.3995])\n",
      "Epoch 129, Loss 20.343742\n",
      "  params :  tensor([ 2.8810, -3.2277])\n",
      "  grad :  tensor([-0.4232,  2.3954])\n",
      "Epoch 130, Loss 20.284624\n",
      "  params :  tensor([ 2.8852, -3.2516])\n",
      "  grad :  tensor([-0.4224,  2.3914])\n",
      "Epoch 131, Loss 20.225702\n",
      "  params :  tensor([ 2.8894, -3.2755])\n",
      "  grad :  tensor([-0.4217,  2.3873])\n",
      "Epoch 132, Loss 20.166981\n",
      "  params :  tensor([ 2.8936, -3.2993])\n",
      "  grad :  tensor([-0.4210,  2.3832])\n",
      "Epoch 133, Loss 20.108461\n",
      "  params :  tensor([ 2.8978, -3.3231])\n",
      "  grad :  tensor([-0.4203,  2.3792])\n",
      "Epoch 134, Loss 20.050137\n",
      "  params :  tensor([ 2.9020, -3.3469])\n",
      "  grad :  tensor([-0.4196,  2.3752])\n",
      "Epoch 135, Loss 19.992016\n",
      "  params :  tensor([ 2.9062, -3.3706])\n",
      "  grad :  tensor([-0.4189,  2.3711])\n",
      "Epoch 136, Loss 19.934086\n",
      "  params :  tensor([ 2.9104, -3.3942])\n",
      "  grad :  tensor([-0.4182,  2.3671])\n",
      "Epoch 137, Loss 19.876352\n",
      "  params :  tensor([ 2.9146, -3.4179])\n",
      "  grad :  tensor([-0.4174,  2.3631])\n",
      "Epoch 138, Loss 19.818823\n",
      "  params :  tensor([ 2.9187, -3.4415])\n",
      "  grad :  tensor([-0.4167,  2.3591])\n",
      "Epoch 139, Loss 19.761480\n",
      "  params :  tensor([ 2.9229, -3.4650])\n",
      "  grad :  tensor([-0.4160,  2.3550])\n",
      "Epoch 140, Loss 19.704336\n",
      "  params :  tensor([ 2.9270, -3.4885])\n",
      "  grad :  tensor([-0.4153,  2.3510])\n",
      "Epoch 141, Loss 19.647385\n",
      "  params :  tensor([ 2.9312, -3.5120])\n",
      "  grad :  tensor([-0.4146,  2.3471])\n",
      "Epoch 142, Loss 19.590626\n",
      "  params :  tensor([ 2.9353, -3.5354])\n",
      "  grad :  tensor([-0.4139,  2.3431])\n",
      "Epoch 143, Loss 19.534061\n",
      "  params :  tensor([ 2.9395, -3.5588])\n",
      "  grad :  tensor([-0.4132,  2.3391])\n",
      "Epoch 144, Loss 19.477690\n",
      "  params :  tensor([ 2.9436, -3.5822])\n",
      "  grad :  tensor([-0.4125,  2.3351])\n",
      "Epoch 145, Loss 19.421507\n",
      "  params :  tensor([ 2.9477, -3.6055])\n",
      "  grad :  tensor([-0.4118,  2.3311])\n",
      "Epoch 146, Loss 19.365515\n",
      "  params :  tensor([ 2.9518, -3.6287])\n",
      "  grad :  tensor([-0.4111,  2.3272])\n",
      "Epoch 147, Loss 19.309715\n",
      "  params :  tensor([ 2.9559, -3.6520])\n",
      "  grad :  tensor([-0.4104,  2.3232])\n",
      "Epoch 148, Loss 19.254107\n",
      "  params :  tensor([ 2.9600, -3.6752])\n",
      "  grad :  tensor([-0.4097,  2.3193])\n",
      "Epoch 149, Loss 19.198685\n",
      "  params :  tensor([ 2.9641, -3.6983])\n",
      "  grad :  tensor([-0.4090,  2.3153])\n",
      "Epoch 150, Loss 19.143446\n",
      "  params :  tensor([ 2.9682, -3.7214])\n",
      "  grad :  tensor([-0.4083,  2.3114])\n",
      "Epoch 151, Loss 19.088402\n",
      "  params :  tensor([ 2.9723, -3.7445])\n",
      "  grad :  tensor([-0.4076,  2.3075])\n",
      "Epoch 152, Loss 19.033543\n",
      "  params :  tensor([ 2.9763, -3.7675])\n",
      "  grad :  tensor([-0.4069,  2.3036])\n",
      "Epoch 153, Loss 18.978868\n",
      "  params :  tensor([ 2.9804, -3.7905])\n",
      "  grad :  tensor([-0.4062,  2.2997])\n",
      "Epoch 154, Loss 18.924377\n",
      "  params :  tensor([ 2.9844, -3.8135])\n",
      "  grad :  tensor([-0.4056,  2.2957])\n",
      "Epoch 155, Loss 18.870081\n",
      "  params :  tensor([ 2.9885, -3.8364])\n",
      "  grad :  tensor([-0.4049,  2.2918])\n",
      "Epoch 156, Loss 18.815960\n",
      "  params :  tensor([ 2.9925, -3.8593])\n",
      "  grad :  tensor([-0.4042,  2.2880])\n",
      "Epoch 157, Loss 18.762022\n",
      "  params :  tensor([ 2.9966, -3.8821])\n",
      "  grad :  tensor([-0.4035,  2.2841])\n",
      "Epoch 158, Loss 18.708271\n",
      "  params :  tensor([ 3.0006, -3.9049])\n",
      "  grad :  tensor([-0.4028,  2.2802])\n",
      "Epoch 159, Loss 18.654699\n",
      "  params :  tensor([ 3.0046, -3.9277])\n",
      "  grad :  tensor([-0.4021,  2.2763])\n",
      "Epoch 160, Loss 18.601313\n",
      "  params :  tensor([ 3.0086, -3.9504])\n",
      "  grad :  tensor([-0.4014,  2.2724])\n",
      "Epoch 161, Loss 18.548109\n",
      "  params :  tensor([ 3.0126, -3.9731])\n",
      "  grad :  tensor([-0.4007,  2.2686])\n",
      "Epoch 162, Loss 18.495085\n",
      "  params :  tensor([ 3.0166, -3.9958])\n",
      "  grad :  tensor([-0.4001,  2.2647])\n",
      "Epoch 163, Loss 18.442236\n",
      "  params :  tensor([ 3.0206, -4.0184])\n",
      "  grad :  tensor([-0.3994,  2.2609])\n",
      "Epoch 164, Loss 18.389570\n",
      "  params :  tensor([ 3.0246, -4.0409])\n",
      "  grad :  tensor([-0.3987,  2.2570])\n",
      "Epoch 165, Loss 18.337080\n",
      "  params :  tensor([ 3.0286, -4.0635])\n",
      "  grad :  tensor([-0.3980,  2.2532])\n",
      "Epoch 166, Loss 18.284777\n",
      "  params :  tensor([ 3.0326, -4.0860])\n",
      "  grad :  tensor([-0.3974,  2.2494])\n",
      "Epoch 167, Loss 18.232641\n",
      "  params :  tensor([ 3.0365, -4.1084])\n",
      "  grad :  tensor([-0.3967,  2.2456])\n",
      "Epoch 168, Loss 18.180685\n",
      "  params :  tensor([ 3.0405, -4.1308])\n",
      "  grad :  tensor([-0.3960,  2.2417])\n",
      "Epoch 169, Loss 18.128906\n",
      "  params :  tensor([ 3.0445, -4.1532])\n",
      "  grad :  tensor([-0.3953,  2.2379])\n",
      "Epoch 170, Loss 18.077301\n",
      "  params :  tensor([ 3.0484, -4.1756])\n",
      "  grad :  tensor([-0.3947,  2.2341])\n",
      "Epoch 171, Loss 18.025877\n",
      "  params :  tensor([ 3.0523, -4.1979])\n",
      "  grad :  tensor([-0.3940,  2.2303])\n",
      "Epoch 172, Loss 17.974623\n",
      "  params :  tensor([ 3.0563, -4.2201])\n",
      "  grad :  tensor([-0.3933,  2.2266])\n",
      "Epoch 173, Loss 17.923546\n",
      "  params :  tensor([ 3.0602, -4.2424])\n",
      "  grad :  tensor([-0.3927,  2.2228])\n",
      "Epoch 174, Loss 17.872643\n",
      "  params :  tensor([ 3.0641, -4.2646])\n",
      "  grad :  tensor([-0.3920,  2.2190])\n",
      "Epoch 175, Loss 17.821909\n",
      "  params :  tensor([ 3.0680, -4.2867])\n",
      "  grad :  tensor([-0.3913,  2.2152])\n",
      "Epoch 176, Loss 17.771345\n",
      "  params :  tensor([ 3.0719, -4.3088])\n",
      "  grad :  tensor([-0.3907,  2.2115])\n",
      "Epoch 177, Loss 17.720955\n",
      "  params :  tensor([ 3.0758, -4.3309])\n",
      "  grad :  tensor([-0.3900,  2.2077])\n",
      "Epoch 178, Loss 17.670738\n",
      "  params :  tensor([ 3.0797, -4.3529])\n",
      "  grad :  tensor([-0.3893,  2.2040])\n",
      "Epoch 179, Loss 17.620689\n",
      "  params :  tensor([ 3.0836, -4.3749])\n",
      "  grad :  tensor([-0.3887,  2.2002])\n",
      "Epoch 180, Loss 17.570814\n",
      "  params :  tensor([ 3.0875, -4.3969])\n",
      "  grad :  tensor([-0.3880,  2.1965])\n",
      "Epoch 181, Loss 17.521103\n",
      "  params :  tensor([ 3.0914, -4.4188])\n",
      "  grad :  tensor([-0.3873,  2.1927])\n",
      "Epoch 182, Loss 17.471565\n",
      "  params :  tensor([ 3.0952, -4.4407])\n",
      "  grad :  tensor([-0.3867,  2.1890])\n",
      "Epoch 183, Loss 17.422192\n",
      "  params :  tensor([ 3.0991, -4.4626])\n",
      "  grad :  tensor([-0.3860,  2.1853])\n",
      "Epoch 184, Loss 17.372993\n",
      "  params :  tensor([ 3.1030, -4.4844])\n",
      "  grad :  tensor([-0.3854,  2.1816])\n",
      "Epoch 185, Loss 17.323954\n",
      "  params :  tensor([ 3.1068, -4.5062])\n",
      "  grad :  tensor([-0.3847,  2.1779])\n",
      "Epoch 186, Loss 17.275084\n",
      "  params :  tensor([ 3.1106, -4.5279])\n",
      "  grad :  tensor([-0.3841,  2.1742])\n",
      "Epoch 187, Loss 17.226379\n",
      "  params :  tensor([ 3.1145, -4.5496])\n",
      "  grad :  tensor([-0.3834,  2.1705])\n",
      "Epoch 188, Loss 17.177839\n",
      "  params :  tensor([ 3.1183, -4.5713])\n",
      "  grad :  tensor([-0.3828,  2.1668])\n",
      "Epoch 189, Loss 17.129463\n",
      "  params :  tensor([ 3.1221, -4.5929])\n",
      "  grad :  tensor([-0.3821,  2.1631])\n",
      "Epoch 190, Loss 17.081255\n",
      "  params :  tensor([ 3.1259, -4.6145])\n",
      "  grad :  tensor([-0.3815,  2.1594])\n",
      "Epoch 191, Loss 17.033209\n",
      "  params :  tensor([ 3.1298, -4.6361])\n",
      "  grad :  tensor([-0.3808,  2.1558])\n",
      "Epoch 192, Loss 16.985327\n",
      "  params :  tensor([ 3.1336, -4.6576])\n",
      "  grad :  tensor([-0.3802,  2.1521])\n",
      "Epoch 193, Loss 16.937605\n",
      "  params :  tensor([ 3.1374, -4.6791])\n",
      "  grad :  tensor([-0.3795,  2.1485])\n",
      "Epoch 194, Loss 16.890047\n",
      "  params :  tensor([ 3.1411, -4.7005])\n",
      "  grad :  tensor([-0.3789,  2.1448])\n",
      "Epoch 195, Loss 16.842649\n",
      "  params :  tensor([ 3.1449, -4.7219])\n",
      "  grad :  tensor([-0.3782,  2.1412])\n",
      "Epoch 196, Loss 16.795412\n",
      "  params :  tensor([ 3.1487, -4.7433])\n",
      "  grad :  tensor([-0.3776,  2.1375])\n",
      "Epoch 197, Loss 16.748339\n",
      "  params :  tensor([ 3.1525, -4.7646])\n",
      "  grad :  tensor([-0.3770,  2.1339])\n",
      "Epoch 198, Loss 16.701422\n",
      "  params :  tensor([ 3.1562, -4.7859])\n",
      "  grad :  tensor([-0.3763,  2.1303])\n",
      "Epoch 199, Loss 16.654661\n",
      "  params :  tensor([ 3.1600, -4.8072])\n",
      "  grad :  tensor([-0.3757,  2.1267])\n",
      "Epoch 200, Loss 16.608067\n",
      "  params :  tensor([ 3.1637, -4.8284])\n",
      "  grad :  tensor([-0.3750,  2.1230])\n",
      "Epoch 201, Loss 16.561623\n",
      "  params :  tensor([ 3.1675, -4.8496])\n",
      "  grad :  tensor([-0.3744,  2.1194])\n",
      "Epoch 202, Loss 16.515343\n",
      "  params :  tensor([ 3.1712, -4.8708])\n",
      "  grad :  tensor([-0.3738,  2.1158])\n",
      "Epoch 203, Loss 16.469219\n",
      "  params :  tensor([ 3.1750, -4.8919])\n",
      "  grad :  tensor([-0.3731,  2.1122])\n",
      "Epoch 204, Loss 16.423248\n",
      "  params :  tensor([ 3.1787, -4.9130])\n",
      "  grad :  tensor([-0.3725,  2.1087])\n",
      "Epoch 205, Loss 16.377434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([ 3.1824, -4.9341])\n",
      "  grad :  tensor([-0.3719,  2.1051])\n",
      "Epoch 206, Loss 16.331776\n",
      "  params :  tensor([ 3.1861, -4.9551])\n",
      "  grad :  tensor([-0.3712,  2.1015])\n",
      "Epoch 207, Loss 16.286276\n",
      "  params :  tensor([ 3.1898, -4.9760])\n",
      "  grad :  tensor([-0.3706,  2.0979])\n",
      "Epoch 208, Loss 16.240929\n",
      "  params :  tensor([ 3.1935, -4.9970])\n",
      "  grad :  tensor([-0.3700,  2.0944])\n",
      "Epoch 209, Loss 16.195732\n",
      "  params :  tensor([ 3.1972, -5.0179])\n",
      "  grad :  tensor([-0.3694,  2.0908])\n",
      "Epoch 210, Loss 16.150694\n",
      "  params :  tensor([ 3.2009, -5.0388])\n",
      "  grad :  tensor([-0.3687,  2.0873])\n",
      "Epoch 211, Loss 16.105806\n",
      "  params :  tensor([ 3.2046, -5.0596])\n",
      "  grad :  tensor([-0.3681,  2.0837])\n",
      "Epoch 212, Loss 16.061073\n",
      "  params :  tensor([ 3.2082, -5.0804])\n",
      "  grad :  tensor([-0.3675,  2.0802])\n",
      "Epoch 213, Loss 16.016487\n",
      "  params :  tensor([ 3.2119, -5.1012])\n",
      "  grad :  tensor([-0.3668,  2.0766])\n",
      "Epoch 214, Loss 15.972058\n",
      "  params :  tensor([ 3.2156, -5.1219])\n",
      "  grad :  tensor([-0.3662,  2.0731])\n",
      "Epoch 215, Loss 15.927776\n",
      "  params :  tensor([ 3.2192, -5.1426])\n",
      "  grad :  tensor([-0.3656,  2.0696])\n",
      "Epoch 216, Loss 15.883645\n",
      "  params :  tensor([ 3.2229, -5.1633])\n",
      "  grad :  tensor([-0.3650,  2.0661])\n",
      "Epoch 217, Loss 15.839664\n",
      "  params :  tensor([ 3.2265, -5.1839])\n",
      "  grad :  tensor([-0.3644,  2.0626])\n",
      "Epoch 218, Loss 15.795832\n",
      "  params :  tensor([ 3.2302, -5.2045])\n",
      "  grad :  tensor([-0.3637,  2.0591])\n",
      "Epoch 219, Loss 15.752152\n",
      "  params :  tensor([ 3.2338, -5.2250])\n",
      "  grad :  tensor([-0.3631,  2.0556])\n",
      "Epoch 220, Loss 15.708612\n",
      "  params :  tensor([ 3.2374, -5.2456])\n",
      "  grad :  tensor([-0.3625,  2.0521])\n",
      "Epoch 221, Loss 15.665226\n",
      "  params :  tensor([ 3.2410, -5.2660])\n",
      "  grad :  tensor([-0.3619,  2.0486])\n",
      "Epoch 222, Loss 15.621990\n",
      "  params :  tensor([ 3.2447, -5.2865])\n",
      "  grad :  tensor([-0.3613,  2.0451])\n",
      "Epoch 223, Loss 15.578897\n",
      "  params :  tensor([ 3.2483, -5.3069])\n",
      "  grad :  tensor([-0.3607,  2.0416])\n",
      "Epoch 224, Loss 15.535950\n",
      "  params :  tensor([ 3.2519, -5.3273])\n",
      "  grad :  tensor([-0.3601,  2.0382])\n",
      "Epoch 225, Loss 15.493150\n",
      "  params :  tensor([ 3.2555, -5.3476])\n",
      "  grad :  tensor([-0.3594,  2.0347])\n",
      "Epoch 226, Loss 15.450495\n",
      "  params :  tensor([ 3.2590, -5.3680])\n",
      "  grad :  tensor([-0.3588,  2.0312])\n",
      "Epoch 227, Loss 15.407981\n",
      "  params :  tensor([ 3.2626, -5.3882])\n",
      "  grad :  tensor([-0.3582,  2.0278])\n",
      "Epoch 228, Loss 15.365616\n",
      "  params :  tensor([ 3.2662, -5.4085])\n",
      "  grad :  tensor([-0.3576,  2.0243])\n",
      "Epoch 229, Loss 15.323396\n",
      "  params :  tensor([ 3.2698, -5.4287])\n",
      "  grad :  tensor([-0.3570,  2.0209])\n",
      "Epoch 230, Loss 15.281317\n",
      "  params :  tensor([ 3.2733, -5.4489])\n",
      "  grad :  tensor([-0.3564,  2.0175])\n",
      "Epoch 231, Loss 15.239380\n",
      "  params :  tensor([ 3.2769, -5.4690])\n",
      "  grad :  tensor([-0.3558,  2.0140])\n",
      "Epoch 232, Loss 15.197585\n",
      "  params :  tensor([ 3.2804, -5.4891])\n",
      "  grad :  tensor([-0.3552,  2.0106])\n",
      "Epoch 233, Loss 15.155932\n",
      "  params :  tensor([ 3.2840, -5.5092])\n",
      "  grad :  tensor([-0.3546,  2.0072])\n",
      "Epoch 234, Loss 15.114425\n",
      "  params :  tensor([ 3.2875, -5.5292])\n",
      "  grad :  tensor([-0.3540,  2.0038])\n",
      "Epoch 235, Loss 15.073055\n",
      "  params :  tensor([ 3.2911, -5.5492])\n",
      "  grad :  tensor([-0.3534,  2.0004])\n",
      "Epoch 236, Loss 15.031823\n",
      "  params :  tensor([ 3.2946, -5.5692])\n",
      "  grad :  tensor([-0.3528,  1.9970])\n",
      "Epoch 237, Loss 14.990734\n",
      "  params :  tensor([ 3.2981, -5.5891])\n",
      "  grad :  tensor([-0.3522,  1.9936])\n",
      "Epoch 238, Loss 14.949784\n",
      "  params :  tensor([ 3.3016, -5.6090])\n",
      "  grad :  tensor([-0.3516,  1.9902])\n",
      "Epoch 239, Loss 14.908973\n",
      "  params :  tensor([ 3.3051, -5.6289])\n",
      "  grad :  tensor([-0.3510,  1.9868])\n",
      "Epoch 240, Loss 14.868304\n",
      "  params :  tensor([ 3.3086, -5.6487])\n",
      "  grad :  tensor([-0.3504,  1.9835])\n",
      "Epoch 241, Loss 14.827767\n",
      "  params :  tensor([ 3.3121, -5.6685])\n",
      "  grad :  tensor([-0.3498,  1.9801])\n",
      "Epoch 242, Loss 14.787370\n",
      "  params :  tensor([ 3.3156, -5.6883])\n",
      "  grad :  tensor([-0.3492,  1.9767])\n",
      "Epoch 243, Loss 14.747109\n",
      "  params :  tensor([ 3.3191, -5.7080])\n",
      "  grad :  tensor([-0.3486,  1.9734])\n",
      "Epoch 244, Loss 14.706989\n",
      "  params :  tensor([ 3.3226, -5.7277])\n",
      "  grad :  tensor([-0.3480,  1.9700])\n",
      "Epoch 245, Loss 14.667002\n",
      "  params :  tensor([ 3.3261, -5.7474])\n",
      "  grad :  tensor([-0.3474,  1.9667])\n",
      "Epoch 246, Loss 14.627151\n",
      "  params :  tensor([ 3.3295, -5.7670])\n",
      "  grad :  tensor([-0.3468,  1.9633])\n",
      "Epoch 247, Loss 14.587436\n",
      "  params :  tensor([ 3.3330, -5.7866])\n",
      "  grad :  tensor([-0.3462,  1.9600])\n",
      "Epoch 248, Loss 14.547855\n",
      "  params :  tensor([ 3.3365, -5.8062])\n",
      "  grad :  tensor([-0.3456,  1.9567])\n",
      "Epoch 249, Loss 14.508409\n",
      "  params :  tensor([ 3.3399, -5.8257])\n",
      "  grad :  tensor([-0.3451,  1.9533])\n",
      "Epoch 250, Loss 14.469097\n",
      "  params :  tensor([ 3.3434, -5.8452])\n",
      "  grad :  tensor([-0.3445,  1.9500])\n",
      "Epoch 251, Loss 14.429920\n",
      "  params :  tensor([ 3.3468, -5.8647])\n",
      "  grad :  tensor([-0.3439,  1.9467])\n",
      "Epoch 252, Loss 14.390870\n",
      "  params :  tensor([ 3.3502, -5.8841])\n",
      "  grad :  tensor([-0.3433,  1.9434])\n",
      "Epoch 253, Loss 14.351956\n",
      "  params :  tensor([ 3.3537, -5.9035])\n",
      "  grad :  tensor([-0.3427,  1.9401])\n",
      "Epoch 254, Loss 14.313177\n",
      "  params :  tensor([ 3.3571, -5.9229])\n",
      "  grad :  tensor([-0.3421,  1.9368])\n",
      "Epoch 255, Loss 14.274529\n",
      "  params :  tensor([ 3.3605, -5.9422])\n",
      "  grad :  tensor([-0.3416,  1.9335])\n",
      "Epoch 256, Loss 14.236009\n",
      "  params :  tensor([ 3.3639, -5.9615])\n",
      "  grad :  tensor([-0.3410,  1.9302])\n",
      "Epoch 257, Loss 14.197620\n",
      "  params :  tensor([ 3.3673, -5.9808])\n",
      "  grad :  tensor([-0.3404,  1.9269])\n",
      "Epoch 258, Loss 14.159363\n",
      "  params :  tensor([ 3.3707, -6.0000])\n",
      "  grad :  tensor([-0.3398,  1.9237])\n",
      "Epoch 259, Loss 14.121234\n",
      "  params :  tensor([ 3.3741, -6.0192])\n",
      "  grad :  tensor([-0.3392,  1.9204])\n",
      "Epoch 260, Loss 14.083236\n",
      "  params :  tensor([ 3.3775, -6.0384])\n",
      "  grad :  tensor([-0.3387,  1.9171])\n",
      "Epoch 261, Loss 14.045367\n",
      "  params :  tensor([ 3.3809, -6.0576])\n",
      "  grad :  tensor([-0.3381,  1.9139])\n",
      "Epoch 262, Loss 14.007627\n",
      "  params :  tensor([ 3.3842, -6.0767])\n",
      "  grad :  tensor([-0.3375,  1.9106])\n",
      "Epoch 263, Loss 13.970016\n",
      "  params :  tensor([ 3.3876, -6.0957])\n",
      "  grad :  tensor([-0.3369,  1.9074])\n",
      "Epoch 264, Loss 13.932531\n",
      "  params :  tensor([ 3.3910, -6.1148])\n",
      "  grad :  tensor([-0.3364,  1.9041])\n",
      "Epoch 265, Loss 13.895172\n",
      "  params :  tensor([ 3.3943, -6.1338])\n",
      "  grad :  tensor([-0.3358,  1.9009])\n",
      "Epoch 266, Loss 13.857944\n",
      "  params :  tensor([ 3.3977, -6.1528])\n",
      "  grad :  tensor([-0.3352,  1.8977])\n",
      "Epoch 267, Loss 13.820837\n",
      "  params :  tensor([ 3.4010, -6.1717])\n",
      "  grad :  tensor([-0.3347,  1.8945])\n",
      "Epoch 268, Loss 13.783858\n",
      "  params :  tensor([ 3.4044, -6.1906])\n",
      "  grad :  tensor([-0.3341,  1.8912])\n",
      "Epoch 269, Loss 13.747006\n",
      "  params :  tensor([ 3.4077, -6.2095])\n",
      "  grad :  tensor([-0.3335,  1.8880])\n",
      "Epoch 270, Loss 13.710278\n",
      "  params :  tensor([ 3.4110, -6.2284])\n",
      "  grad :  tensor([-0.3330,  1.8848])\n",
      "Epoch 271, Loss 13.673676\n",
      "  params :  tensor([ 3.4144, -6.2472])\n",
      "  grad :  tensor([-0.3324,  1.8816])\n",
      "Epoch 272, Loss 13.637196\n",
      "  params :  tensor([ 3.4177, -6.2660])\n",
      "  grad :  tensor([-0.3318,  1.8784])\n",
      "Epoch 273, Loss 13.600842\n",
      "  params :  tensor([ 3.4210, -6.2847])\n",
      "  grad :  tensor([-0.3313,  1.8752])\n",
      "Epoch 274, Loss 13.564609\n",
      "  params :  tensor([ 3.4243, -6.3034])\n",
      "  grad :  tensor([-0.3307,  1.8720])\n",
      "Epoch 275, Loss 13.528501\n",
      "  params :  tensor([ 3.4276, -6.3221])\n",
      "  grad :  tensor([-0.3301,  1.8689])\n",
      "Epoch 276, Loss 13.492514\n",
      "  params :  tensor([ 3.4309, -6.3408])\n",
      "  grad :  tensor([-0.3296,  1.8657])\n",
      "Epoch 277, Loss 13.456651\n",
      "  params :  tensor([ 3.4342, -6.3594])\n",
      "  grad :  tensor([-0.3290,  1.8625])\n",
      "Epoch 278, Loss 13.420910\n",
      "  params :  tensor([ 3.4375, -6.3780])\n",
      "  grad :  tensor([-0.3285,  1.8594])\n",
      "Epoch 279, Loss 13.385287\n",
      "  params :  tensor([ 3.4407, -6.3966])\n",
      "  grad :  tensor([-0.3279,  1.8562])\n",
      "Epoch 280, Loss 13.349789\n",
      "  params :  tensor([ 3.4440, -6.4151])\n",
      "  grad :  tensor([-0.3274,  1.8530])\n",
      "Epoch 281, Loss 13.314407\n",
      "  params :  tensor([ 3.4473, -6.4336])\n",
      "  grad :  tensor([-0.3268,  1.8499])\n",
      "Epoch 282, Loss 13.279150\n",
      "  params :  tensor([ 3.4506, -6.4520])\n",
      "  grad :  tensor([-0.3262,  1.8468])\n",
      "Epoch 283, Loss 13.244009\n",
      "  params :  tensor([ 3.4538, -6.4705])\n",
      "  grad :  tensor([-0.3257,  1.8436])\n",
      "Epoch 284, Loss 13.208991\n",
      "  params :  tensor([ 3.4571, -6.4889])\n",
      "  grad :  tensor([-0.3251,  1.8405])\n",
      "Epoch 285, Loss 13.174088\n",
      "  params :  tensor([ 3.4603, -6.5073])\n",
      "  grad :  tensor([-0.3246,  1.8374])\n",
      "Epoch 286, Loss 13.139307\n",
      "  params :  tensor([ 3.4635, -6.5256])\n",
      "  grad :  tensor([-0.3240,  1.8342])\n",
      "Epoch 287, Loss 13.104639\n",
      "  params :  tensor([ 3.4668, -6.5439])\n",
      "  grad :  tensor([-0.3235,  1.8311])\n",
      "Epoch 288, Loss 13.070092\n",
      "  params :  tensor([ 3.4700, -6.5622])\n",
      "  grad :  tensor([-0.3229,  1.8280])\n",
      "Epoch 289, Loss 13.035664\n",
      "  params :  tensor([ 3.4732, -6.5804])\n",
      "  grad :  tensor([-0.3224,  1.8249])\n",
      "Epoch 290, Loss 13.001349\n",
      "  params :  tensor([ 3.4765, -6.5987])\n",
      "  grad :  tensor([-0.3218,  1.8218])\n",
      "Epoch 291, Loss 12.967152\n",
      "  params :  tensor([ 3.4797, -6.6169])\n",
      "  grad :  tensor([-0.3213,  1.8187])\n",
      "Epoch 292, Loss 12.933075\n",
      "  params :  tensor([ 3.4829, -6.6350])\n",
      "  grad :  tensor([-0.3207,  1.8156])\n",
      "Epoch 293, Loss 12.899109\n",
      "  params :  tensor([ 3.4861, -6.6531])\n",
      "  grad :  tensor([-0.3202,  1.8125])\n",
      "Epoch 294, Loss 12.865259\n",
      "  params :  tensor([ 3.4893, -6.6712])\n",
      "  grad :  tensor([-0.3196,  1.8095])\n",
      "Epoch 295, Loss 12.831525\n",
      "  params :  tensor([ 3.4925, -6.6893])\n",
      "  grad :  tensor([-0.3191,  1.8064])\n",
      "Epoch 296, Loss 12.797904\n",
      "  params :  tensor([ 3.4956, -6.7073])\n",
      "  grad :  tensor([-0.3186,  1.8033])\n",
      "Epoch 297, Loss 12.764399\n",
      "  params :  tensor([ 3.4988, -6.7253])\n",
      "  grad :  tensor([-0.3180,  1.8003])\n",
      "Epoch 298, Loss 12.731007\n",
      "  params :  tensor([ 3.5020, -6.7433])\n",
      "  grad :  tensor([-0.3175,  1.7972])\n",
      "Epoch 299, Loss 12.697727\n",
      "  params :  tensor([ 3.5052, -6.7612])\n",
      "  grad :  tensor([-0.3169,  1.7941])\n",
      "Epoch 300, Loss 12.664559\n",
      "  params :  tensor([ 3.5083, -6.7792])\n",
      "  grad :  tensor([-0.3164,  1.7911])\n",
      "Epoch 301, Loss 12.631507\n",
      "  params :  tensor([ 3.5115, -6.7970])\n",
      "  grad :  tensor([-0.3159,  1.7881])\n",
      "Epoch 302, Loss 12.598568\n",
      "  params :  tensor([ 3.5146, -6.8149])\n",
      "  grad :  tensor([-0.3153,  1.7850])\n",
      "Epoch 303, Loss 12.565738\n",
      "  params :  tensor([ 3.5178, -6.8327])\n",
      "  grad :  tensor([-0.3148,  1.7820])\n",
      "Epoch 304, Loss 12.533021\n",
      "  params :  tensor([ 3.5209, -6.8505])\n",
      "  grad :  tensor([-0.3143,  1.7790])\n",
      "Epoch 305, Loss 12.500413\n",
      "  params :  tensor([ 3.5241, -6.8683])\n",
      "  grad :  tensor([-0.3137,  1.7759])\n",
      "Epoch 306, Loss 12.467919\n",
      "  params :  tensor([ 3.5272, -6.8860])\n",
      "  grad :  tensor([-0.3132,  1.7729])\n",
      "Epoch 307, Loss 12.435532\n",
      "  params :  tensor([ 3.5303, -6.9037])\n",
      "  grad :  tensor([-0.3127,  1.7699])\n",
      "Epoch 308, Loss 12.403256\n",
      "  params :  tensor([ 3.5335, -6.9213])\n",
      "  grad :  tensor([-0.3121,  1.7669])\n",
      "Epoch 309, Loss 12.371090\n",
      "  params :  tensor([ 3.5366, -6.9390])\n",
      "  grad :  tensor([-0.3116,  1.7639])\n",
      "Epoch 310, Loss 12.339031\n",
      "  params :  tensor([ 3.5397, -6.9566])\n",
      "  grad :  tensor([-0.3111,  1.7609])\n",
      "Epoch 311, Loss 12.307082\n",
      "  params :  tensor([ 3.5428, -6.9742])\n",
      "  grad :  tensor([-0.3105,  1.7579])\n",
      "Epoch 312, Loss 12.275247\n",
      "  params :  tensor([ 3.5459, -6.9917])\n",
      "  grad :  tensor([-0.3100,  1.7549])\n",
      "Epoch 313, Loss 12.243509\n",
      "  params :  tensor([ 3.5490, -7.0092])\n",
      "  grad :  tensor([-0.3095,  1.7519])\n",
      "Epoch 314, Loss 12.211887\n",
      "  params :  tensor([ 3.5521, -7.0267])\n",
      "  grad :  tensor([-0.3090,  1.7490])\n",
      "Epoch 315, Loss 12.180370\n",
      "  params :  tensor([ 3.5552, -7.0442])\n",
      "  grad :  tensor([-0.3084,  1.7460])\n",
      "Epoch 316, Loss 12.148962\n",
      "  params :  tensor([ 3.5582, -7.0616])\n",
      "  grad :  tensor([-0.3079,  1.7430])\n",
      "Epoch 317, Loss 12.117657\n",
      "  params :  tensor([ 3.5613, -7.0790])\n",
      "  grad :  tensor([-0.3074,  1.7401])\n",
      "Epoch 318, Loss 12.086462\n",
      "  params :  tensor([ 3.5644, -7.0964])\n",
      "  grad :  tensor([-0.3069,  1.7371])\n",
      "Epoch 319, Loss 12.055373\n",
      "  params :  tensor([ 3.5674, -7.1137])\n",
      "  grad :  tensor([-0.3063,  1.7342])\n",
      "Epoch 320, Loss 12.024384\n",
      "  params :  tensor([ 3.5705, -7.1310])\n",
      "  grad :  tensor([-0.3058,  1.7312])\n",
      "Epoch 321, Loss 11.993508\n",
      "  params :  tensor([ 3.5736, -7.1483])\n",
      "  grad :  tensor([-0.3053,  1.7283])\n",
      "Epoch 322, Loss 11.962731\n",
      "  params :  tensor([ 3.5766, -7.1656])\n",
      "  grad :  tensor([-0.3048,  1.7253])\n",
      "Epoch 323, Loss 11.932056\n",
      "  params :  tensor([ 3.5796, -7.1828])\n",
      "  grad :  tensor([-0.3043,  1.7224])\n",
      "Epoch 324, Loss 11.901492\n",
      "  params :  tensor([ 3.5827, -7.2000])\n",
      "  grad :  tensor([-0.3037,  1.7195])\n",
      "Epoch 325, Loss 11.871029\n",
      "  params :  tensor([ 3.5857, -7.2172])\n",
      "  grad :  tensor([-0.3032,  1.7166])\n",
      "Epoch 326, Loss 11.840671\n",
      "  params :  tensor([ 3.5887, -7.2343])\n",
      "  grad :  tensor([-0.3027,  1.7136])\n",
      "Epoch 327, Loss 11.810413\n",
      "  params :  tensor([ 3.5918, -7.2514])\n",
      "  grad :  tensor([-0.3022,  1.7107])\n",
      "Epoch 328, Loss 11.780257\n",
      "  params :  tensor([ 3.5948, -7.2685])\n",
      "  grad :  tensor([-0.3017,  1.7078])\n",
      "Epoch 329, Loss 11.750208\n",
      "  params :  tensor([ 3.5978, -7.2855])\n",
      "  grad :  tensor([-0.3012,  1.7049])\n",
      "Epoch 330, Loss 11.720258\n",
      "  params :  tensor([ 3.6008, -7.3026])\n",
      "  grad :  tensor([-0.3007,  1.7020])\n",
      "Epoch 331, Loss 11.690412\n",
      "  params :  tensor([ 3.6038, -7.3196])\n",
      "  grad :  tensor([-0.3002,  1.6991])\n",
      "Epoch 332, Loss 11.660664\n",
      "  params :  tensor([ 3.6068, -7.3365])\n",
      "  grad :  tensor([-0.2996,  1.6963])\n",
      "Epoch 333, Loss 11.631015\n",
      "  params :  tensor([ 3.6098, -7.3535])\n",
      "  grad :  tensor([-0.2991,  1.6934])\n",
      "Epoch 334, Loss 11.601473\n",
      "  params :  tensor([ 3.6128, -7.3704])\n",
      "  grad :  tensor([-0.2986,  1.6905])\n",
      "Epoch 335, Loss 11.572030\n",
      "  params :  tensor([ 3.6158, -7.3872])\n",
      "  grad :  tensor([-0.2981,  1.6876])\n",
      "Epoch 336, Loss 11.542686\n",
      "  params :  tensor([ 3.6187, -7.4041])\n",
      "  grad :  tensor([-0.2976,  1.6848])\n",
      "Epoch 337, Loss 11.513440\n",
      "  params :  tensor([ 3.6217, -7.4209])\n",
      "  grad :  tensor([-0.2971,  1.6819])\n",
      "Epoch 338, Loss 11.484293\n",
      "  params :  tensor([ 3.6247, -7.4377])\n",
      "  grad :  tensor([-0.2966,  1.6790])\n",
      "Epoch 339, Loss 11.455246\n",
      "  params :  tensor([ 3.6276, -7.4545])\n",
      "  grad :  tensor([-0.2961,  1.6762])\n",
      "Epoch 340, Loss 11.426300\n",
      "  params :  tensor([ 3.6306, -7.4712])\n",
      "  grad :  tensor([-0.2956,  1.6733])\n",
      "Epoch 341, Loss 11.397448\n",
      "  params :  tensor([ 3.6335, -7.4879])\n",
      "  grad :  tensor([-0.2951,  1.6705])\n",
      "Epoch 342, Loss 11.368696\n",
      "  params :  tensor([ 3.6365, -7.5046])\n",
      "  grad :  tensor([-0.2946,  1.6677])\n",
      "Epoch 343, Loss 11.340043\n",
      "  params :  tensor([ 3.6394, -7.5212])\n",
      "  grad :  tensor([-0.2941,  1.6648])\n",
      "Epoch 344, Loss 11.311487\n",
      "  params :  tensor([ 3.6424, -7.5378])\n",
      "  grad :  tensor([-0.2936,  1.6620])\n",
      "Epoch 345, Loss 11.283028\n",
      "  params :  tensor([ 3.6453, -7.5544])\n",
      "  grad :  tensor([-0.2931,  1.6592])\n",
      "Epoch 346, Loss 11.254662\n",
      "  params :  tensor([ 3.6482, -7.5710])\n",
      "  grad :  tensor([-0.2926,  1.6564])\n",
      "Epoch 347, Loss 11.226396\n",
      "  params :  tensor([ 3.6511, -7.5875])\n",
      "  grad :  tensor([-0.2921,  1.6535])\n",
      "Epoch 348, Loss 11.198220\n",
      "  params :  tensor([ 3.6541, -7.6040])\n",
      "  grad :  tensor([-0.2916,  1.6507])\n",
      "Epoch 349, Loss 11.170150\n",
      "  params :  tensor([ 3.6570, -7.6205])\n",
      "  grad :  tensor([-0.2911,  1.6479])\n",
      "Epoch 350, Loss 11.142170\n",
      "  params :  tensor([ 3.6599, -7.6370])\n",
      "  grad :  tensor([-0.2906,  1.6451])\n",
      "Epoch 351, Loss 11.114282\n",
      "  params :  tensor([ 3.6628, -7.6534])\n",
      "  grad :  tensor([-0.2901,  1.6423])\n",
      "Epoch 352, Loss 11.086491\n",
      "  params :  tensor([ 3.6657, -7.6698])\n",
      "  grad :  tensor([-0.2896,  1.6395])\n",
      "Epoch 353, Loss 11.058797\n",
      "  params :  tensor([ 3.6686, -7.6861])\n",
      "  grad :  tensor([-0.2892,  1.6368])\n",
      "Epoch 354, Loss 11.031193\n",
      "  params :  tensor([ 3.6714, -7.7025])\n",
      "  grad :  tensor([-0.2886,  1.6340])\n",
      "Epoch 355, Loss 11.003686\n",
      "  params :  tensor([ 3.6743, -7.7188])\n",
      "  grad :  tensor([-0.2882,  1.6312])\n",
      "Epoch 356, Loss 10.976270\n",
      "  params :  tensor([ 3.6772, -7.7351])\n",
      "  grad :  tensor([-0.2877,  1.6284])\n",
      "Epoch 357, Loss 10.948948\n",
      "  params :  tensor([ 3.6801, -7.7513])\n",
      "  grad :  tensor([-0.2872,  1.6257])\n",
      "Epoch 358, Loss 10.921719\n",
      "  params :  tensor([ 3.6829, -7.7676])\n",
      "  grad :  tensor([-0.2867,  1.6229])\n",
      "Epoch 359, Loss 10.894581\n",
      "  params :  tensor([ 3.6858, -7.7838])\n",
      "  grad :  tensor([-0.2862,  1.6201])\n",
      "Epoch 360, Loss 10.867537\n",
      "  params :  tensor([ 3.6887, -7.7999])\n",
      "  grad :  tensor([-0.2857,  1.6174])\n",
      "Epoch 361, Loss 10.840583\n",
      "  params :  tensor([ 3.6915, -7.8161])\n",
      "  grad :  tensor([-0.2852,  1.6146])\n",
      "Epoch 362, Loss 10.813721\n",
      "  params :  tensor([ 3.6944, -7.8322])\n",
      "  grad :  tensor([-0.2847,  1.6119])\n",
      "Epoch 363, Loss 10.786950\n",
      "  params :  tensor([ 3.6972, -7.8483])\n",
      "  grad :  tensor([-0.2843,  1.6092])\n",
      "Epoch 364, Loss 10.760270\n",
      "  params :  tensor([ 3.7000, -7.8644])\n",
      "  grad :  tensor([-0.2838,  1.6064])\n",
      "Epoch 365, Loss 10.733681\n",
      "  params :  tensor([ 3.7029, -7.8804])\n",
      "  grad :  tensor([-0.2833,  1.6037])\n",
      "Epoch 366, Loss 10.707184\n",
      "  params :  tensor([ 3.7057, -7.8964])\n",
      "  grad :  tensor([-0.2828,  1.6010])\n",
      "Epoch 367, Loss 10.680775\n",
      "  params :  tensor([ 3.7085, -7.9124])\n",
      "  grad :  tensor([-0.2823,  1.5983])\n",
      "Epoch 368, Loss 10.654454\n",
      "  params :  tensor([ 3.7113, -7.9284])\n",
      "  grad :  tensor([-0.2819,  1.5955])\n",
      "Epoch 369, Loss 10.628225\n",
      "  params :  tensor([ 3.7142, -7.9443])\n",
      "  grad :  tensor([-0.2814,  1.5928])\n",
      "Epoch 370, Loss 10.602086\n",
      "  params :  tensor([ 3.7170, -7.9602])\n",
      "  grad :  tensor([-0.2809,  1.5901])\n",
      "Epoch 371, Loss 10.576034\n",
      "  params :  tensor([ 3.7198, -7.9761])\n",
      "  grad :  tensor([-0.2804,  1.5874])\n",
      "Epoch 372, Loss 10.550071\n",
      "  params :  tensor([ 3.7226, -7.9919])\n",
      "  grad :  tensor([-0.2799,  1.5847])\n",
      "Epoch 373, Loss 10.524194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([ 3.7254, -8.0077])\n",
      "  grad :  tensor([-0.2795,  1.5820])\n",
      "Epoch 374, Loss 10.498409\n",
      "  params :  tensor([ 3.7282, -8.0235])\n",
      "  grad :  tensor([-0.2790,  1.5794])\n",
      "Epoch 375, Loss 10.472707\n",
      "  params :  tensor([ 3.7309, -8.0393])\n",
      "  grad :  tensor([-0.2785,  1.5767])\n",
      "Epoch 376, Loss 10.447093\n",
      "  params :  tensor([ 3.7337, -8.0550])\n",
      "  grad :  tensor([-0.2780,  1.5740])\n",
      "Epoch 377, Loss 10.421569\n",
      "  params :  tensor([ 3.7365, -8.0707])\n",
      "  grad :  tensor([-0.2776,  1.5713])\n",
      "Epoch 378, Loss 10.396132\n",
      "  params :  tensor([ 3.7393, -8.0864])\n",
      "  grad :  tensor([-0.2771,  1.5686])\n",
      "Epoch 379, Loss 10.370779\n",
      "  params :  tensor([ 3.7420, -8.1021])\n",
      "  grad :  tensor([-0.2766,  1.5660])\n",
      "Epoch 380, Loss 10.345510\n",
      "  params :  tensor([ 3.7448, -8.1177])\n",
      "  grad :  tensor([-0.2762,  1.5633])\n",
      "Epoch 381, Loss 10.320328\n",
      "  params :  tensor([ 3.7476, -8.1333])\n",
      "  grad :  tensor([-0.2757,  1.5607])\n",
      "Epoch 382, Loss 10.295234\n",
      "  params :  tensor([ 3.7503, -8.1489])\n",
      "  grad :  tensor([-0.2752,  1.5580])\n",
      "Epoch 383, Loss 10.270224\n",
      "  params :  tensor([ 3.7531, -8.1645])\n",
      "  grad :  tensor([-0.2748,  1.5554])\n",
      "Epoch 384, Loss 10.245296\n",
      "  params :  tensor([ 3.7558, -8.1800])\n",
      "  grad :  tensor([-0.2743,  1.5527])\n",
      "Epoch 385, Loss 10.220457\n",
      "  params :  tensor([ 3.7585, -8.1955])\n",
      "  grad :  tensor([-0.2738,  1.5501])\n",
      "Epoch 386, Loss 10.195701\n",
      "  params :  tensor([ 3.7613, -8.2110])\n",
      "  grad :  tensor([-0.2734,  1.5475])\n",
      "Epoch 387, Loss 10.171029\n",
      "  params :  tensor([ 3.7640, -8.2264])\n",
      "  grad :  tensor([-0.2729,  1.5448])\n",
      "Epoch 388, Loss 10.146438\n",
      "  params :  tensor([ 3.7667, -8.2418])\n",
      "  grad :  tensor([-0.2724,  1.5422])\n",
      "Epoch 389, Loss 10.121935\n",
      "  params :  tensor([ 3.7694, -8.2572])\n",
      "  grad :  tensor([-0.2720,  1.5396])\n",
      "Epoch 390, Loss 10.097512\n",
      "  params :  tensor([ 3.7722, -8.2726])\n",
      "  grad :  tensor([-0.2715,  1.5370])\n",
      "Epoch 391, Loss 10.073173\n",
      "  params :  tensor([ 3.7749, -8.2879])\n",
      "  grad :  tensor([-0.2711,  1.5344])\n",
      "Epoch 392, Loss 10.048919\n",
      "  params :  tensor([ 3.7776, -8.3033])\n",
      "  grad :  tensor([-0.2706,  1.5317])\n",
      "Epoch 393, Loss 10.024743\n",
      "  params :  tensor([ 3.7803, -8.3185])\n",
      "  grad :  tensor([-0.2701,  1.5291])\n",
      "Epoch 394, Loss 10.000652\n",
      "  params :  tensor([ 3.7830, -8.3338])\n",
      "  grad :  tensor([-0.2697,  1.5265])\n",
      "Epoch 395, Loss 9.976640\n",
      "  params :  tensor([ 3.7857, -8.3491])\n",
      "  grad :  tensor([-0.2692,  1.5240])\n",
      "Epoch 396, Loss 9.952712\n",
      "  params :  tensor([ 3.7884, -8.3643])\n",
      "  grad :  tensor([-0.2688,  1.5214])\n",
      "Epoch 397, Loss 9.928862\n",
      "  params :  tensor([ 3.7910, -8.3795])\n",
      "  grad :  tensor([-0.2683,  1.5188])\n",
      "Epoch 398, Loss 9.905093\n",
      "  params :  tensor([ 3.7937, -8.3946])\n",
      "  grad :  tensor([-0.2678,  1.5162])\n",
      "Epoch 399, Loss 9.881409\n",
      "  params :  tensor([ 3.7964, -8.4098])\n",
      "  grad :  tensor([-0.2674,  1.5136])\n",
      "Epoch 400, Loss 9.857804\n",
      "  params :  tensor([ 3.7991, -8.4249])\n",
      "  grad :  tensor([-0.2669,  1.5111])\n",
      "Epoch 401, Loss 9.834277\n",
      "  params :  tensor([ 3.8017, -8.4399])\n",
      "  grad :  tensor([-0.2665,  1.5085])\n",
      "Epoch 402, Loss 9.810831\n",
      "  params :  tensor([ 3.8044, -8.4550])\n",
      "  grad :  tensor([-0.2660,  1.5059])\n",
      "Epoch 403, Loss 9.787466\n",
      "  params :  tensor([ 3.8070, -8.4700])\n",
      "  grad :  tensor([-0.2656,  1.5034])\n",
      "Epoch 404, Loss 9.764176\n",
      "  params :  tensor([ 3.8097, -8.4851])\n",
      "  grad :  tensor([-0.2651,  1.5008])\n",
      "Epoch 405, Loss 9.740973\n",
      "  params :  tensor([ 3.8123, -8.5000])\n",
      "  grad :  tensor([-0.2647,  1.4983])\n",
      "Epoch 406, Loss 9.717843\n",
      "  params :  tensor([ 3.8150, -8.5150])\n",
      "  grad :  tensor([-0.2642,  1.4957])\n",
      "Epoch 407, Loss 9.694793\n",
      "  params :  tensor([ 3.8176, -8.5299])\n",
      "  grad :  tensor([-0.2638,  1.4932])\n",
      "Epoch 408, Loss 9.671824\n",
      "  params :  tensor([ 3.8202, -8.5448])\n",
      "  grad :  tensor([-0.2633,  1.4906])\n",
      "Epoch 409, Loss 9.648926\n",
      "  params :  tensor([ 3.8229, -8.5597])\n",
      "  grad :  tensor([-0.2629,  1.4881])\n",
      "Epoch 410, Loss 9.626110\n",
      "  params :  tensor([ 3.8255, -8.5746])\n",
      "  grad :  tensor([-0.2624,  1.4856])\n",
      "Epoch 411, Loss 9.603373\n",
      "  params :  tensor([ 3.8281, -8.5894])\n",
      "  grad :  tensor([-0.2620,  1.4831])\n",
      "Epoch 412, Loss 9.580709\n",
      "  params :  tensor([ 3.8307, -8.6042])\n",
      "  grad :  tensor([-0.2615,  1.4805])\n",
      "Epoch 413, Loss 9.558125\n",
      "  params :  tensor([ 3.8333, -8.6190])\n",
      "  grad :  tensor([-0.2611,  1.4780])\n",
      "Epoch 414, Loss 9.535617\n",
      "  params :  tensor([ 3.8360, -8.6337])\n",
      "  grad :  tensor([-0.2606,  1.4755])\n",
      "Epoch 415, Loss 9.513184\n",
      "  params :  tensor([ 3.8386, -8.6485])\n",
      "  grad :  tensor([-0.2602,  1.4730])\n",
      "Epoch 416, Loss 9.490829\n",
      "  params :  tensor([ 3.8412, -8.6632])\n",
      "  grad :  tensor([-0.2598,  1.4705])\n",
      "Epoch 417, Loss 9.468551\n",
      "  params :  tensor([ 3.8437, -8.6779])\n",
      "  grad :  tensor([-0.2593,  1.4680])\n",
      "Epoch 418, Loss 9.446347\n",
      "  params :  tensor([ 3.8463, -8.6925])\n",
      "  grad :  tensor([-0.2589,  1.4655])\n",
      "Epoch 419, Loss 9.424216\n",
      "  params :  tensor([ 3.8489, -8.7071])\n",
      "  grad :  tensor([-0.2584,  1.4630])\n",
      "Epoch 420, Loss 9.402164\n",
      "  params :  tensor([ 3.8515, -8.7217])\n",
      "  grad :  tensor([-0.2580,  1.4605])\n",
      "Epoch 421, Loss 9.380184\n",
      "  params :  tensor([ 3.8541, -8.7363])\n",
      "  grad :  tensor([-0.2576,  1.4581])\n",
      "Epoch 422, Loss 9.358282\n",
      "  params :  tensor([ 3.8566, -8.7509])\n",
      "  grad :  tensor([-0.2571,  1.4556])\n",
      "Epoch 423, Loss 9.336448\n",
      "  params :  tensor([ 3.8592, -8.7654])\n",
      "  grad :  tensor([-0.2567,  1.4531])\n",
      "Epoch 424, Loss 9.314695\n",
      "  params :  tensor([ 3.8618, -8.7799])\n",
      "  grad :  tensor([-0.2563,  1.4506])\n",
      "Epoch 425, Loss 9.293012\n",
      "  params :  tensor([ 3.8643, -8.7944])\n",
      "  grad :  tensor([-0.2558,  1.4482])\n",
      "Epoch 426, Loss 9.271403\n",
      "  params :  tensor([ 3.8669, -8.8089])\n",
      "  grad :  tensor([-0.2554,  1.4457])\n",
      "Epoch 427, Loss 9.249871\n",
      "  params :  tensor([ 3.8694, -8.8233])\n",
      "  grad :  tensor([-0.2550,  1.4433])\n",
      "Epoch 428, Loss 9.228410\n",
      "  params :  tensor([ 3.8720, -8.8377])\n",
      "  grad :  tensor([-0.2545,  1.4408])\n",
      "Epoch 429, Loss 9.207022\n",
      "  params :  tensor([ 3.8745, -8.8521])\n",
      "  grad :  tensor([-0.2541,  1.4384])\n",
      "Epoch 430, Loss 9.185704\n",
      "  params :  tensor([ 3.8771, -8.8664])\n",
      "  grad :  tensor([-0.2537,  1.4359])\n",
      "Epoch 431, Loss 9.164462\n",
      "  params :  tensor([ 3.8796, -8.8808])\n",
      "  grad :  tensor([-0.2532,  1.4335])\n",
      "Epoch 432, Loss 9.143289\n",
      "  params :  tensor([ 3.8821, -8.8951])\n",
      "  grad :  tensor([-0.2528,  1.4310])\n",
      "Epoch 433, Loss 9.122189\n",
      "  params :  tensor([ 3.8846, -8.9094])\n",
      "  grad :  tensor([-0.2524,  1.4286])\n",
      "Epoch 434, Loss 9.101160\n",
      "  params :  tensor([ 3.8872, -8.9236])\n",
      "  grad :  tensor([-0.2519,  1.4262])\n",
      "Epoch 435, Loss 9.080204\n",
      "  params :  tensor([ 3.8897, -8.9379])\n",
      "  grad :  tensor([-0.2515,  1.4238])\n",
      "Epoch 436, Loss 9.059318\n",
      "  params :  tensor([ 3.8922, -8.9521])\n",
      "  grad :  tensor([-0.2511,  1.4213])\n",
      "Epoch 437, Loss 9.038502\n",
      "  params :  tensor([ 3.8947, -8.9663])\n",
      "  grad :  tensor([-0.2507,  1.4189])\n",
      "Epoch 438, Loss 9.017757\n",
      "  params :  tensor([ 3.8972, -8.9804])\n",
      "  grad :  tensor([-0.2502,  1.4165])\n",
      "Epoch 439, Loss 8.997084\n",
      "  params :  tensor([ 3.8997, -8.9946])\n",
      "  grad :  tensor([-0.2498,  1.4141])\n",
      "Epoch 440, Loss 8.976479\n",
      "  params :  tensor([ 3.9022, -9.0087])\n",
      "  grad :  tensor([-0.2494,  1.4117])\n",
      "Epoch 441, Loss 8.955944\n",
      "  params :  tensor([ 3.9047, -9.0228])\n",
      "  grad :  tensor([-0.2489,  1.4093])\n",
      "Epoch 442, Loss 8.935480\n",
      "  params :  tensor([ 3.9072, -9.0369])\n",
      "  grad :  tensor([-0.2485,  1.4069])\n",
      "Epoch 443, Loss 8.915089\n",
      "  params :  tensor([ 3.9096, -9.0509])\n",
      "  grad :  tensor([-0.2481,  1.4045])\n",
      "Epoch 444, Loss 8.894762\n",
      "  params :  tensor([ 3.9121, -9.0649])\n",
      "  grad :  tensor([-0.2477,  1.4021])\n",
      "Epoch 445, Loss 8.874508\n",
      "  params :  tensor([ 3.9146, -9.0789])\n",
      "  grad :  tensor([-0.2473,  1.3998])\n",
      "Epoch 446, Loss 8.854318\n",
      "  params :  tensor([ 3.9171, -9.0929])\n",
      "  grad :  tensor([-0.2468,  1.3974])\n",
      "Epoch 447, Loss 8.834197\n",
      "  params :  tensor([ 3.9195, -9.1068])\n",
      "  grad :  tensor([-0.2464,  1.3950])\n",
      "Epoch 448, Loss 8.814149\n",
      "  params :  tensor([ 3.9220, -9.1208])\n",
      "  grad :  tensor([-0.2460,  1.3926])\n",
      "Epoch 449, Loss 8.794162\n",
      "  params :  tensor([ 3.9244, -9.1347])\n",
      "  grad :  tensor([-0.2456,  1.3903])\n",
      "Epoch 450, Loss 8.774253\n",
      "  params :  tensor([ 3.9269, -9.1486])\n",
      "  grad :  tensor([-0.2452,  1.3879])\n",
      "Epoch 451, Loss 8.754405\n",
      "  params :  tensor([ 3.9293, -9.1624])\n",
      "  grad :  tensor([-0.2448,  1.3856])\n",
      "Epoch 452, Loss 8.734623\n",
      "  params :  tensor([ 3.9318, -9.1762])\n",
      "  grad :  tensor([-0.2443,  1.3832])\n",
      "Epoch 453, Loss 8.714911\n",
      "  params :  tensor([ 3.9342, -9.1901])\n",
      "  grad :  tensor([-0.2439,  1.3808])\n",
      "Epoch 454, Loss 8.695266\n",
      "  params :  tensor([ 3.9367, -9.2038])\n",
      "  grad :  tensor([-0.2435,  1.3785])\n",
      "Epoch 455, Loss 8.675688\n",
      "  params :  tensor([ 3.9391, -9.2176])\n",
      "  grad :  tensor([-0.2431,  1.3762])\n",
      "Epoch 456, Loss 8.656173\n",
      "  params :  tensor([ 3.9415, -9.2313])\n",
      "  grad :  tensor([-0.2427,  1.3738])\n",
      "Epoch 457, Loss 8.636729\n",
      "  params :  tensor([ 3.9439, -9.2451])\n",
      "  grad :  tensor([-0.2423,  1.3715])\n",
      "Epoch 458, Loss 8.617347\n",
      "  params :  tensor([ 3.9464, -9.2587])\n",
      "  grad :  tensor([-0.2419,  1.3692])\n",
      "Epoch 459, Loss 8.598029\n",
      "  params :  tensor([ 3.9488, -9.2724])\n",
      "  grad :  tensor([-0.2414,  1.3668])\n",
      "Epoch 460, Loss 8.578781\n",
      "  params :  tensor([ 3.9512, -9.2861])\n",
      "  grad :  tensor([-0.2410,  1.3645])\n",
      "Epoch 461, Loss 8.559597\n",
      "  params :  tensor([ 3.9536, -9.2997])\n",
      "  grad :  tensor([-0.2406,  1.3622])\n",
      "Epoch 462, Loss 8.540479\n",
      "  params :  tensor([ 3.9560, -9.3133])\n",
      "  grad :  tensor([-0.2402,  1.3599])\n",
      "Epoch 463, Loss 8.521426\n",
      "  params :  tensor([ 3.9584, -9.3269])\n",
      "  grad :  tensor([-0.2398,  1.3576])\n",
      "Epoch 464, Loss 8.502437\n",
      "  params :  tensor([ 3.9608, -9.3404])\n",
      "  grad :  tensor([-0.2394,  1.3553])\n",
      "Epoch 465, Loss 8.483517\n",
      "  params :  tensor([ 3.9632, -9.3539])\n",
      "  grad :  tensor([-0.2390,  1.3530])\n",
      "Epoch 466, Loss 8.464652\n",
      "  params :  tensor([ 3.9656, -9.3674])\n",
      "  grad :  tensor([-0.2386,  1.3507])\n",
      "Epoch 467, Loss 8.445858\n",
      "  params :  tensor([ 3.9679, -9.3809])\n",
      "  grad :  tensor([-0.2382,  1.3484])\n",
      "Epoch 468, Loss 8.427128\n",
      "  params :  tensor([ 3.9703, -9.3944])\n",
      "  grad :  tensor([-0.2378,  1.3461])\n",
      "Epoch 469, Loss 8.408454\n",
      "  params :  tensor([ 3.9727, -9.4078])\n",
      "  grad :  tensor([-0.2374,  1.3438])\n",
      "Epoch 470, Loss 8.389848\n",
      "  params :  tensor([ 3.9751, -9.4212])\n",
      "  grad :  tensor([-0.2370,  1.3415])\n",
      "Epoch 471, Loss 8.371305\n",
      "  params :  tensor([ 3.9774, -9.4346])\n",
      "  grad :  tensor([-0.2366,  1.3392])\n",
      "Epoch 472, Loss 8.352828\n",
      "  params :  tensor([ 3.9798, -9.4480])\n",
      "  grad :  tensor([-0.2362,  1.3370])\n",
      "Epoch 473, Loss 8.334409\n",
      "  params :  tensor([ 3.9822, -9.4614])\n",
      "  grad :  tensor([-0.2358,  1.3347])\n",
      "Epoch 474, Loss 8.316054\n",
      "  params :  tensor([ 3.9845, -9.4747])\n",
      "  grad :  tensor([-0.2354,  1.3324])\n",
      "Epoch 475, Loss 8.297764\n",
      "  params :  tensor([ 3.9869, -9.4880])\n",
      "  grad :  tensor([-0.2350,  1.3301])\n",
      "Epoch 476, Loss 8.279534\n",
      "  params :  tensor([ 3.9892, -9.5013])\n",
      "  grad :  tensor([-0.2346,  1.3279])\n",
      "Epoch 477, Loss 8.261369\n",
      "  params :  tensor([ 3.9915, -9.5145])\n",
      "  grad :  tensor([-0.2342,  1.3256])\n",
      "Epoch 478, Loss 8.243259\n",
      "  params :  tensor([ 3.9939, -9.5277])\n",
      "  grad :  tensor([-0.2338,  1.3234])\n",
      "Epoch 479, Loss 8.225213\n",
      "  params :  tensor([ 3.9962, -9.5410])\n",
      "  grad :  tensor([-0.2334,  1.3211])\n",
      "Epoch 480, Loss 8.207231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([ 3.9985, -9.5541])\n",
      "  grad :  tensor([-0.2330,  1.3189])\n",
      "Epoch 481, Loss 8.189310\n",
      "  params :  tensor([ 4.0009, -9.5673])\n",
      "  grad :  tensor([-0.2326,  1.3166])\n",
      "Epoch 482, Loss 8.171452\n",
      "  params :  tensor([ 4.0032, -9.5805])\n",
      "  grad :  tensor([-0.2322,  1.3144])\n",
      "Epoch 483, Loss 8.153647\n",
      "  params :  tensor([ 4.0055, -9.5936])\n",
      "  grad :  tensor([-0.2318,  1.3122])\n",
      "Epoch 484, Loss 8.135906\n",
      "  params :  tensor([ 4.0078, -9.6067])\n",
      "  grad :  tensor([-0.2314,  1.3100])\n",
      "Epoch 485, Loss 8.118226\n",
      "  params :  tensor([ 4.0101, -9.6198])\n",
      "  grad :  tensor([-0.2310,  1.3077])\n",
      "Epoch 486, Loss 8.100607\n",
      "  params :  tensor([ 4.0124, -9.6328])\n",
      "  grad :  tensor([-0.2306,  1.3055])\n",
      "Epoch 487, Loss 8.083045\n",
      "  params :  tensor([ 4.0147, -9.6458])\n",
      "  grad :  tensor([-0.2302,  1.3033])\n",
      "Epoch 488, Loss 8.065548\n",
      "  params :  tensor([ 4.0170, -9.6589])\n",
      "  grad :  tensor([-0.2298,  1.3011])\n",
      "Epoch 489, Loss 8.048104\n",
      "  params :  tensor([ 4.0193, -9.6718])\n",
      "  grad :  tensor([-0.2295,  1.2989])\n",
      "Epoch 490, Loss 8.030724\n",
      "  params :  tensor([ 4.0216, -9.6848])\n",
      "  grad :  tensor([-0.2291,  1.2967])\n",
      "Epoch 491, Loss 8.013401\n",
      "  params :  tensor([ 4.0239, -9.6978])\n",
      "  grad :  tensor([-0.2287,  1.2945])\n",
      "Epoch 492, Loss 7.996137\n",
      "  params :  tensor([ 4.0262, -9.7107])\n",
      "  grad :  tensor([-0.2283,  1.2923])\n",
      "Epoch 493, Loss 7.978930\n",
      "  params :  tensor([ 4.0285, -9.7236])\n",
      "  grad :  tensor([-0.2279,  1.2901])\n",
      "Epoch 494, Loss 7.961783\n",
      "  params :  tensor([ 4.0308, -9.7365])\n",
      "  grad :  tensor([-0.2275,  1.2879])\n",
      "Epoch 495, Loss 7.944690\n",
      "  params :  tensor([ 4.0330, -9.7493])\n",
      "  grad :  tensor([-0.2271,  1.2857])\n",
      "Epoch 496, Loss 7.927663\n",
      "  params :  tensor([ 4.0353, -9.7621])\n",
      "  grad :  tensor([-0.2267,  1.2835])\n",
      "Epoch 497, Loss 7.910690\n",
      "  params :  tensor([ 4.0376, -9.7750])\n",
      "  grad :  tensor([-0.2263,  1.2813])\n",
      "Epoch 498, Loss 7.893775\n",
      "  params :  tensor([ 4.0398, -9.7878])\n",
      "  grad :  tensor([-0.2260,  1.2791])\n",
      "Epoch 499, Loss 7.876915\n",
      "  params :  tensor([ 4.0421, -9.8005])\n",
      "  grad :  tensor([-0.2256,  1.2770])\n",
      "Epoch 500, Loss 7.860115\n",
      "  params :  tensor([ 4.0443, -9.8133])\n",
      "  grad :  tensor([-0.2252,  1.2748])\n",
      "Epoch 501, Loss 7.843369\n",
      "  params :  tensor([ 4.0466, -9.8260])\n",
      "  grad :  tensor([-0.2248,  1.2726])\n",
      "Epoch 502, Loss 7.826683\n",
      "  params :  tensor([ 4.0488, -9.8387])\n",
      "  grad :  tensor([-0.2244,  1.2705])\n",
      "Epoch 503, Loss 7.810053\n",
      "  params :  tensor([ 4.0511, -9.8514])\n",
      "  grad :  tensor([-0.2241,  1.2683])\n",
      "Epoch 504, Loss 7.793481\n",
      "  params :  tensor([ 4.0533, -9.8640])\n",
      "  grad :  tensor([-0.2237,  1.2662])\n",
      "Epoch 505, Loss 7.776962\n",
      "  params :  tensor([ 4.0555, -9.8767])\n",
      "  grad :  tensor([-0.2233,  1.2640])\n",
      "Epoch 506, Loss 7.760498\n",
      "  params :  tensor([ 4.0578, -9.8893])\n",
      "  grad :  tensor([-0.2229,  1.2619])\n",
      "Epoch 507, Loss 7.744092\n",
      "  params :  tensor([ 4.0600, -9.9019])\n",
      "  grad :  tensor([-0.2225,  1.2597])\n",
      "Epoch 508, Loss 7.727745\n",
      "  params :  tensor([ 4.0622, -9.9145])\n",
      "  grad :  tensor([-0.2222,  1.2576])\n",
      "Epoch 509, Loss 7.711449\n",
      "  params :  tensor([ 4.0644, -9.9270])\n",
      "  grad :  tensor([-0.2218,  1.2554])\n",
      "Epoch 510, Loss 7.695211\n",
      "  params :  tensor([ 4.0666, -9.9396])\n",
      "  grad :  tensor([-0.2214,  1.2533])\n",
      "Epoch 511, Loss 7.679024\n",
      "  params :  tensor([ 4.0688, -9.9521])\n",
      "  grad :  tensor([-0.2210,  1.2512])\n",
      "Epoch 512, Loss 7.662896\n",
      "  params :  tensor([ 4.0710, -9.9646])\n",
      "  grad :  tensor([-0.2207,  1.2490])\n",
      "Epoch 513, Loss 7.646820\n",
      "  params :  tensor([ 4.0733, -9.9770])\n",
      "  grad :  tensor([-0.2203,  1.2469])\n",
      "Epoch 514, Loss 7.630803\n",
      "  params :  tensor([ 4.0754, -9.9895])\n",
      "  grad :  tensor([-0.2199,  1.2448])\n",
      "Epoch 515, Loss 7.614836\n",
      "  params :  tensor([  4.0776, -10.0019])\n",
      "  grad :  tensor([-0.2195,  1.2427])\n",
      "Epoch 516, Loss 7.598925\n",
      "  params :  tensor([  4.0798, -10.0143])\n",
      "  grad :  tensor([-0.2192,  1.2406])\n",
      "Epoch 517, Loss 7.583069\n",
      "  params :  tensor([  4.0820, -10.0267])\n",
      "  grad :  tensor([-0.2188,  1.2385])\n",
      "Epoch 518, Loss 7.567265\n",
      "  params :  tensor([  4.0842, -10.0391])\n",
      "  grad :  tensor([-0.2184,  1.2364])\n",
      "Epoch 519, Loss 7.551515\n",
      "  params :  tensor([  4.0864, -10.0514])\n",
      "  grad :  tensor([-0.2180,  1.2343])\n",
      "Epoch 520, Loss 7.535818\n",
      "  params :  tensor([  4.0886, -10.0637])\n",
      "  grad :  tensor([-0.2177,  1.2322])\n",
      "Epoch 521, Loss 7.520176\n",
      "  params :  tensor([  4.0907, -10.0760])\n",
      "  grad :  tensor([-0.2173,  1.2301])\n",
      "Epoch 522, Loss 7.504587\n",
      "  params :  tensor([  4.0929, -10.0883])\n",
      "  grad :  tensor([-0.2169,  1.2280])\n",
      "Epoch 523, Loss 7.489048\n",
      "  params :  tensor([  4.0951, -10.1006])\n",
      "  grad :  tensor([-0.2165,  1.2259])\n",
      "Epoch 524, Loss 7.473566\n",
      "  params :  tensor([  4.0972, -10.1128])\n",
      "  grad :  tensor([-0.2162,  1.2238])\n",
      "Epoch 525, Loss 7.458135\n",
      "  params :  tensor([  4.0994, -10.1250])\n",
      "  grad :  tensor([-0.2158,  1.2217])\n",
      "Epoch 526, Loss 7.442750\n",
      "  params :  tensor([  4.1015, -10.1372])\n",
      "  grad :  tensor([-0.2155,  1.2197])\n",
      "Epoch 527, Loss 7.427427\n",
      "  params :  tensor([  4.1037, -10.1494])\n",
      "  grad :  tensor([-0.2151,  1.2176])\n",
      "Epoch 528, Loss 7.412152\n",
      "  params :  tensor([  4.1058, -10.1616])\n",
      "  grad :  tensor([-0.2147,  1.2155])\n",
      "Epoch 529, Loss 7.396928\n",
      "  params :  tensor([  4.1080, -10.1737])\n",
      "  grad :  tensor([-0.2144,  1.2135])\n",
      "Epoch 530, Loss 7.381757\n",
      "  params :  tensor([  4.1101, -10.1858])\n",
      "  grad :  tensor([-0.2140,  1.2114])\n",
      "Epoch 531, Loss 7.366637\n",
      "  params :  tensor([  4.1123, -10.1979])\n",
      "  grad :  tensor([-0.2136,  1.2093])\n",
      "Epoch 532, Loss 7.351567\n",
      "  params :  tensor([  4.1144, -10.2100])\n",
      "  grad :  tensor([-0.2133,  1.2073])\n",
      "Epoch 533, Loss 7.336549\n",
      "  params :  tensor([  4.1165, -10.2220])\n",
      "  grad :  tensor([-0.2129,  1.2052])\n",
      "Epoch 534, Loss 7.321584\n",
      "  params :  tensor([  4.1187, -10.2340])\n",
      "  grad :  tensor([-0.2125,  1.2032])\n",
      "Epoch 535, Loss 7.306671\n",
      "  params :  tensor([  4.1208, -10.2461])\n",
      "  grad :  tensor([-0.2122,  1.2012])\n",
      "Epoch 536, Loss 7.291804\n",
      "  params :  tensor([  4.1229, -10.2581])\n",
      "  grad :  tensor([-0.2118,  1.1991])\n",
      "Epoch 537, Loss 7.276989\n",
      "  params :  tensor([  4.1250, -10.2700])\n",
      "  grad :  tensor([-0.2115,  1.1971])\n",
      "Epoch 538, Loss 7.262227\n",
      "  params :  tensor([  4.1271, -10.2820])\n",
      "  grad :  tensor([-0.2111,  1.1950])\n",
      "Epoch 539, Loss 7.247512\n",
      "  params :  tensor([  4.1292, -10.2939])\n",
      "  grad :  tensor([-0.2108,  1.1930])\n",
      "Epoch 540, Loss 7.232845\n",
      "  params :  tensor([  4.1313, -10.3058])\n",
      "  grad :  tensor([-0.2104,  1.1910])\n",
      "Epoch 541, Loss 7.218231\n",
      "  params :  tensor([  4.1334, -10.3177])\n",
      "  grad :  tensor([-0.2100,  1.1890])\n",
      "Epoch 542, Loss 7.203665\n",
      "  params :  tensor([  4.1355, -10.3296])\n",
      "  grad :  tensor([-0.2097,  1.1869])\n",
      "Epoch 543, Loss 7.189151\n",
      "  params :  tensor([  4.1376, -10.3414])\n",
      "  grad :  tensor([-0.2093,  1.1849])\n",
      "Epoch 544, Loss 7.174683\n",
      "  params :  tensor([  4.1397, -10.3533])\n",
      "  grad :  tensor([-0.2090,  1.1829])\n",
      "Epoch 545, Loss 7.160266\n",
      "  params :  tensor([  4.1418, -10.3651])\n",
      "  grad :  tensor([-0.2086,  1.1809])\n",
      "Epoch 546, Loss 7.145897\n",
      "  params :  tensor([  4.1439, -10.3769])\n",
      "  grad :  tensor([-0.2083,  1.1789])\n",
      "Epoch 547, Loss 7.131581\n",
      "  params :  tensor([  4.1460, -10.3886])\n",
      "  grad :  tensor([-0.2079,  1.1769])\n",
      "Epoch 548, Loss 7.117305\n",
      "  params :  tensor([  4.1480, -10.4004])\n",
      "  grad :  tensor([-0.2075,  1.1749])\n",
      "Epoch 549, Loss 7.103083\n",
      "  params :  tensor([  4.1501, -10.4121])\n",
      "  grad :  tensor([-0.2072,  1.1729])\n",
      "Epoch 550, Loss 7.088911\n",
      "  params :  tensor([  4.1522, -10.4238])\n",
      "  grad :  tensor([-0.2068,  1.1709])\n",
      "Epoch 551, Loss 7.074785\n",
      "  params :  tensor([  4.1542, -10.4355])\n",
      "  grad :  tensor([-0.2065,  1.1689])\n",
      "Epoch 552, Loss 7.060707\n",
      "  params :  tensor([  4.1563, -10.4472])\n",
      "  grad :  tensor([-0.2062,  1.1669])\n",
      "Epoch 553, Loss 7.046676\n",
      "  params :  tensor([  4.1584, -10.4588])\n",
      "  grad :  tensor([-0.2058,  1.1649])\n",
      "Epoch 554, Loss 7.032695\n",
      "  params :  tensor([  4.1604, -10.4704])\n",
      "  grad :  tensor([-0.2054,  1.1630])\n",
      "Epoch 555, Loss 7.018755\n",
      "  params :  tensor([  4.1625, -10.4821])\n",
      "  grad :  tensor([-0.2051,  1.1610])\n",
      "Epoch 556, Loss 7.004870\n",
      "  params :  tensor([  4.1645, -10.4936])\n",
      "  grad :  tensor([-0.2047,  1.1590])\n",
      "Epoch 557, Loss 6.991028\n",
      "  params :  tensor([  4.1666, -10.5052])\n",
      "  grad :  tensor([-0.2044,  1.1571])\n",
      "Epoch 558, Loss 6.977232\n",
      "  params :  tensor([  4.1686, -10.5168])\n",
      "  grad :  tensor([-0.2041,  1.1551])\n",
      "Epoch 559, Loss 6.963488\n",
      "  params :  tensor([  4.1706, -10.5283])\n",
      "  grad :  tensor([-0.2037,  1.1531])\n",
      "Epoch 560, Loss 6.949787\n",
      "  params :  tensor([  4.1727, -10.5398])\n",
      "  grad :  tensor([-0.2034,  1.1512])\n",
      "Epoch 561, Loss 6.936135\n",
      "  params :  tensor([  4.1747, -10.5513])\n",
      "  grad :  tensor([-0.2030,  1.1492])\n",
      "Epoch 562, Loss 6.922528\n",
      "  params :  tensor([  4.1767, -10.5628])\n",
      "  grad :  tensor([-0.2027,  1.1473])\n",
      "Epoch 563, Loss 6.908967\n",
      "  params :  tensor([  4.1787, -10.5742])\n",
      "  grad :  tensor([-0.2023,  1.1453])\n",
      "Epoch 564, Loss 6.895452\n",
      "  params :  tensor([  4.1808, -10.5857])\n",
      "  grad :  tensor([-0.2020,  1.1434])\n",
      "Epoch 565, Loss 6.881980\n",
      "  params :  tensor([  4.1828, -10.5971])\n",
      "  grad :  tensor([-0.2016,  1.1414])\n",
      "Epoch 566, Loss 6.868559\n",
      "  params :  tensor([  4.1848, -10.6085])\n",
      "  grad :  tensor([-0.2013,  1.1395])\n",
      "Epoch 567, Loss 6.855180\n",
      "  params :  tensor([  4.1868, -10.6198])\n",
      "  grad :  tensor([-0.2010,  1.1375])\n",
      "Epoch 568, Loss 6.841848\n",
      "  params :  tensor([  4.1888, -10.6312])\n",
      "  grad :  tensor([-0.2006,  1.1356])\n",
      "Epoch 569, Loss 6.828561\n",
      "  params :  tensor([  4.1908, -10.6425])\n",
      "  grad :  tensor([-0.2003,  1.1337])\n",
      "Epoch 570, Loss 6.815319\n",
      "  params :  tensor([  4.1928, -10.6539])\n",
      "  grad :  tensor([-0.1999,  1.1318])\n",
      "Epoch 571, Loss 6.802118\n",
      "  params :  tensor([  4.1948, -10.6652])\n",
      "  grad :  tensor([-0.1996,  1.1298])\n",
      "Epoch 572, Loss 6.788968\n",
      "  params :  tensor([  4.1968, -10.6764])\n",
      "  grad :  tensor([-0.1993,  1.1279])\n",
      "Epoch 573, Loss 6.775864\n",
      "  params :  tensor([  4.1988, -10.6877])\n",
      "  grad :  tensor([-0.1989,  1.1260])\n",
      "Epoch 574, Loss 6.762797\n",
      "  params :  tensor([  4.2008, -10.6989])\n",
      "  grad :  tensor([-0.1986,  1.1241])\n",
      "Epoch 575, Loss 6.749779\n",
      "  params :  tensor([  4.2028, -10.7102])\n",
      "  grad :  tensor([-0.1982,  1.1222])\n",
      "Epoch 576, Loss 6.736804\n",
      "  params :  tensor([  4.2047, -10.7214])\n",
      "  grad :  tensor([-0.1979,  1.1203])\n",
      "Epoch 577, Loss 6.723876\n",
      "  params :  tensor([  4.2067, -10.7325])\n",
      "  grad :  tensor([-0.1976,  1.1184])\n",
      "Epoch 578, Loss 6.710987\n",
      "  params :  tensor([  4.2087, -10.7437])\n",
      "  grad :  tensor([-0.1972,  1.1165])\n",
      "Epoch 579, Loss 6.698142\n",
      "  params :  tensor([  4.2107, -10.7549])\n",
      "  grad :  tensor([-0.1969,  1.1146])\n",
      "Epoch 580, Loss 6.685345\n",
      "  params :  tensor([  4.2126, -10.7660])\n",
      "  grad :  tensor([-0.1966,  1.1127])\n",
      "Epoch 581, Loss 6.672589\n",
      "  params :  tensor([  4.2146, -10.7771])\n",
      "  grad :  tensor([-0.1962,  1.1108])\n",
      "Epoch 582, Loss 6.659873\n",
      "  params :  tensor([  4.2165, -10.7882])\n",
      "  grad :  tensor([-0.1959,  1.1089])\n",
      "Epoch 583, Loss 6.647207\n",
      "  params :  tensor([  4.2185, -10.7992])\n",
      "  grad :  tensor([-0.1956,  1.1070])\n",
      "Epoch 584, Loss 6.634578\n",
      "  params :  tensor([  4.2204, -10.8103])\n",
      "  grad :  tensor([-0.1952,  1.1051])\n",
      "Epoch 585, Loss 6.621994\n",
      "  params :  tensor([  4.2224, -10.8213])\n",
      "  grad :  tensor([-0.1949,  1.1033])\n",
      "Epoch 586, Loss 6.609454\n",
      "  params :  tensor([  4.2243, -10.8323])\n",
      "  grad :  tensor([-0.1946,  1.1014])\n",
      "Epoch 587, Loss 6.596953\n",
      "  params :  tensor([  4.2263, -10.8433])\n",
      "  grad :  tensor([-0.1942,  1.0995])\n",
      "Epoch 588, Loss 6.584499\n",
      "  params :  tensor([  4.2282, -10.8543])\n",
      "  grad :  tensor([-0.1939,  1.0976])\n",
      "Epoch 589, Loss 6.572087\n",
      "  params :  tensor([  4.2302, -10.8653])\n",
      "  grad :  tensor([-0.1936,  1.0958])\n",
      "Epoch 590, Loss 6.559712\n",
      "  params :  tensor([  4.2321, -10.8762])\n",
      "  grad :  tensor([-0.1932,  1.0939])\n",
      "Epoch 591, Loss 6.547384\n",
      "  params :  tensor([  4.2340, -10.8871])\n",
      "  grad :  tensor([-0.1929,  1.0921])\n",
      "Epoch 592, Loss 6.535097\n",
      "  params :  tensor([  4.2359, -10.8980])\n",
      "  grad :  tensor([-0.1926,  1.0902])\n",
      "Epoch 593, Loss 6.522851\n",
      "  params :  tensor([  4.2379, -10.9089])\n",
      "  grad :  tensor([-0.1923,  1.0884])\n",
      "Epoch 594, Loss 6.510646\n",
      "  params :  tensor([  4.2398, -10.9198])\n",
      "  grad :  tensor([-0.1919,  1.0865])\n",
      "Epoch 595, Loss 6.498482\n",
      "  params :  tensor([  4.2417, -10.9306])\n",
      "  grad :  tensor([-0.1916,  1.0847])\n",
      "Epoch 596, Loss 6.486361\n",
      "  params :  tensor([  4.2436, -10.9415])\n",
      "  grad :  tensor([-0.1913,  1.0828])\n",
      "Epoch 597, Loss 6.474282\n",
      "  params :  tensor([  4.2455, -10.9523])\n",
      "  grad :  tensor([-0.1910,  1.0810])\n",
      "Epoch 598, Loss 6.462241\n",
      "  params :  tensor([  4.2474, -10.9631])\n",
      "  grad :  tensor([-0.1906,  1.0791])\n",
      "Epoch 599, Loss 6.450243\n",
      "  params :  tensor([  4.2493, -10.9738])\n",
      "  grad :  tensor([-0.1903,  1.0773])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600, Loss 6.438284\n",
      "  params :  tensor([  4.2512, -10.9846])\n",
      "  grad :  tensor([-0.1900,  1.0755])\n",
      "Epoch 601, Loss 6.426368\n",
      "  params :  tensor([  4.2531, -10.9953])\n",
      "  grad :  tensor([-0.1897,  1.0737])\n",
      "Epoch 602, Loss 6.414490\n",
      "  params :  tensor([  4.2550, -11.0060])\n",
      "  grad :  tensor([-0.1893,  1.0718])\n",
      "Epoch 603, Loss 6.402653\n",
      "  params :  tensor([  4.2569, -11.0167])\n",
      "  grad :  tensor([-0.1890,  1.0700])\n",
      "Epoch 604, Loss 6.390859\n",
      "  params :  tensor([  4.2588, -11.0274])\n",
      "  grad :  tensor([-0.1887,  1.0682])\n",
      "Epoch 605, Loss 6.379103\n",
      "  params :  tensor([  4.2607, -11.0381])\n",
      "  grad :  tensor([-0.1884,  1.0664])\n",
      "Epoch 606, Loss 6.367385\n",
      "  params :  tensor([  4.2626, -11.0487])\n",
      "  grad :  tensor([-0.1880,  1.0646])\n",
      "Epoch 607, Loss 6.355706\n",
      "  params :  tensor([  4.2644, -11.0594])\n",
      "  grad :  tensor([-0.1877,  1.0628])\n",
      "Epoch 608, Loss 6.344070\n",
      "  params :  tensor([  4.2663, -11.0700])\n",
      "  grad :  tensor([-0.1874,  1.0609])\n",
      "Epoch 609, Loss 6.332472\n",
      "  params :  tensor([  4.2682, -11.0806])\n",
      "  grad :  tensor([-0.1871,  1.0591])\n",
      "Epoch 610, Loss 6.320912\n",
      "  params :  tensor([  4.2701, -11.0911])\n",
      "  grad :  tensor([-0.1868,  1.0573])\n",
      "Epoch 611, Loss 6.309395\n",
      "  params :  tensor([  4.2719, -11.1017])\n",
      "  grad :  tensor([-0.1865,  1.0555])\n",
      "Epoch 612, Loss 6.297915\n",
      "  params :  tensor([  4.2738, -11.1122])\n",
      "  grad :  tensor([-0.1861,  1.0538])\n",
      "Epoch 613, Loss 6.286473\n",
      "  params :  tensor([  4.2756, -11.1227])\n",
      "  grad :  tensor([-0.1858,  1.0520])\n",
      "Epoch 614, Loss 6.275074\n",
      "  params :  tensor([  4.2775, -11.1333])\n",
      "  grad :  tensor([-0.1855,  1.0502])\n",
      "Epoch 615, Loss 6.263708\n",
      "  params :  tensor([  4.2794, -11.1437])\n",
      "  grad :  tensor([-0.1852,  1.0484])\n",
      "Epoch 616, Loss 6.252382\n",
      "  params :  tensor([  4.2812, -11.1542])\n",
      "  grad :  tensor([-0.1849,  1.0466])\n",
      "Epoch 617, Loss 6.241098\n",
      "  params :  tensor([  4.2830, -11.1646])\n",
      "  grad :  tensor([-0.1846,  1.0448])\n",
      "Epoch 618, Loss 6.229849\n",
      "  params :  tensor([  4.2849, -11.1751])\n",
      "  grad :  tensor([-0.1843,  1.0431])\n",
      "Epoch 619, Loss 6.218639\n",
      "  params :  tensor([  4.2867, -11.1855])\n",
      "  grad :  tensor([-0.1840,  1.0413])\n",
      "Epoch 620, Loss 6.207470\n",
      "  params :  tensor([  4.2886, -11.1959])\n",
      "  grad :  tensor([-0.1836,  1.0395])\n",
      "Epoch 621, Loss 6.196334\n",
      "  params :  tensor([  4.2904, -11.2063])\n",
      "  grad :  tensor([-0.1833,  1.0378])\n",
      "Epoch 622, Loss 6.185240\n",
      "  params :  tensor([  4.2922, -11.2166])\n",
      "  grad :  tensor([-0.1830,  1.0360])\n",
      "Epoch 623, Loss 6.174181\n",
      "  params :  tensor([  4.2941, -11.2270])\n",
      "  grad :  tensor([-0.1827,  1.0342])\n",
      "Epoch 624, Loss 6.163159\n",
      "  params :  tensor([  4.2959, -11.2373])\n",
      "  grad :  tensor([-0.1824,  1.0325])\n",
      "Epoch 625, Loss 6.152177\n",
      "  params :  tensor([  4.2977, -11.2476])\n",
      "  grad :  tensor([-0.1821,  1.0307])\n",
      "Epoch 626, Loss 6.141230\n",
      "  params :  tensor([  4.2995, -11.2579])\n",
      "  grad :  tensor([-0.1818,  1.0290])\n",
      "Epoch 627, Loss 6.130322\n",
      "  params :  tensor([  4.3013, -11.2682])\n",
      "  grad :  tensor([-0.1815,  1.0272])\n",
      "Epoch 628, Loss 6.119448\n",
      "  params :  tensor([  4.3031, -11.2784])\n",
      "  grad :  tensor([-0.1811,  1.0255])\n",
      "Epoch 629, Loss 6.108614\n",
      "  params :  tensor([  4.3050, -11.2887])\n",
      "  grad :  tensor([-0.1808,  1.0237])\n",
      "Epoch 630, Loss 6.097815\n",
      "  params :  tensor([  4.3068, -11.2989])\n",
      "  grad :  tensor([-0.1805,  1.0220])\n",
      "Epoch 631, Loss 6.087054\n",
      "  params :  tensor([  4.3086, -11.3091])\n",
      "  grad :  tensor([-0.1802,  1.0203])\n",
      "Epoch 632, Loss 6.076329\n",
      "  params :  tensor([  4.3104, -11.3193])\n",
      "  grad :  tensor([-0.1799,  1.0185])\n",
      "Epoch 633, Loss 6.065644\n",
      "  params :  tensor([  4.3122, -11.3294])\n",
      "  grad :  tensor([-0.1796,  1.0168])\n",
      "Epoch 634, Loss 6.054988\n",
      "  params :  tensor([  4.3139, -11.3396])\n",
      "  grad :  tensor([-0.1793,  1.0151])\n",
      "Epoch 635, Loss 6.044372\n",
      "  params :  tensor([  4.3157, -11.3497])\n",
      "  grad :  tensor([-0.1790,  1.0133])\n",
      "Epoch 636, Loss 6.033794\n",
      "  params :  tensor([  4.3175, -11.3598])\n",
      "  grad :  tensor([-0.1787,  1.0116])\n",
      "Epoch 637, Loss 6.023247\n",
      "  params :  tensor([  4.3193, -11.3699])\n",
      "  grad :  tensor([-0.1784,  1.0099])\n",
      "Epoch 638, Loss 6.012738\n",
      "  params :  tensor([  4.3211, -11.3800])\n",
      "  grad :  tensor([-0.1781,  1.0082])\n",
      "Epoch 639, Loss 6.002264\n",
      "  params :  tensor([  4.3229, -11.3901])\n",
      "  grad :  tensor([-0.1778,  1.0065])\n",
      "Epoch 640, Loss 5.991828\n",
      "  params :  tensor([  4.3246, -11.4001])\n",
      "  grad :  tensor([-0.1775,  1.0048])\n",
      "Epoch 641, Loss 5.981425\n",
      "  params :  tensor([  4.3264, -11.4102])\n",
      "  grad :  tensor([-0.1772,  1.0031])\n",
      "Epoch 642, Loss 5.971058\n",
      "  params :  tensor([  4.3282, -11.4202])\n",
      "  grad :  tensor([-0.1769,  1.0014])\n",
      "Epoch 643, Loss 5.960727\n",
      "  params :  tensor([  4.3300, -11.4302])\n",
      "  grad :  tensor([-0.1766,  0.9997])\n",
      "Epoch 644, Loss 5.950432\n",
      "  params :  tensor([  4.3317, -11.4401])\n",
      "  grad :  tensor([-0.1763,  0.9980])\n",
      "Epoch 645, Loss 5.940171\n",
      "  params :  tensor([  4.3335, -11.4501])\n",
      "  grad :  tensor([-0.1760,  0.9963])\n",
      "Epoch 646, Loss 5.929944\n",
      "  params :  tensor([  4.3352, -11.4601])\n",
      "  grad :  tensor([-0.1757,  0.9946])\n",
      "Epoch 647, Loss 5.919752\n",
      "  params :  tensor([  4.3370, -11.4700])\n",
      "  grad :  tensor([-0.1754,  0.9929])\n",
      "Epoch 648, Loss 5.909596\n",
      "  params :  tensor([  4.3387, -11.4799])\n",
      "  grad :  tensor([-0.1751,  0.9912])\n",
      "Epoch 649, Loss 5.899472\n",
      "  params :  tensor([  4.3405, -11.4898])\n",
      "  grad :  tensor([-0.1748,  0.9895])\n",
      "Epoch 650, Loss 5.889383\n",
      "  params :  tensor([  4.3422, -11.4997])\n",
      "  grad :  tensor([-0.1745,  0.9878])\n",
      "Epoch 651, Loss 5.879326\n",
      "  params :  tensor([  4.3440, -11.5095])\n",
      "  grad :  tensor([-0.1742,  0.9862])\n",
      "Epoch 652, Loss 5.869310\n",
      "  params :  tensor([  4.3457, -11.5194])\n",
      "  grad :  tensor([-0.1739,  0.9845])\n",
      "Epoch 653, Loss 5.859322\n",
      "  params :  tensor([  4.3474, -11.5292])\n",
      "  grad :  tensor([-0.1736,  0.9828])\n",
      "Epoch 654, Loss 5.849374\n",
      "  params :  tensor([  4.3492, -11.5390])\n",
      "  grad :  tensor([-0.1733,  0.9811])\n",
      "Epoch 655, Loss 5.839453\n",
      "  params :  tensor([  4.3509, -11.5488])\n",
      "  grad :  tensor([-0.1730,  0.9795])\n",
      "Epoch 656, Loss 5.829570\n",
      "  params :  tensor([  4.3526, -11.5586])\n",
      "  grad :  tensor([-0.1727,  0.9778])\n",
      "Epoch 657, Loss 5.819718\n",
      "  params :  tensor([  4.3544, -11.5683])\n",
      "  grad :  tensor([-0.1724,  0.9761])\n",
      "Epoch 658, Loss 5.809901\n",
      "  params :  tensor([  4.3561, -11.5781])\n",
      "  grad :  tensor([-0.1722,  0.9745])\n",
      "Epoch 659, Loss 5.800116\n",
      "  params :  tensor([  4.3578, -11.5878])\n",
      "  grad :  tensor([-0.1719,  0.9728])\n",
      "Epoch 660, Loss 5.790367\n",
      "  params :  tensor([  4.3595, -11.5975])\n",
      "  grad :  tensor([-0.1716,  0.9712])\n",
      "Epoch 661, Loss 5.780646\n",
      "  params :  tensor([  4.3612, -11.6072])\n",
      "  grad :  tensor([-0.1713,  0.9695])\n",
      "Epoch 662, Loss 5.770962\n",
      "  params :  tensor([  4.3629, -11.6169])\n",
      "  grad :  tensor([-0.1710,  0.9679])\n",
      "Epoch 663, Loss 5.761312\n",
      "  params :  tensor([  4.3646, -11.6266])\n",
      "  grad :  tensor([-0.1707,  0.9662])\n",
      "Epoch 664, Loss 5.751694\n",
      "  params :  tensor([  4.3664, -11.6362])\n",
      "  grad :  tensor([-0.1704,  0.9646])\n",
      "Epoch 665, Loss 5.742105\n",
      "  params :  tensor([  4.3681, -11.6458])\n",
      "  grad :  tensor([-0.1701,  0.9630])\n",
      "Epoch 666, Loss 5.732550\n",
      "  params :  tensor([  4.3697, -11.6555])\n",
      "  grad :  tensor([-0.1698,  0.9613])\n",
      "Epoch 667, Loss 5.723031\n",
      "  params :  tensor([  4.3714, -11.6651])\n",
      "  grad :  tensor([-0.1695,  0.9597])\n",
      "Epoch 668, Loss 5.713540\n",
      "  params :  tensor([  4.3731, -11.6746])\n",
      "  grad :  tensor([-0.1692,  0.9581])\n",
      "Epoch 669, Loss 5.704083\n",
      "  params :  tensor([  4.3748, -11.6842])\n",
      "  grad :  tensor([-0.1690,  0.9564])\n",
      "Epoch 670, Loss 5.694659\n",
      "  params :  tensor([  4.3765, -11.6937])\n",
      "  grad :  tensor([-0.1687,  0.9548])\n",
      "Epoch 671, Loss 5.685266\n",
      "  params :  tensor([  4.3782, -11.7033])\n",
      "  grad :  tensor([-0.1684,  0.9532])\n",
      "Epoch 672, Loss 5.675904\n",
      "  params :  tensor([  4.3799, -11.7128])\n",
      "  grad :  tensor([-0.1681,  0.9516])\n",
      "Epoch 673, Loss 5.666573\n",
      "  params :  tensor([  4.3816, -11.7223])\n",
      "  grad :  tensor([-0.1678,  0.9499])\n",
      "Epoch 674, Loss 5.657277\n",
      "  params :  tensor([  4.3832, -11.7318])\n",
      "  grad :  tensor([-0.1675,  0.9483])\n",
      "Epoch 675, Loss 5.648010\n",
      "  params :  tensor([  4.3849, -11.7412])\n",
      "  grad :  tensor([-0.1673,  0.9467])\n",
      "Epoch 676, Loss 5.638776\n",
      "  params :  tensor([  4.3866, -11.7507])\n",
      "  grad :  tensor([-0.1670,  0.9451])\n",
      "Epoch 677, Loss 5.629574\n",
      "  params :  tensor([  4.3882, -11.7601])\n",
      "  grad :  tensor([-0.1667,  0.9435])\n",
      "Epoch 678, Loss 5.620402\n",
      "  params :  tensor([  4.3899, -11.7696])\n",
      "  grad :  tensor([-0.1664,  0.9419])\n",
      "Epoch 679, Loss 5.611260\n",
      "  params :  tensor([  4.3916, -11.7790])\n",
      "  grad :  tensor([-0.1661,  0.9403])\n",
      "Epoch 680, Loss 5.602149\n",
      "  params :  tensor([  4.3932, -11.7883])\n",
      "  grad :  tensor([-0.1658,  0.9387])\n",
      "Epoch 681, Loss 5.593071\n",
      "  params :  tensor([  4.3949, -11.7977])\n",
      "  grad :  tensor([-0.1656,  0.9371])\n",
      "Epoch 682, Loss 5.584022\n",
      "  params :  tensor([  4.3965, -11.8071])\n",
      "  grad :  tensor([-0.1653,  0.9355])\n",
      "Epoch 683, Loss 5.575005\n",
      "  params :  tensor([  4.3982, -11.8164])\n",
      "  grad :  tensor([-0.1650,  0.9339])\n",
      "Epoch 684, Loss 5.566019\n",
      "  params :  tensor([  4.3998, -11.8257])\n",
      "  grad :  tensor([-0.1647,  0.9323])\n",
      "Epoch 685, Loss 5.557063\n",
      "  params :  tensor([  4.4015, -11.8350])\n",
      "  grad :  tensor([-0.1644,  0.9308])\n",
      "Epoch 686, Loss 5.548136\n",
      "  params :  tensor([  4.4031, -11.8443])\n",
      "  grad :  tensor([-0.1641,  0.9292])\n",
      "Epoch 687, Loss 5.539241\n",
      "  params :  tensor([  4.4048, -11.8536])\n",
      "  grad :  tensor([-0.1639,  0.9276])\n",
      "Epoch 688, Loss 5.530376\n",
      "  params :  tensor([  4.4064, -11.8629])\n",
      "  grad :  tensor([-0.1636,  0.9260])\n",
      "Epoch 689, Loss 5.521540\n",
      "  params :  tensor([  4.4080, -11.8721])\n",
      "  grad :  tensor([-0.1633,  0.9245])\n",
      "Epoch 690, Loss 5.512734\n",
      "  params :  tensor([  4.4097, -11.8813])\n",
      "  grad :  tensor([-0.1630,  0.9229])\n",
      "Epoch 691, Loss 5.503958\n",
      "  params :  tensor([  4.4113, -11.8906])\n",
      "  grad :  tensor([-0.1628,  0.9213])\n",
      "Epoch 692, Loss 5.495212\n",
      "  params :  tensor([  4.4129, -11.8998])\n",
      "  grad :  tensor([-0.1625,  0.9197])\n",
      "Epoch 693, Loss 5.486496\n",
      "  params :  tensor([  4.4145, -11.9089])\n",
      "  grad :  tensor([-0.1622,  0.9182])\n",
      "Epoch 694, Loss 5.477808\n",
      "  params :  tensor([  4.4161, -11.9181])\n",
      "  grad :  tensor([-0.1619,  0.9166])\n",
      "Epoch 695, Loss 5.469152\n",
      "  params :  tensor([  4.4178, -11.9272])\n",
      "  grad :  tensor([-0.1617,  0.9151])\n",
      "Epoch 696, Loss 5.460525\n",
      "  params :  tensor([  4.4194, -11.9364])\n",
      "  grad :  tensor([-0.1614,  0.9135])\n",
      "Epoch 697, Loss 5.451928\n",
      "  params :  tensor([  4.4210, -11.9455])\n",
      "  grad :  tensor([-0.1611,  0.9120])\n",
      "Epoch 698, Loss 5.443358\n",
      "  params :  tensor([  4.4226, -11.9546])\n",
      "  grad :  tensor([-0.1608,  0.9104])\n",
      "Epoch 699, Loss 5.434819\n",
      "  params :  tensor([  4.4242, -11.9637])\n",
      "  grad :  tensor([-0.1605,  0.9089])\n",
      "Epoch 700, Loss 5.426309\n",
      "  params :  tensor([  4.4258, -11.9728])\n",
      "  grad :  tensor([-0.1603,  0.9073])\n",
      "Epoch 701, Loss 5.417827\n",
      "  params :  tensor([  4.4274, -11.9818])\n",
      "  grad :  tensor([-0.1600,  0.9058])\n",
      "Epoch 702, Loss 5.409372\n",
      "  params :  tensor([  4.4290, -11.9909])\n",
      "  grad :  tensor([-0.1597,  0.9042])\n",
      "Epoch 703, Loss 5.400949\n",
      "  params :  tensor([  4.4306, -11.9999])\n",
      "  grad :  tensor([-0.1595,  0.9027])\n",
      "Epoch 704, Loss 5.392550\n",
      "  params :  tensor([  4.4322, -12.0089])\n",
      "  grad :  tensor([-0.1592,  0.9012])\n",
      "Epoch 705, Loss 5.384184\n",
      "  params :  tensor([  4.4338, -12.0179])\n",
      "  grad :  tensor([-0.1589,  0.8996])\n",
      "Epoch 706, Loss 5.375846\n",
      "  params :  tensor([  4.4354, -12.0269])\n",
      "  grad :  tensor([-0.1586,  0.8981])\n",
      "Epoch 707, Loss 5.367537\n",
      "  params :  tensor([  4.4369, -12.0359])\n",
      "  grad :  tensor([-0.1584,  0.8966])\n",
      "Epoch 708, Loss 5.359253\n",
      "  params :  tensor([  4.4385, -12.0448])\n",
      "  grad :  tensor([-0.1581,  0.8951])\n",
      "Epoch 709, Loss 5.350999\n",
      "  params :  tensor([  4.4401, -12.0537])\n",
      "  grad :  tensor([-0.1578,  0.8935])\n",
      "Epoch 710, Loss 5.342772\n",
      "  params :  tensor([  4.4417, -12.0627])\n",
      "  grad :  tensor([-0.1576,  0.8920])\n",
      "Epoch 711, Loss 5.334575\n",
      "  params :  tensor([  4.4433, -12.0716])\n",
      "  grad :  tensor([-0.1573,  0.8905])\n",
      "Epoch 712, Loss 5.326402\n",
      "  params :  tensor([  4.4448, -12.0805])\n",
      "  grad :  tensor([-0.1570,  0.8890])\n",
      "Epoch 713, Loss 5.318260\n",
      "  params :  tensor([  4.4464, -12.0893])\n",
      "  grad :  tensor([-0.1568,  0.8875])\n",
      "Epoch 714, Loss 5.310144\n",
      "  params :  tensor([  4.4480, -12.0982])\n",
      "  grad :  tensor([-0.1565,  0.8860])\n",
      "Epoch 715, Loss 5.302055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  4.4495, -12.1070])\n",
      "  grad :  tensor([-0.1562,  0.8845])\n",
      "Epoch 716, Loss 5.293994\n",
      "  params :  tensor([  4.4511, -12.1159])\n",
      "  grad :  tensor([-0.1560,  0.8830])\n",
      "Epoch 717, Loss 5.285964\n",
      "  params :  tensor([  4.4526, -12.1247])\n",
      "  grad :  tensor([-0.1557,  0.8815])\n",
      "Epoch 718, Loss 5.277958\n",
      "  params :  tensor([  4.4542, -12.1335])\n",
      "  grad :  tensor([-0.1555,  0.8800])\n",
      "Epoch 719, Loss 5.269979\n",
      "  params :  tensor([  4.4557, -12.1423])\n",
      "  grad :  tensor([-0.1552,  0.8785])\n",
      "Epoch 720, Loss 5.262027\n",
      "  params :  tensor([  4.4573, -12.1510])\n",
      "  grad :  tensor([-0.1549,  0.8770])\n",
      "Epoch 721, Loss 5.254103\n",
      "  params :  tensor([  4.4588, -12.1598])\n",
      "  grad :  tensor([-0.1547,  0.8755])\n",
      "Epoch 722, Loss 5.246205\n",
      "  params :  tensor([  4.4604, -12.1685])\n",
      "  grad :  tensor([-0.1544,  0.8740])\n",
      "Epoch 723, Loss 5.238335\n",
      "  params :  tensor([  4.4619, -12.1773])\n",
      "  grad :  tensor([-0.1541,  0.8725])\n",
      "Epoch 724, Loss 5.230492\n",
      "  params :  tensor([  4.4635, -12.1860])\n",
      "  grad :  tensor([-0.1539,  0.8710])\n",
      "Epoch 725, Loss 5.222674\n",
      "  params :  tensor([  4.4650, -12.1947])\n",
      "  grad :  tensor([-0.1536,  0.8696])\n",
      "Epoch 726, Loss 5.214881\n",
      "  params :  tensor([  4.4665, -12.2033])\n",
      "  grad :  tensor([-0.1533,  0.8681])\n",
      "Epoch 727, Loss 5.207120\n",
      "  params :  tensor([  4.4681, -12.2120])\n",
      "  grad :  tensor([-0.1531,  0.8666])\n",
      "Epoch 728, Loss 5.199381\n",
      "  params :  tensor([  4.4696, -12.2207])\n",
      "  grad :  tensor([-0.1528,  0.8651])\n",
      "Epoch 729, Loss 5.191670\n",
      "  params :  tensor([  4.4711, -12.2293])\n",
      "  grad :  tensor([-0.1526,  0.8637])\n",
      "Epoch 730, Loss 5.183985\n",
      "  params :  tensor([  4.4726, -12.2379])\n",
      "  grad :  tensor([-0.1523,  0.8622])\n",
      "Epoch 731, Loss 5.176324\n",
      "  params :  tensor([  4.4742, -12.2465])\n",
      "  grad :  tensor([-0.1520,  0.8607])\n",
      "Epoch 732, Loss 5.168688\n",
      "  params :  tensor([  4.4757, -12.2551])\n",
      "  grad :  tensor([-0.1518,  0.8593])\n",
      "Epoch 733, Loss 5.161084\n",
      "  params :  tensor([  4.4772, -12.2637])\n",
      "  grad :  tensor([-0.1515,  0.8578])\n",
      "Epoch 734, Loss 5.153500\n",
      "  params :  tensor([  4.4787, -12.2723])\n",
      "  grad :  tensor([-0.1513,  0.8564])\n",
      "Epoch 735, Loss 5.145944\n",
      "  params :  tensor([  4.4802, -12.2808])\n",
      "  grad :  tensor([-0.1510,  0.8549])\n",
      "Epoch 736, Loss 5.138413\n",
      "  params :  tensor([  4.4817, -12.2893])\n",
      "  grad :  tensor([-0.1508,  0.8535])\n",
      "Epoch 737, Loss 5.130910\n",
      "  params :  tensor([  4.4832, -12.2979])\n",
      "  grad :  tensor([-0.1505,  0.8520])\n",
      "Epoch 738, Loss 5.123428\n",
      "  params :  tensor([  4.4847, -12.3064])\n",
      "  grad :  tensor([-0.1502,  0.8506])\n",
      "Epoch 739, Loss 5.115978\n",
      "  params :  tensor([  4.4862, -12.3149])\n",
      "  grad :  tensor([-0.1500,  0.8491])\n",
      "Epoch 740, Loss 5.108547\n",
      "  params :  tensor([  4.4877, -12.3233])\n",
      "  grad :  tensor([-0.1497,  0.8477])\n",
      "Epoch 741, Loss 5.101143\n",
      "  params :  tensor([  4.4892, -12.3318])\n",
      "  grad :  tensor([-0.1495,  0.8462])\n",
      "Epoch 742, Loss 5.093765\n",
      "  params :  tensor([  4.4907, -12.3402])\n",
      "  grad :  tensor([-0.1492,  0.8448])\n",
      "Epoch 743, Loss 5.086414\n",
      "  params :  tensor([  4.4922, -12.3487])\n",
      "  grad :  tensor([-0.1490,  0.8434])\n",
      "Epoch 744, Loss 5.079086\n",
      "  params :  tensor([  4.4937, -12.3571])\n",
      "  grad :  tensor([-0.1487,  0.8419])\n",
      "Epoch 745, Loss 5.071781\n",
      "  params :  tensor([  4.4952, -12.3655])\n",
      "  grad :  tensor([-0.1485,  0.8405])\n",
      "Epoch 746, Loss 5.064505\n",
      "  params :  tensor([  4.4967, -12.3739])\n",
      "  grad :  tensor([-0.1482,  0.8391])\n",
      "Epoch 747, Loss 5.057247\n",
      "  params :  tensor([  4.4981, -12.3823])\n",
      "  grad :  tensor([-0.1480,  0.8376])\n",
      "Epoch 748, Loss 5.050021\n",
      "  params :  tensor([  4.4996, -12.3906])\n",
      "  grad :  tensor([-0.1477,  0.8362])\n",
      "Epoch 749, Loss 5.042817\n",
      "  params :  tensor([  4.5011, -12.3990])\n",
      "  grad :  tensor([-0.1475,  0.8348])\n",
      "Epoch 750, Loss 5.035636\n",
      "  params :  tensor([  4.5026, -12.4073])\n",
      "  grad :  tensor([-0.1472,  0.8334])\n",
      "Epoch 751, Loss 5.028476\n",
      "  params :  tensor([  4.5040, -12.4156])\n",
      "  grad :  tensor([-0.1470,  0.8320])\n",
      "Epoch 752, Loss 5.021346\n",
      "  params :  tensor([  4.5055, -12.4239])\n",
      "  grad :  tensor([-0.1467,  0.8305])\n",
      "Epoch 753, Loss 5.014240\n",
      "  params :  tensor([  4.5070, -12.4322])\n",
      "  grad :  tensor([-0.1465,  0.8291])\n",
      "Epoch 754, Loss 5.007157\n",
      "  params :  tensor([  4.5084, -12.4405])\n",
      "  grad :  tensor([-0.1462,  0.8277])\n",
      "Epoch 755, Loss 5.000099\n",
      "  params :  tensor([  4.5099, -12.4488])\n",
      "  grad :  tensor([-0.1460,  0.8263])\n",
      "Epoch 756, Loss 4.993064\n",
      "  params :  tensor([  4.5113, -12.4570])\n",
      "  grad :  tensor([-0.1457,  0.8249])\n",
      "Epoch 757, Loss 4.986051\n",
      "  params :  tensor([  4.5128, -12.4653])\n",
      "  grad :  tensor([-0.1455,  0.8235])\n",
      "Epoch 758, Loss 4.979064\n",
      "  params :  tensor([  4.5143, -12.4735])\n",
      "  grad :  tensor([-0.1452,  0.8221])\n",
      "Epoch 759, Loss 4.972100\n",
      "  params :  tensor([  4.5157, -12.4817])\n",
      "  grad :  tensor([-0.1450,  0.8207])\n",
      "Epoch 760, Loss 4.965159\n",
      "  params :  tensor([  4.5172, -12.4899])\n",
      "  grad :  tensor([-0.1447,  0.8193])\n",
      "Epoch 761, Loss 4.958245\n",
      "  params :  tensor([  4.5186, -12.4981])\n",
      "  grad :  tensor([-0.1445,  0.8179])\n",
      "Epoch 762, Loss 4.951351\n",
      "  params :  tensor([  4.5200, -12.5062])\n",
      "  grad :  tensor([-0.1443,  0.8165])\n",
      "Epoch 763, Loss 4.944479\n",
      "  params :  tensor([  4.5215, -12.5144])\n",
      "  grad :  tensor([-0.1440,  0.8152])\n",
      "Epoch 764, Loss 4.937633\n",
      "  params :  tensor([  4.5229, -12.5225])\n",
      "  grad :  tensor([-0.1438,  0.8138])\n",
      "Epoch 765, Loss 4.930812\n",
      "  params :  tensor([  4.5244, -12.5306])\n",
      "  grad :  tensor([-0.1435,  0.8124])\n",
      "Epoch 766, Loss 4.924009\n",
      "  params :  tensor([  4.5258, -12.5387])\n",
      "  grad :  tensor([-0.1433,  0.8110])\n",
      "Epoch 767, Loss 4.917234\n",
      "  params :  tensor([  4.5272, -12.5468])\n",
      "  grad :  tensor([-0.1430,  0.8096])\n",
      "Epoch 768, Loss 4.910480\n",
      "  params :  tensor([  4.5286, -12.5549])\n",
      "  grad :  tensor([-0.1428,  0.8083])\n",
      "Epoch 769, Loss 4.903749\n",
      "  params :  tensor([  4.5301, -12.5630])\n",
      "  grad :  tensor([-0.1426,  0.8069])\n",
      "Epoch 770, Loss 4.897040\n",
      "  params :  tensor([  4.5315, -12.5711])\n",
      "  grad :  tensor([-0.1423,  0.8055])\n",
      "Epoch 771, Loss 4.890356\n",
      "  params :  tensor([  4.5329, -12.5791])\n",
      "  grad :  tensor([-0.1420,  0.8042])\n",
      "Epoch 772, Loss 4.883692\n",
      "  params :  tensor([  4.5343, -12.5871])\n",
      "  grad :  tensor([-0.1418,  0.8028])\n",
      "Epoch 773, Loss 4.877052\n",
      "  params :  tensor([  4.5357, -12.5951])\n",
      "  grad :  tensor([-0.1416,  0.8014])\n",
      "Epoch 774, Loss 4.870436\n",
      "  params :  tensor([  4.5372, -12.6031])\n",
      "  grad :  tensor([-0.1413,  0.8001])\n",
      "Epoch 775, Loss 4.863839\n",
      "  params :  tensor([  4.5386, -12.6111])\n",
      "  grad :  tensor([-0.1411,  0.7987])\n",
      "Epoch 776, Loss 4.857268\n",
      "  params :  tensor([  4.5400, -12.6191])\n",
      "  grad :  tensor([-0.1408,  0.7973])\n",
      "Epoch 777, Loss 4.850718\n",
      "  params :  tensor([  4.5414, -12.6271])\n",
      "  grad :  tensor([-0.1406,  0.7960])\n",
      "Epoch 778, Loss 4.844189\n",
      "  params :  tensor([  4.5428, -12.6350])\n",
      "  grad :  tensor([-0.1404,  0.7946])\n",
      "Epoch 779, Loss 4.837683\n",
      "  params :  tensor([  4.5442, -12.6429])\n",
      "  grad :  tensor([-0.1401,  0.7933])\n",
      "Epoch 780, Loss 4.831196\n",
      "  params :  tensor([  4.5456, -12.6509])\n",
      "  grad :  tensor([-0.1399,  0.7919])\n",
      "Epoch 781, Loss 4.824737\n",
      "  params :  tensor([  4.5470, -12.6588])\n",
      "  grad :  tensor([-0.1397,  0.7906])\n",
      "Epoch 782, Loss 4.818298\n",
      "  params :  tensor([  4.5484, -12.6667])\n",
      "  grad :  tensor([-0.1394,  0.7893])\n",
      "Epoch 783, Loss 4.811879\n",
      "  params :  tensor([  4.5498, -12.6745])\n",
      "  grad :  tensor([-0.1392,  0.7879])\n",
      "Epoch 784, Loss 4.805481\n",
      "  params :  tensor([  4.5512, -12.6824])\n",
      "  grad :  tensor([-0.1389,  0.7866])\n",
      "Epoch 785, Loss 4.799106\n",
      "  params :  tensor([  4.5525, -12.6902])\n",
      "  grad :  tensor([-0.1387,  0.7852])\n",
      "Epoch 786, Loss 4.792755\n",
      "  params :  tensor([  4.5539, -12.6981])\n",
      "  grad :  tensor([-0.1385,  0.7839])\n",
      "Epoch 787, Loss 4.786422\n",
      "  params :  tensor([  4.5553, -12.7059])\n",
      "  grad :  tensor([-0.1383,  0.7826])\n",
      "Epoch 788, Loss 4.780112\n",
      "  params :  tensor([  4.5567, -12.7137])\n",
      "  grad :  tensor([-0.1380,  0.7812])\n",
      "Epoch 789, Loss 4.773824\n",
      "  params :  tensor([  4.5581, -12.7215])\n",
      "  grad :  tensor([-0.1378,  0.7799])\n",
      "Epoch 790, Loss 4.767558\n",
      "  params :  tensor([  4.5594, -12.7293])\n",
      "  grad :  tensor([-0.1375,  0.7786])\n",
      "Epoch 791, Loss 4.761312\n",
      "  params :  tensor([  4.5608, -12.7371])\n",
      "  grad :  tensor([-0.1373,  0.7773])\n",
      "Epoch 792, Loss 4.755087\n",
      "  params :  tensor([  4.5622, -12.7448])\n",
      "  grad :  tensor([-0.1371,  0.7759])\n",
      "Epoch 793, Loss 4.748885\n",
      "  params :  tensor([  4.5636, -12.7526])\n",
      "  grad :  tensor([-0.1368,  0.7746])\n",
      "Epoch 794, Loss 4.742700\n",
      "  params :  tensor([  4.5649, -12.7603])\n",
      "  grad :  tensor([-0.1366,  0.7733])\n",
      "Epoch 795, Loss 4.736537\n",
      "  params :  tensor([  4.5663, -12.7680])\n",
      "  grad :  tensor([-0.1364,  0.7720])\n",
      "Epoch 796, Loss 4.730397\n",
      "  params :  tensor([  4.5677, -12.7758])\n",
      "  grad :  tensor([-0.1361,  0.7707])\n",
      "Epoch 797, Loss 4.724279\n",
      "  params :  tensor([  4.5690, -12.7834])\n",
      "  grad :  tensor([-0.1359,  0.7694])\n",
      "Epoch 798, Loss 4.718181\n",
      "  params :  tensor([  4.5704, -12.7911])\n",
      "  grad :  tensor([-0.1357,  0.7681])\n",
      "Epoch 799, Loss 4.712101\n",
      "  params :  tensor([  4.5717, -12.7988])\n",
      "  grad :  tensor([-0.1354,  0.7668])\n",
      "Epoch 800, Loss 4.706046\n",
      "  params :  tensor([  4.5731, -12.8064])\n",
      "  grad :  tensor([-0.1352,  0.7655])\n",
      "Epoch 801, Loss 4.700009\n",
      "  params :  tensor([  4.5744, -12.8141])\n",
      "  grad :  tensor([-0.1350,  0.7642])\n",
      "Epoch 802, Loss 4.693990\n",
      "  params :  tensor([  4.5758, -12.8217])\n",
      "  grad :  tensor([-0.1347,  0.7629])\n",
      "Epoch 803, Loss 4.687995\n",
      "  params :  tensor([  4.5771, -12.8293])\n",
      "  grad :  tensor([-0.1345,  0.7616])\n",
      "Epoch 804, Loss 4.682020\n",
      "  params :  tensor([  4.5785, -12.8369])\n",
      "  grad :  tensor([-0.1343,  0.7603])\n",
      "Epoch 805, Loss 4.676063\n",
      "  params :  tensor([  4.5798, -12.8445])\n",
      "  grad :  tensor([-0.1341,  0.7590])\n",
      "Epoch 806, Loss 4.670130\n",
      "  params :  tensor([  4.5811, -12.8521])\n",
      "  grad :  tensor([-0.1338,  0.7577])\n",
      "Epoch 807, Loss 4.664214\n",
      "  params :  tensor([  4.5825, -12.8597])\n",
      "  grad :  tensor([-0.1336,  0.7564])\n",
      "Epoch 808, Loss 4.658319\n",
      "  params :  tensor([  4.5838, -12.8672])\n",
      "  grad :  tensor([-0.1334,  0.7551])\n",
      "Epoch 809, Loss 4.652445\n",
      "  params :  tensor([  4.5851, -12.8748])\n",
      "  grad :  tensor([-0.1332,  0.7538])\n",
      "Epoch 810, Loss 4.646592\n",
      "  params :  tensor([  4.5865, -12.8823])\n",
      "  grad :  tensor([-0.1330,  0.7526])\n",
      "Epoch 811, Loss 4.640754\n",
      "  params :  tensor([  4.5878, -12.8898])\n",
      "  grad :  tensor([-0.1327,  0.7513])\n",
      "Epoch 812, Loss 4.634938\n",
      "  params :  tensor([  4.5891, -12.8973])\n",
      "  grad :  tensor([-0.1325,  0.7500])\n",
      "Epoch 813, Loss 4.629142\n",
      "  params :  tensor([  4.5904, -12.9048])\n",
      "  grad :  tensor([-0.1323,  0.7487])\n",
      "Epoch 814, Loss 4.623367\n",
      "  params :  tensor([  4.5918, -12.9123])\n",
      "  grad :  tensor([-0.1320,  0.7475])\n",
      "Epoch 815, Loss 4.617611\n",
      "  params :  tensor([  4.5931, -12.9197])\n",
      "  grad :  tensor([-0.1318,  0.7462])\n",
      "Epoch 816, Loss 4.611872\n",
      "  params :  tensor([  4.5944, -12.9272])\n",
      "  grad :  tensor([-0.1316,  0.7449])\n",
      "Epoch 817, Loss 4.606156\n",
      "  params :  tensor([  4.5957, -12.9346])\n",
      "  grad :  tensor([-0.1314,  0.7437])\n",
      "Epoch 818, Loss 4.600458\n",
      "  params :  tensor([  4.5970, -12.9420])\n",
      "  grad :  tensor([-0.1311,  0.7424])\n",
      "Epoch 819, Loss 4.594780\n",
      "  params :  tensor([  4.5983, -12.9494])\n",
      "  grad :  tensor([-0.1309,  0.7411])\n",
      "Epoch 820, Loss 4.589119\n",
      "  params :  tensor([  4.5996, -12.9568])\n",
      "  grad :  tensor([-0.1307,  0.7399])\n",
      "Epoch 821, Loss 4.583479\n",
      "  params :  tensor([  4.6009, -12.9642])\n",
      "  grad :  tensor([-0.1305,  0.7386])\n",
      "Epoch 822, Loss 4.577857\n",
      "  params :  tensor([  4.6022, -12.9716])\n",
      "  grad :  tensor([-0.1303,  0.7374])\n",
      "Epoch 823, Loss 4.572256\n",
      "  params :  tensor([  4.6035, -12.9790])\n",
      "  grad :  tensor([-0.1300,  0.7361])\n",
      "Epoch 824, Loss 4.566675\n",
      "  params :  tensor([  4.6048, -12.9863])\n",
      "  grad :  tensor([-0.1298,  0.7349])\n",
      "Epoch 825, Loss 4.561108\n",
      "  params :  tensor([  4.6061, -12.9936])\n",
      "  grad :  tensor([-0.1296,  0.7336])\n",
      "Epoch 826, Loss 4.555565\n",
      "  params :  tensor([  4.6074, -13.0010])\n",
      "  grad :  tensor([-0.1294,  0.7324])\n",
      "Epoch 827, Loss 4.550039\n",
      "  params :  tensor([  4.6087, -13.0083])\n",
      "  grad :  tensor([-0.1292,  0.7311])\n",
      "Epoch 828, Loss 4.544534\n",
      "  params :  tensor([  4.6100, -13.0156])\n",
      "  grad :  tensor([-0.1289,  0.7299])\n",
      "Epoch 829, Loss 4.539044\n",
      "  params :  tensor([  4.6113, -13.0229])\n",
      "  grad :  tensor([-0.1287,  0.7286])\n",
      "Epoch 830, Loss 4.533575\n",
      "  params :  tensor([  4.6126, -13.0301])\n",
      "  grad :  tensor([-0.1285,  0.7274])\n",
      "Epoch 831, Loss 4.528122\n",
      "  params :  tensor([  4.6139, -13.0374])\n",
      "  grad :  tensor([-0.1283,  0.7262])\n",
      "Epoch 832, Loss 4.522691\n",
      "  params :  tensor([  4.6152, -13.0446])\n",
      "  grad :  tensor([-0.1280,  0.7249])\n",
      "Epoch 833, Loss 4.517276\n",
      "  params :  tensor([  4.6164, -13.0519])\n",
      "  grad :  tensor([-0.1278,  0.7237])\n",
      "Epoch 834, Loss 4.511879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  4.6177, -13.0591])\n",
      "  grad :  tensor([-0.1276,  0.7225])\n",
      "Epoch 835, Loss 4.506505\n",
      "  params :  tensor([  4.6190, -13.0663])\n",
      "  grad :  tensor([-0.1274,  0.7212])\n",
      "Epoch 836, Loss 4.501141\n",
      "  params :  tensor([  4.6203, -13.0735])\n",
      "  grad :  tensor([-0.1272,  0.7200])\n",
      "Epoch 837, Loss 4.495801\n",
      "  params :  tensor([  4.6215, -13.0807])\n",
      "  grad :  tensor([-0.1270,  0.7188])\n",
      "Epoch 838, Loss 4.490475\n",
      "  params :  tensor([  4.6228, -13.0879])\n",
      "  grad :  tensor([-0.1268,  0.7176])\n",
      "Epoch 839, Loss 4.485169\n",
      "  params :  tensor([  4.6241, -13.0950])\n",
      "  grad :  tensor([-0.1266,  0.7163])\n",
      "Epoch 840, Loss 4.479884\n",
      "  params :  tensor([  4.6253, -13.1022])\n",
      "  grad :  tensor([-0.1263,  0.7151])\n",
      "Epoch 841, Loss 4.474613\n",
      "  params :  tensor([  4.6266, -13.1093])\n",
      "  grad :  tensor([-0.1261,  0.7139])\n",
      "Epoch 842, Loss 4.469364\n",
      "  params :  tensor([  4.6278, -13.1165])\n",
      "  grad :  tensor([-0.1259,  0.7127])\n",
      "Epoch 843, Loss 4.464130\n",
      "  params :  tensor([  4.6291, -13.1236])\n",
      "  grad :  tensor([-0.1257,  0.7115])\n",
      "Epoch 844, Loss 4.458913\n",
      "  params :  tensor([  4.6304, -13.1307])\n",
      "  grad :  tensor([-0.1255,  0.7103])\n",
      "Epoch 845, Loss 4.453716\n",
      "  params :  tensor([  4.6316, -13.1378])\n",
      "  grad :  tensor([-0.1253,  0.7091])\n",
      "Epoch 846, Loss 4.448535\n",
      "  params :  tensor([  4.6329, -13.1449])\n",
      "  grad :  tensor([-0.1250,  0.7079])\n",
      "Epoch 847, Loss 4.443372\n",
      "  params :  tensor([  4.6341, -13.1519])\n",
      "  grad :  tensor([-0.1249,  0.7067])\n",
      "Epoch 848, Loss 4.438226\n",
      "  params :  tensor([  4.6353, -13.1590])\n",
      "  grad :  tensor([-0.1246,  0.7055])\n",
      "Epoch 849, Loss 4.433099\n",
      "  params :  tensor([  4.6366, -13.1660])\n",
      "  grad :  tensor([-0.1244,  0.7043])\n",
      "Epoch 850, Loss 4.427990\n",
      "  params :  tensor([  4.6378, -13.1730])\n",
      "  grad :  tensor([-0.1242,  0.7031])\n",
      "Epoch 851, Loss 4.422897\n",
      "  params :  tensor([  4.6391, -13.1801])\n",
      "  grad :  tensor([-0.1240,  0.7019])\n",
      "Epoch 852, Loss 4.417819\n",
      "  params :  tensor([  4.6403, -13.1871])\n",
      "  grad :  tensor([-0.1238,  0.7007])\n",
      "Epoch 853, Loss 4.412762\n",
      "  params :  tensor([  4.6415, -13.1941])\n",
      "  grad :  tensor([-0.1236,  0.6995])\n",
      "Epoch 854, Loss 4.407721\n",
      "  params :  tensor([  4.6428, -13.2010])\n",
      "  grad :  tensor([-0.1234,  0.6983])\n",
      "Epoch 855, Loss 4.402698\n",
      "  params :  tensor([  4.6440, -13.2080])\n",
      "  grad :  tensor([-0.1232,  0.6971])\n",
      "Epoch 856, Loss 4.397688\n",
      "  params :  tensor([  4.6452, -13.2150])\n",
      "  grad :  tensor([-0.1229,  0.6959])\n",
      "Epoch 857, Loss 4.392697\n",
      "  params :  tensor([  4.6465, -13.2219])\n",
      "  grad :  tensor([-0.1227,  0.6948])\n",
      "Epoch 858, Loss 4.387725\n",
      "  params :  tensor([  4.6477, -13.2289])\n",
      "  grad :  tensor([-0.1225,  0.6936])\n",
      "Epoch 859, Loss 4.382770\n",
      "  params :  tensor([  4.6489, -13.2358])\n",
      "  grad :  tensor([-0.1223,  0.6924])\n",
      "Epoch 860, Loss 4.377828\n",
      "  params :  tensor([  4.6501, -13.2427])\n",
      "  grad :  tensor([-0.1221,  0.6912])\n",
      "Epoch 861, Loss 4.372905\n",
      "  params :  tensor([  4.6514, -13.2496])\n",
      "  grad :  tensor([-0.1219,  0.6901])\n",
      "Epoch 862, Loss 4.368000\n",
      "  params :  tensor([  4.6526, -13.2565])\n",
      "  grad :  tensor([-0.1217,  0.6889])\n",
      "Epoch 863, Loss 4.363111\n",
      "  params :  tensor([  4.6538, -13.2634])\n",
      "  grad :  tensor([-0.1215,  0.6877])\n",
      "Epoch 864, Loss 4.358238\n",
      "  params :  tensor([  4.6550, -13.2702])\n",
      "  grad :  tensor([-0.1213,  0.6865])\n",
      "Epoch 865, Loss 4.353383\n",
      "  params :  tensor([  4.6562, -13.2771])\n",
      "  grad :  tensor([-0.1211,  0.6854])\n",
      "Epoch 866, Loss 4.348542\n",
      "  params :  tensor([  4.6574, -13.2839])\n",
      "  grad :  tensor([-0.1209,  0.6842])\n",
      "Epoch 867, Loss 4.343716\n",
      "  params :  tensor([  4.6586, -13.2908])\n",
      "  grad :  tensor([-0.1207,  0.6830])\n",
      "Epoch 868, Loss 4.338911\n",
      "  params :  tensor([  4.6598, -13.2976])\n",
      "  grad :  tensor([-0.1205,  0.6819])\n",
      "Epoch 869, Loss 4.334120\n",
      "  params :  tensor([  4.6610, -13.3044])\n",
      "  grad :  tensor([-0.1203,  0.6807])\n",
      "Epoch 870, Loss 4.329345\n",
      "  params :  tensor([  4.6622, -13.3112])\n",
      "  grad :  tensor([-0.1201,  0.6796])\n",
      "Epoch 871, Loss 4.324588\n",
      "  params :  tensor([  4.6634, -13.3180])\n",
      "  grad :  tensor([-0.1198,  0.6784])\n",
      "Epoch 872, Loss 4.319846\n",
      "  params :  tensor([  4.6646, -13.3247])\n",
      "  grad :  tensor([-0.1196,  0.6773])\n",
      "Epoch 873, Loss 4.315117\n",
      "  params :  tensor([  4.6658, -13.3315])\n",
      "  grad :  tensor([-0.1195,  0.6761])\n",
      "Epoch 874, Loss 4.310409\n",
      "  params :  tensor([  4.6670, -13.3382])\n",
      "  grad :  tensor([-0.1192,  0.6750])\n",
      "Epoch 875, Loss 4.305714\n",
      "  params :  tensor([  4.6682, -13.3450])\n",
      "  grad :  tensor([-0.1190,  0.6738])\n",
      "Epoch 876, Loss 4.301036\n",
      "  params :  tensor([  4.6694, -13.3517])\n",
      "  grad :  tensor([-0.1188,  0.6727])\n",
      "Epoch 877, Loss 4.296376\n",
      "  params :  tensor([  4.6706, -13.3584])\n",
      "  grad :  tensor([-0.1186,  0.6715])\n",
      "Epoch 878, Loss 4.291727\n",
      "  params :  tensor([  4.6718, -13.3651])\n",
      "  grad :  tensor([-0.1184,  0.6704])\n",
      "Epoch 879, Loss 4.287098\n",
      "  params :  tensor([  4.6730, -13.3718])\n",
      "  grad :  tensor([-0.1182,  0.6693])\n",
      "Epoch 880, Loss 4.282482\n",
      "  params :  tensor([  4.6741, -13.3785])\n",
      "  grad :  tensor([-0.1180,  0.6681])\n",
      "Epoch 881, Loss 4.277882\n",
      "  params :  tensor([  4.6753, -13.3852])\n",
      "  grad :  tensor([-0.1178,  0.6670])\n",
      "Epoch 882, Loss 4.273299\n",
      "  params :  tensor([  4.6765, -13.3918])\n",
      "  grad :  tensor([-0.1176,  0.6658])\n",
      "Epoch 883, Loss 4.268732\n",
      "  params :  tensor([  4.6777, -13.3985])\n",
      "  grad :  tensor([-0.1174,  0.6647])\n",
      "Epoch 884, Loss 4.264178\n",
      "  params :  tensor([  4.6788, -13.4051])\n",
      "  grad :  tensor([-0.1172,  0.6636])\n",
      "Epoch 885, Loss 4.259643\n",
      "  params :  tensor([  4.6800, -13.4117])\n",
      "  grad :  tensor([-0.1170,  0.6625])\n",
      "Epoch 886, Loss 4.255120\n",
      "  params :  tensor([  4.6812, -13.4184])\n",
      "  grad :  tensor([-0.1168,  0.6613])\n",
      "Epoch 887, Loss 4.250614\n",
      "  params :  tensor([  4.6823, -13.4250])\n",
      "  grad :  tensor([-0.1166,  0.6602])\n",
      "Epoch 888, Loss 4.246124\n",
      "  params :  tensor([  4.6835, -13.4316])\n",
      "  grad :  tensor([-0.1164,  0.6591])\n",
      "Epoch 889, Loss 4.241648\n",
      "  params :  tensor([  4.6847, -13.4381])\n",
      "  grad :  tensor([-0.1162,  0.6580])\n",
      "Epoch 890, Loss 4.237185\n",
      "  params :  tensor([  4.6858, -13.4447])\n",
      "  grad :  tensor([-0.1160,  0.6569])\n",
      "Epoch 891, Loss 4.232740\n",
      "  params :  tensor([  4.6870, -13.4513])\n",
      "  grad :  tensor([-0.1158,  0.6557])\n",
      "Epoch 892, Loss 4.228308\n",
      "  params :  tensor([  4.6881, -13.4578])\n",
      "  grad :  tensor([-0.1157,  0.6546])\n",
      "Epoch 893, Loss 4.223895\n",
      "  params :  tensor([  4.6893, -13.4643])\n",
      "  grad :  tensor([-0.1154,  0.6535])\n",
      "Epoch 894, Loss 4.219494\n",
      "  params :  tensor([  4.6904, -13.4709])\n",
      "  grad :  tensor([-0.1153,  0.6524])\n",
      "Epoch 895, Loss 4.215109\n",
      "  params :  tensor([  4.6916, -13.4774])\n",
      "  grad :  tensor([-0.1151,  0.6513])\n",
      "Epoch 896, Loss 4.210737\n",
      "  params :  tensor([  4.6927, -13.4839])\n",
      "  grad :  tensor([-0.1148,  0.6502])\n",
      "Epoch 897, Loss 4.206383\n",
      "  params :  tensor([  4.6939, -13.4904])\n",
      "  grad :  tensor([-0.1147,  0.6491])\n",
      "Epoch 898, Loss 4.202043\n",
      "  params :  tensor([  4.6950, -13.4968])\n",
      "  grad :  tensor([-0.1145,  0.6480])\n",
      "Epoch 899, Loss 4.197715\n",
      "  params :  tensor([  4.6962, -13.5033])\n",
      "  grad :  tensor([-0.1143,  0.6469])\n",
      "Epoch 900, Loss 4.193405\n",
      "  params :  tensor([  4.6973, -13.5098])\n",
      "  grad :  tensor([-0.1141,  0.6458])\n",
      "Epoch 901, Loss 4.189108\n",
      "  params :  tensor([  4.6985, -13.5162])\n",
      "  grad :  tensor([-0.1139,  0.6447])\n",
      "Epoch 902, Loss 4.184825\n",
      "  params :  tensor([  4.6996, -13.5227])\n",
      "  grad :  tensor([-0.1137,  0.6436])\n",
      "Epoch 903, Loss 4.180559\n",
      "  params :  tensor([  4.7007, -13.5291])\n",
      "  grad :  tensor([-0.1135,  0.6425])\n",
      "Epoch 904, Loss 4.176305\n",
      "  params :  tensor([  4.7019, -13.5355])\n",
      "  grad :  tensor([-0.1133,  0.6414])\n",
      "Epoch 905, Loss 4.172065\n",
      "  params :  tensor([  4.7030, -13.5419])\n",
      "  grad :  tensor([-0.1131,  0.6403])\n",
      "Epoch 906, Loss 4.167842\n",
      "  params :  tensor([  4.7041, -13.5483])\n",
      "  grad :  tensor([-0.1129,  0.6392])\n",
      "Epoch 907, Loss 4.163630\n",
      "  params :  tensor([  4.7053, -13.5547])\n",
      "  grad :  tensor([-0.1127,  0.6381])\n",
      "Epoch 908, Loss 4.159436\n",
      "  params :  tensor([  4.7064, -13.5610])\n",
      "  grad :  tensor([-0.1125,  0.6371])\n",
      "Epoch 909, Loss 4.155253\n",
      "  params :  tensor([  4.7075, -13.5674])\n",
      "  grad :  tensor([-0.1124,  0.6360])\n",
      "Epoch 910, Loss 4.151086\n",
      "  params :  tensor([  4.7086, -13.5738])\n",
      "  grad :  tensor([-0.1122,  0.6349])\n",
      "Epoch 911, Loss 4.146934\n",
      "  params :  tensor([  4.7097, -13.5801])\n",
      "  grad :  tensor([-0.1120,  0.6338])\n",
      "Epoch 912, Loss 4.142794\n",
      "  params :  tensor([  4.7109, -13.5864])\n",
      "  grad :  tensor([-0.1118,  0.6327])\n",
      "Epoch 913, Loss 4.138669\n",
      "  params :  tensor([  4.7120, -13.5927])\n",
      "  grad :  tensor([-0.1116,  0.6317])\n",
      "Epoch 914, Loss 4.134559\n",
      "  params :  tensor([  4.7131, -13.5990])\n",
      "  grad :  tensor([-0.1114,  0.6306])\n",
      "Epoch 915, Loss 4.130465\n",
      "  params :  tensor([  4.7142, -13.6053])\n",
      "  grad :  tensor([-0.1112,  0.6295])\n",
      "Epoch 916, Loss 4.126378\n",
      "  params :  tensor([  4.7153, -13.6116])\n",
      "  grad :  tensor([-0.1110,  0.6284])\n",
      "Epoch 917, Loss 4.122310\n",
      "  params :  tensor([  4.7164, -13.6179])\n",
      "  grad :  tensor([-0.1108,  0.6274])\n",
      "Epoch 918, Loss 4.118253\n",
      "  params :  tensor([  4.7175, -13.6242])\n",
      "  grad :  tensor([-0.1107,  0.6263])\n",
      "Epoch 919, Loss 4.114213\n",
      "  params :  tensor([  4.7186, -13.6304])\n",
      "  grad :  tensor([-0.1104,  0.6253])\n",
      "Epoch 920, Loss 4.110184\n",
      "  params :  tensor([  4.7197, -13.6367])\n",
      "  grad :  tensor([-0.1103,  0.6242])\n",
      "Epoch 921, Loss 4.106170\n",
      "  params :  tensor([  4.7208, -13.6429])\n",
      "  grad :  tensor([-0.1101,  0.6231])\n",
      "Epoch 922, Loss 4.102171\n",
      "  params :  tensor([  4.7219, -13.6491])\n",
      "  grad :  tensor([-0.1099,  0.6221])\n",
      "Epoch 923, Loss 4.098181\n",
      "  params :  tensor([  4.7230, -13.6553])\n",
      "  grad :  tensor([-0.1097,  0.6210])\n",
      "Epoch 924, Loss 4.094209\n",
      "  params :  tensor([  4.7241, -13.6615])\n",
      "  grad :  tensor([-0.1095,  0.6200])\n",
      "Epoch 925, Loss 4.090250\n",
      "  params :  tensor([  4.7252, -13.6677])\n",
      "  grad :  tensor([-0.1093,  0.6189])\n",
      "Epoch 926, Loss 4.086300\n",
      "  params :  tensor([  4.7263, -13.6739])\n",
      "  grad :  tensor([-0.1091,  0.6179])\n",
      "Epoch 927, Loss 4.082366\n",
      "  params :  tensor([  4.7274, -13.6800])\n",
      "  grad :  tensor([-0.1090,  0.6168])\n",
      "Epoch 928, Loss 4.078448\n",
      "  params :  tensor([  4.7285, -13.6862])\n",
      "  grad :  tensor([-0.1088,  0.6158])\n",
      "Epoch 929, Loss 4.074540\n",
      "  params :  tensor([  4.7296, -13.6924])\n",
      "  grad :  tensor([-0.1086,  0.6147])\n",
      "Epoch 930, Loss 4.070650\n",
      "  params :  tensor([  4.7307, -13.6985])\n",
      "  grad :  tensor([-0.1084,  0.6137])\n",
      "Epoch 931, Loss 4.066769\n",
      "  params :  tensor([  4.7317, -13.7046])\n",
      "  grad :  tensor([-0.1082,  0.6126])\n",
      "Epoch 932, Loss 4.062900\n",
      "  params :  tensor([  4.7328, -13.7107])\n",
      "  grad :  tensor([-0.1080,  0.6116])\n",
      "Epoch 933, Loss 4.059047\n",
      "  params :  tensor([  4.7339, -13.7168])\n",
      "  grad :  tensor([-0.1079,  0.6105])\n",
      "Epoch 934, Loss 4.055204\n",
      "  params :  tensor([  4.7350, -13.7229])\n",
      "  grad :  tensor([-0.1077,  0.6095])\n",
      "Epoch 935, Loss 4.051378\n",
      "  params :  tensor([  4.7360, -13.7290])\n",
      "  grad :  tensor([-0.1075,  0.6085])\n",
      "Epoch 936, Loss 4.047564\n",
      "  params :  tensor([  4.7371, -13.7351])\n",
      "  grad :  tensor([-0.1073,  0.6074])\n",
      "Epoch 937, Loss 4.043762\n",
      "  params :  tensor([  4.7382, -13.7412])\n",
      "  grad :  tensor([-0.1071,  0.6064])\n",
      "Epoch 938, Loss 4.039972\n",
      "  params :  tensor([  4.7393, -13.7472])\n",
      "  grad :  tensor([-0.1069,  0.6054])\n",
      "Epoch 939, Loss 4.036197\n",
      "  params :  tensor([  4.7403, -13.7533])\n",
      "  grad :  tensor([-0.1068,  0.6043])\n",
      "Epoch 940, Loss 4.032433\n",
      "  params :  tensor([  4.7414, -13.7593])\n",
      "  grad :  tensor([-0.1066,  0.6033])\n",
      "Epoch 941, Loss 4.028685\n",
      "  params :  tensor([  4.7425, -13.7653])\n",
      "  grad :  tensor([-0.1064,  0.6023])\n",
      "Epoch 942, Loss 4.024947\n",
      "  params :  tensor([  4.7435, -13.7713])\n",
      "  grad :  tensor([-0.1062,  0.6013])\n",
      "Epoch 943, Loss 4.021221\n",
      "  params :  tensor([  4.7446, -13.7773])\n",
      "  grad :  tensor([-0.1060,  0.6003])\n",
      "Epoch 944, Loss 4.017508\n",
      "  params :  tensor([  4.7456, -13.7833])\n",
      "  grad :  tensor([-0.1058,  0.5992])\n",
      "Epoch 945, Loss 4.013809\n",
      "  params :  tensor([  4.7467, -13.7893])\n",
      "  grad :  tensor([-0.1057,  0.5982])\n",
      "Epoch 946, Loss 4.010123\n",
      "  params :  tensor([  4.7478, -13.7953])\n",
      "  grad :  tensor([-0.1055,  0.5972])\n",
      "Epoch 947, Loss 4.006446\n",
      "  params :  tensor([  4.7488, -13.8012])\n",
      "  grad :  tensor([-0.1053,  0.5962])\n",
      "Epoch 948, Loss 4.002786\n",
      "  params :  tensor([  4.7499, -13.8072])\n",
      "  grad :  tensor([-0.1051,  0.5952])\n",
      "Epoch 949, Loss 3.999135\n",
      "  params :  tensor([  4.7509, -13.8131])\n",
      "  grad :  tensor([-0.1050,  0.5942])\n",
      "Epoch 950, Loss 3.995498\n",
      "  params :  tensor([  4.7520, -13.8191])\n",
      "  grad :  tensor([-0.1048,  0.5931])\n",
      "Epoch 951, Loss 3.991874\n",
      "  params :  tensor([  4.7530, -13.8250])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad :  tensor([-0.1046,  0.5921])\n",
      "Epoch 952, Loss 3.988261\n",
      "  params :  tensor([  4.7540, -13.8309])\n",
      "  grad :  tensor([-0.1044,  0.5911])\n",
      "Epoch 953, Loss 3.984660\n",
      "  params :  tensor([  4.7551, -13.8368])\n",
      "  grad :  tensor([-0.1042,  0.5901])\n",
      "Epoch 954, Loss 3.981073\n",
      "  params :  tensor([  4.7561, -13.8427])\n",
      "  grad :  tensor([-0.1041,  0.5891])\n",
      "Epoch 955, Loss 3.977496\n",
      "  params :  tensor([  4.7572, -13.8486])\n",
      "  grad :  tensor([-0.1039,  0.5881])\n",
      "Epoch 956, Loss 3.973931\n",
      "  params :  tensor([  4.7582, -13.8544])\n",
      "  grad :  tensor([-0.1037,  0.5871])\n",
      "Epoch 957, Loss 3.970381\n",
      "  params :  tensor([  4.7592, -13.8603])\n",
      "  grad :  tensor([-0.1035,  0.5861])\n",
      "Epoch 958, Loss 3.966841\n",
      "  params :  tensor([  4.7603, -13.8661])\n",
      "  grad :  tensor([-0.1034,  0.5851])\n",
      "Epoch 959, Loss 3.963313\n",
      "  params :  tensor([  4.7613, -13.8720])\n",
      "  grad :  tensor([-0.1032,  0.5841])\n",
      "Epoch 960, Loss 3.959796\n",
      "  params :  tensor([  4.7623, -13.8778])\n",
      "  grad :  tensor([-0.1030,  0.5831])\n",
      "Epoch 961, Loss 3.956295\n",
      "  params :  tensor([  4.7634, -13.8836])\n",
      "  grad :  tensor([-0.1028,  0.5822])\n",
      "Epoch 962, Loss 3.952801\n",
      "  params :  tensor([  4.7644, -13.8895])\n",
      "  grad :  tensor([-0.1026,  0.5812])\n",
      "Epoch 963, Loss 3.949323\n",
      "  params :  tensor([  4.7654, -13.8953])\n",
      "  grad :  tensor([-0.1025,  0.5802])\n",
      "Epoch 964, Loss 3.945855\n",
      "  params :  tensor([  4.7664, -13.9010])\n",
      "  grad :  tensor([-0.1023,  0.5792])\n",
      "Epoch 965, Loss 3.942398\n",
      "  params :  tensor([  4.7675, -13.9068])\n",
      "  grad :  tensor([-0.1021,  0.5782])\n",
      "Epoch 966, Loss 3.938954\n",
      "  params :  tensor([  4.7685, -13.9126])\n",
      "  grad :  tensor([-0.1020,  0.5772])\n",
      "Epoch 967, Loss 3.935520\n",
      "  params :  tensor([  4.7695, -13.9184])\n",
      "  grad :  tensor([-0.1018,  0.5762])\n",
      "Epoch 968, Loss 3.932096\n",
      "  params :  tensor([  4.7705, -13.9241])\n",
      "  grad :  tensor([-0.1016,  0.5753])\n",
      "Epoch 969, Loss 3.928688\n",
      "  params :  tensor([  4.7715, -13.9299])\n",
      "  grad :  tensor([-0.1015,  0.5743])\n",
      "Epoch 970, Loss 3.925292\n",
      "  params :  tensor([  4.7725, -13.9356])\n",
      "  grad :  tensor([-0.1013,  0.5733])\n",
      "Epoch 971, Loss 3.921906\n",
      "  params :  tensor([  4.7736, -13.9413])\n",
      "  grad :  tensor([-0.1011,  0.5723])\n",
      "Epoch 972, Loss 3.918527\n",
      "  params :  tensor([  4.7746, -13.9470])\n",
      "  grad :  tensor([-0.1009,  0.5714])\n",
      "Epoch 973, Loss 3.915166\n",
      "  params :  tensor([  4.7756, -13.9527])\n",
      "  grad :  tensor([-0.1008,  0.5704])\n",
      "Epoch 974, Loss 3.911815\n",
      "  params :  tensor([  4.7766, -13.9584])\n",
      "  grad :  tensor([-0.1006,  0.5694])\n",
      "Epoch 975, Loss 3.908474\n",
      "  params :  tensor([  4.7776, -13.9641])\n",
      "  grad :  tensor([-0.1004,  0.5685])\n",
      "Epoch 976, Loss 3.905143\n",
      "  params :  tensor([  4.7786, -13.9698])\n",
      "  grad :  tensor([-0.1003,  0.5675])\n",
      "Epoch 977, Loss 3.901825\n",
      "  params :  tensor([  4.7796, -13.9755])\n",
      "  grad :  tensor([-0.1001,  0.5665])\n",
      "Epoch 978, Loss 3.898517\n",
      "  params :  tensor([  4.7806, -13.9811])\n",
      "  grad :  tensor([-0.0999,  0.5656])\n",
      "Epoch 979, Loss 3.895222\n",
      "  params :  tensor([  4.7816, -13.9868])\n",
      "  grad :  tensor([-0.0997,  0.5646])\n",
      "Epoch 980, Loss 3.891935\n",
      "  params :  tensor([  4.7826, -13.9924])\n",
      "  grad :  tensor([-0.0996,  0.5637])\n",
      "Epoch 981, Loss 3.888664\n",
      "  params :  tensor([  4.7836, -13.9980])\n",
      "  grad :  tensor([-0.0994,  0.5627])\n",
      "Epoch 982, Loss 3.885401\n",
      "  params :  tensor([  4.7846, -14.0036])\n",
      "  grad :  tensor([-0.0992,  0.5617])\n",
      "Epoch 983, Loss 3.882150\n",
      "  params :  tensor([  4.7856, -14.0092])\n",
      "  grad :  tensor([-0.0991,  0.5608])\n",
      "Epoch 984, Loss 3.878910\n",
      "  params :  tensor([  4.7865, -14.0148])\n",
      "  grad :  tensor([-0.0989,  0.5598])\n",
      "Epoch 985, Loss 3.875680\n",
      "  params :  tensor([  4.7875, -14.0204])\n",
      "  grad :  tensor([-0.0987,  0.5589])\n",
      "Epoch 986, Loss 3.872463\n",
      "  params :  tensor([  4.7885, -14.0260])\n",
      "  grad :  tensor([-0.0986,  0.5579])\n",
      "Epoch 987, Loss 3.869256\n",
      "  params :  tensor([  4.7895, -14.0316])\n",
      "  grad :  tensor([-0.0984,  0.5570])\n",
      "Epoch 988, Loss 3.866060\n",
      "  params :  tensor([  4.7905, -14.0371])\n",
      "  grad :  tensor([-0.0982,  0.5560])\n",
      "Epoch 989, Loss 3.862872\n",
      "  params :  tensor([  4.7915, -14.0427])\n",
      "  grad :  tensor([-0.0981,  0.5551])\n",
      "Epoch 990, Loss 3.859699\n",
      "  params :  tensor([  4.7924, -14.0482])\n",
      "  grad :  tensor([-0.0979,  0.5541])\n",
      "Epoch 991, Loss 3.856535\n",
      "  params :  tensor([  4.7934, -14.0538])\n",
      "  grad :  tensor([-0.0978,  0.5532])\n",
      "Epoch 992, Loss 3.853381\n",
      "  params :  tensor([  4.7944, -14.0593])\n",
      "  grad :  tensor([-0.0976,  0.5523])\n",
      "Epoch 993, Loss 3.850237\n",
      "  params :  tensor([  4.7954, -14.0648])\n",
      "  grad :  tensor([-0.0974,  0.5513])\n",
      "Epoch 994, Loss 3.847109\n",
      "  params :  tensor([  4.7963, -14.0703])\n",
      "  grad :  tensor([-0.0973,  0.5504])\n",
      "Epoch 995, Loss 3.843984\n",
      "  params :  tensor([  4.7973, -14.0758])\n",
      "  grad :  tensor([-0.0971,  0.5495])\n",
      "Epoch 996, Loss 3.840876\n",
      "  params :  tensor([  4.7983, -14.0813])\n",
      "  grad :  tensor([-0.0969,  0.5485])\n",
      "Epoch 997, Loss 3.837775\n",
      "  params :  tensor([  4.7992, -14.0868])\n",
      "  grad :  tensor([-0.0967,  0.5476])\n",
      "Epoch 998, Loss 3.834686\n",
      "  params :  tensor([  4.8002, -14.0922])\n",
      "  grad :  tensor([-0.0966,  0.5467])\n",
      "Epoch 999, Loss 3.831606\n",
      "  params :  tensor([  4.8012, -14.0977])\n",
      "  grad :  tensor([-0.0964,  0.5457])\n",
      "Epoch 1000, Loss 3.828538\n",
      "  params :  tensor([  4.8021, -14.1031])\n",
      "  grad :  tensor([-0.0962,  0.5448])\n",
      "Epoch 1001, Loss 3.825483\n",
      "  params :  tensor([  4.8031, -14.1086])\n",
      "  grad :  tensor([-0.0961,  0.5439])\n",
      "Epoch 1002, Loss 3.822433\n",
      "  params :  tensor([  4.8041, -14.1140])\n",
      "  grad :  tensor([-0.0959,  0.5430])\n",
      "Epoch 1003, Loss 3.819398\n",
      "  params :  tensor([  4.8050, -14.1194])\n",
      "  grad :  tensor([-0.0957,  0.5420])\n",
      "Epoch 1004, Loss 3.816369\n",
      "  params :  tensor([  4.8060, -14.1248])\n",
      "  grad :  tensor([-0.0956,  0.5411])\n",
      "Epoch 1005, Loss 3.813350\n",
      "  params :  tensor([  4.8069, -14.1302])\n",
      "  grad :  tensor([-0.0954,  0.5402])\n",
      "Epoch 1006, Loss 3.810344\n",
      "  params :  tensor([  4.8079, -14.1356])\n",
      "  grad :  tensor([-0.0953,  0.5393])\n",
      "Epoch 1007, Loss 3.807348\n",
      "  params :  tensor([  4.8088, -14.1410])\n",
      "  grad :  tensor([-0.0951,  0.5384])\n",
      "Epoch 1008, Loss 3.804360\n",
      "  params :  tensor([  4.8098, -14.1464])\n",
      "  grad :  tensor([-0.0949,  0.5375])\n",
      "Epoch 1009, Loss 3.801384\n",
      "  params :  tensor([  4.8107, -14.1518])\n",
      "  grad :  tensor([-0.0948,  0.5365])\n",
      "Epoch 1010, Loss 3.798421\n",
      "  params :  tensor([  4.8117, -14.1571])\n",
      "  grad :  tensor([-0.0946,  0.5356])\n",
      "Epoch 1011, Loss 3.795465\n",
      "  params :  tensor([  4.8126, -14.1625])\n",
      "  grad :  tensor([-0.0945,  0.5347])\n",
      "Epoch 1012, Loss 3.792518\n",
      "  params :  tensor([  4.8136, -14.1678])\n",
      "  grad :  tensor([-0.0943,  0.5338])\n",
      "Epoch 1013, Loss 3.789584\n",
      "  params :  tensor([  4.8145, -14.1731])\n",
      "  grad :  tensor([-0.0942,  0.5329])\n",
      "Epoch 1014, Loss 3.786658\n",
      "  params :  tensor([  4.8154, -14.1784])\n",
      "  grad :  tensor([-0.0940,  0.5320])\n",
      "Epoch 1015, Loss 3.783740\n",
      "  params :  tensor([  4.8164, -14.1838])\n",
      "  grad :  tensor([-0.0938,  0.5311])\n",
      "Epoch 1016, Loss 3.780832\n",
      "  params :  tensor([  4.8173, -14.1891])\n",
      "  grad :  tensor([-0.0937,  0.5302])\n",
      "Epoch 1017, Loss 3.777939\n",
      "  params :  tensor([  4.8183, -14.1943])\n",
      "  grad :  tensor([-0.0935,  0.5293])\n",
      "Epoch 1018, Loss 3.775053\n",
      "  params :  tensor([  4.8192, -14.1996])\n",
      "  grad :  tensor([-0.0933,  0.5284])\n",
      "Epoch 1019, Loss 3.772173\n",
      "  params :  tensor([  4.8201, -14.2049])\n",
      "  grad :  tensor([-0.0932,  0.5275])\n",
      "Epoch 1020, Loss 3.769311\n",
      "  params :  tensor([  4.8210, -14.2102])\n",
      "  grad :  tensor([-0.0930,  0.5266])\n",
      "Epoch 1021, Loss 3.766450\n",
      "  params :  tensor([  4.8220, -14.2154])\n",
      "  grad :  tensor([-0.0929,  0.5257])\n",
      "Epoch 1022, Loss 3.763602\n",
      "  params :  tensor([  4.8229, -14.2207])\n",
      "  grad :  tensor([-0.0927,  0.5248])\n",
      "Epoch 1023, Loss 3.760766\n",
      "  params :  tensor([  4.8238, -14.2259])\n",
      "  grad :  tensor([-0.0926,  0.5239])\n",
      "Epoch 1024, Loss 3.757936\n",
      "  params :  tensor([  4.8248, -14.2311])\n",
      "  grad :  tensor([-0.0924,  0.5230])\n",
      "Epoch 1025, Loss 3.755118\n",
      "  params :  tensor([  4.8257, -14.2364])\n",
      "  grad :  tensor([-0.0922,  0.5221])\n",
      "Epoch 1026, Loss 3.752309\n",
      "  params :  tensor([  4.8266, -14.2416])\n",
      "  grad :  tensor([-0.0921,  0.5213])\n",
      "Epoch 1027, Loss 3.749511\n",
      "  params :  tensor([  4.8275, -14.2468])\n",
      "  grad :  tensor([-0.0919,  0.5204])\n",
      "Epoch 1028, Loss 3.746722\n",
      "  params :  tensor([  4.8284, -14.2520])\n",
      "  grad :  tensor([-0.0918,  0.5195])\n",
      "Epoch 1029, Loss 3.743940\n",
      "  params :  tensor([  4.8293, -14.2572])\n",
      "  grad :  tensor([-0.0916,  0.5186])\n",
      "Epoch 1030, Loss 3.741169\n",
      "  params :  tensor([  4.8303, -14.2623])\n",
      "  grad :  tensor([-0.0915,  0.5177])\n",
      "Epoch 1031, Loss 3.738407\n",
      "  params :  tensor([  4.8312, -14.2675])\n",
      "  grad :  tensor([-0.0913,  0.5168])\n",
      "Epoch 1032, Loss 3.735656\n",
      "  params :  tensor([  4.8321, -14.2727])\n",
      "  grad :  tensor([-0.0912,  0.5160])\n",
      "Epoch 1033, Loss 3.732914\n",
      "  params :  tensor([  4.8330, -14.2778])\n",
      "  grad :  tensor([-0.0910,  0.5151])\n",
      "Epoch 1034, Loss 3.730181\n",
      "  params :  tensor([  4.8339, -14.2830])\n",
      "  grad :  tensor([-0.0908,  0.5142])\n",
      "Epoch 1035, Loss 3.727456\n",
      "  params :  tensor([  4.8348, -14.2881])\n",
      "  grad :  tensor([-0.0907,  0.5133])\n",
      "Epoch 1036, Loss 3.724740\n",
      "  params :  tensor([  4.8357, -14.2932])\n",
      "  grad :  tensor([-0.0905,  0.5125])\n",
      "Epoch 1037, Loss 3.722034\n",
      "  params :  tensor([  4.8366, -14.2983])\n",
      "  grad :  tensor([-0.0904,  0.5116])\n",
      "Epoch 1038, Loss 3.719337\n",
      "  params :  tensor([  4.8375, -14.3034])\n",
      "  grad :  tensor([-0.0902,  0.5107])\n",
      "Epoch 1039, Loss 3.716651\n",
      "  params :  tensor([  4.8384, -14.3085])\n",
      "  grad :  tensor([-0.0901,  0.5099])\n",
      "Epoch 1040, Loss 3.713972\n",
      "  params :  tensor([  4.8393, -14.3136])\n",
      "  grad :  tensor([-0.0899,  0.5090])\n",
      "Epoch 1041, Loss 3.711302\n",
      "  params :  tensor([  4.8402, -14.3187])\n",
      "  grad :  tensor([-0.0898,  0.5081])\n",
      "Epoch 1042, Loss 3.708644\n",
      "  params :  tensor([  4.8411, -14.3238])\n",
      "  grad :  tensor([-0.0896,  0.5073])\n",
      "Epoch 1043, Loss 3.705991\n",
      "  params :  tensor([  4.8420, -14.3288])\n",
      "  grad :  tensor([-0.0895,  0.5064])\n",
      "Epoch 1044, Loss 3.703351\n",
      "  params :  tensor([  4.8429, -14.3339])\n",
      "  grad :  tensor([-0.0893,  0.5055])\n",
      "Epoch 1045, Loss 3.700716\n",
      "  params :  tensor([  4.8438, -14.3390])\n",
      "  grad :  tensor([-0.0892,  0.5047])\n",
      "Epoch 1046, Loss 3.698091\n",
      "  params :  tensor([  4.8447, -14.3440])\n",
      "  grad :  tensor([-0.0890,  0.5038])\n",
      "Epoch 1047, Loss 3.695476\n",
      "  params :  tensor([  4.8456, -14.3490])\n",
      "  grad :  tensor([-0.0888,  0.5030])\n",
      "Epoch 1048, Loss 3.692869\n",
      "  params :  tensor([  4.8465, -14.3540])\n",
      "  grad :  tensor([-0.0887,  0.5021])\n",
      "Epoch 1049, Loss 3.690273\n",
      "  params :  tensor([  4.8473, -14.3591])\n",
      "  grad :  tensor([-0.0886,  0.5013])\n",
      "Epoch 1050, Loss 3.687683\n",
      "  params :  tensor([  4.8482, -14.3641])\n",
      "  grad :  tensor([-0.0884,  0.5004])\n",
      "Epoch 1051, Loss 3.685104\n",
      "  params :  tensor([  4.8491, -14.3691])\n",
      "  grad :  tensor([-0.0882,  0.4996])\n",
      "Epoch 1052, Loss 3.682532\n",
      "  params :  tensor([  4.8500, -14.3740])\n",
      "  grad :  tensor([-0.0881,  0.4987])\n",
      "Epoch 1053, Loss 3.679969\n",
      "  params :  tensor([  4.8509, -14.3790])\n",
      "  grad :  tensor([-0.0879,  0.4979])\n",
      "Epoch 1054, Loss 3.677417\n",
      "  params :  tensor([  4.8518, -14.3840])\n",
      "  grad :  tensor([-0.0878,  0.4970])\n",
      "Epoch 1055, Loss 3.674871\n",
      "  params :  tensor([  4.8526, -14.3889])\n",
      "  grad :  tensor([-0.0877,  0.4962])\n",
      "Epoch 1056, Loss 3.672335\n",
      "  params :  tensor([  4.8535, -14.3939])\n",
      "  grad :  tensor([-0.0875,  0.4953])\n",
      "Epoch 1057, Loss 3.669804\n",
      "  params :  tensor([  4.8544, -14.3988])\n",
      "  grad :  tensor([-0.0873,  0.4945])\n",
      "Epoch 1058, Loss 3.667287\n",
      "  params :  tensor([  4.8552, -14.4038])\n",
      "  grad :  tensor([-0.0872,  0.4936])\n",
      "Epoch 1059, Loss 3.664775\n",
      "  params :  tensor([  4.8561, -14.4087])\n",
      "  grad :  tensor([-0.0870,  0.4928])\n",
      "Epoch 1060, Loss 3.662273\n",
      "  params :  tensor([  4.8570, -14.4136])\n",
      "  grad :  tensor([-0.0869,  0.4920])\n",
      "Epoch 1061, Loss 3.659778\n",
      "  params :  tensor([  4.8579, -14.4185])\n",
      "  grad :  tensor([-0.0868,  0.4911])\n",
      "Epoch 1062, Loss 3.657295\n",
      "  params :  tensor([  4.8587, -14.4234])\n",
      "  grad :  tensor([-0.0866,  0.4903])\n",
      "Epoch 1063, Loss 3.654816\n",
      "  params :  tensor([  4.8596, -14.4283])\n",
      "  grad :  tensor([-0.0865,  0.4895])\n",
      "Epoch 1064, Loss 3.652349\n",
      "  params :  tensor([  4.8604, -14.4332])\n",
      "  grad :  tensor([-0.0863,  0.4886])\n",
      "Epoch 1065, Loss 3.649889\n",
      "  params :  tensor([  4.8613, -14.4381])\n",
      "  grad :  tensor([-0.0862,  0.4878])\n",
      "Epoch 1066, Loss 3.647437\n",
      "  params :  tensor([  4.8622, -14.4430])\n",
      "  grad :  tensor([-0.0860,  0.4870])\n",
      "Epoch 1067, Loss 3.644991\n",
      "  params :  tensor([  4.8630, -14.4478])\n",
      "  grad :  tensor([-0.0859,  0.4862])\n",
      "Epoch 1068, Loss 3.642559\n",
      "  params :  tensor([  4.8639, -14.4527])\n",
      "  grad :  tensor([-0.0857,  0.4853])\n",
      "Epoch 1069, Loss 3.640132\n",
      "  params :  tensor([  4.8647, -14.4575])\n",
      "  grad :  tensor([-0.0856,  0.4845])\n",
      "Epoch 1070, Loss 3.637711\n",
      "  params :  tensor([  4.8656, -14.4624])\n",
      "  grad :  tensor([-0.0854,  0.4837])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1071, Loss 3.635302\n",
      "  params :  tensor([  4.8665, -14.4672])\n",
      "  grad :  tensor([-0.0853,  0.4829])\n",
      "Epoch 1072, Loss 3.632902\n",
      "  params :  tensor([  4.8673, -14.4720])\n",
      "  grad :  tensor([-0.0851,  0.4820])\n",
      "Epoch 1073, Loss 3.630508\n",
      "  params :  tensor([  4.8682, -14.4768])\n",
      "  grad :  tensor([-0.0850,  0.4812])\n",
      "Epoch 1074, Loss 3.628119\n",
      "  params :  tensor([  4.8690, -14.4816])\n",
      "  grad :  tensor([-0.0849,  0.4804])\n",
      "Epoch 1075, Loss 3.625741\n",
      "  params :  tensor([  4.8698, -14.4864])\n",
      "  grad :  tensor([-0.0847,  0.4796])\n",
      "Epoch 1076, Loss 3.623374\n",
      "  params :  tensor([  4.8707, -14.4912])\n",
      "  grad :  tensor([-0.0846,  0.4788])\n",
      "Epoch 1077, Loss 3.621010\n",
      "  params :  tensor([  4.8715, -14.4960])\n",
      "  grad :  tensor([-0.0844,  0.4780])\n",
      "Epoch 1078, Loss 3.618659\n",
      "  params :  tensor([  4.8724, -14.5008])\n",
      "  grad :  tensor([-0.0843,  0.4771])\n",
      "Epoch 1079, Loss 3.616311\n",
      "  params :  tensor([  4.8732, -14.5055])\n",
      "  grad :  tensor([-0.0841,  0.4763])\n",
      "Epoch 1080, Loss 3.613973\n",
      "  params :  tensor([  4.8741, -14.5103])\n",
      "  grad :  tensor([-0.0840,  0.4755])\n",
      "Epoch 1081, Loss 3.611643\n",
      "  params :  tensor([  4.8749, -14.5150])\n",
      "  grad :  tensor([-0.0839,  0.4747])\n",
      "Epoch 1082, Loss 3.609321\n",
      "  params :  tensor([  4.8757, -14.5198])\n",
      "  grad :  tensor([-0.0837,  0.4739])\n",
      "Epoch 1083, Loss 3.607008\n",
      "  params :  tensor([  4.8766, -14.5245])\n",
      "  grad :  tensor([-0.0836,  0.4731])\n",
      "Epoch 1084, Loss 3.604701\n",
      "  params :  tensor([  4.8774, -14.5292])\n",
      "  grad :  tensor([-0.0834,  0.4723])\n",
      "Epoch 1085, Loss 3.602403\n",
      "  params :  tensor([  4.8782, -14.5339])\n",
      "  grad :  tensor([-0.0833,  0.4715])\n",
      "Epoch 1086, Loss 3.600114\n",
      "  params :  tensor([  4.8791, -14.5387])\n",
      "  grad :  tensor([-0.0832,  0.4707])\n",
      "Epoch 1087, Loss 3.597831\n",
      "  params :  tensor([  4.8799, -14.5434])\n",
      "  grad :  tensor([-0.0830,  0.4699])\n",
      "Epoch 1088, Loss 3.595553\n",
      "  params :  tensor([  4.8807, -14.5480])\n",
      "  grad :  tensor([-0.0829,  0.4691])\n",
      "Epoch 1089, Loss 3.593287\n",
      "  params :  tensor([  4.8816, -14.5527])\n",
      "  grad :  tensor([-0.0827,  0.4683])\n",
      "Epoch 1090, Loss 3.591030\n",
      "  params :  tensor([  4.8824, -14.5574])\n",
      "  grad :  tensor([-0.0826,  0.4675])\n",
      "Epoch 1091, Loss 3.588776\n",
      "  params :  tensor([  4.8832, -14.5621])\n",
      "  grad :  tensor([-0.0824,  0.4667])\n",
      "Epoch 1092, Loss 3.586534\n",
      "  params :  tensor([  4.8840, -14.5667])\n",
      "  grad :  tensor([-0.0823,  0.4659])\n",
      "Epoch 1093, Loss 3.584294\n",
      "  params :  tensor([  4.8849, -14.5714])\n",
      "  grad :  tensor([-0.0822,  0.4651])\n",
      "Epoch 1094, Loss 3.582067\n",
      "  params :  tensor([  4.8857, -14.5760])\n",
      "  grad :  tensor([-0.0820,  0.4643])\n",
      "Epoch 1095, Loss 3.579845\n",
      "  params :  tensor([  4.8865, -14.5807])\n",
      "  grad :  tensor([-0.0819,  0.4636])\n",
      "Epoch 1096, Loss 3.577631\n",
      "  params :  tensor([  4.8873, -14.5853])\n",
      "  grad :  tensor([-0.0818,  0.4628])\n",
      "Epoch 1097, Loss 3.575424\n",
      "  params :  tensor([  4.8881, -14.5899])\n",
      "  grad :  tensor([-0.0816,  0.4620])\n",
      "Epoch 1098, Loss 3.573225\n",
      "  params :  tensor([  4.8889, -14.5945])\n",
      "  grad :  tensor([-0.0815,  0.4612])\n",
      "Epoch 1099, Loss 3.571035\n",
      "  params :  tensor([  4.8898, -14.5991])\n",
      "  grad :  tensor([-0.0813,  0.4604])\n",
      "Epoch 1100, Loss 3.568848\n",
      "  params :  tensor([  4.8906, -14.6037])\n",
      "  grad :  tensor([-0.0812,  0.4596])\n",
      "Epoch 1101, Loss 3.566673\n",
      "  params :  tensor([  4.8914, -14.6083])\n",
      "  grad :  tensor([-0.0810,  0.4588])\n",
      "Epoch 1102, Loss 3.564506\n",
      "  params :  tensor([  4.8922, -14.6129])\n",
      "  grad :  tensor([-0.0809,  0.4581])\n",
      "Epoch 1103, Loss 3.562341\n",
      "  params :  tensor([  4.8930, -14.6175])\n",
      "  grad :  tensor([-0.0808,  0.4573])\n",
      "Epoch 1104, Loss 3.560185\n",
      "  params :  tensor([  4.8938, -14.6220])\n",
      "  grad :  tensor([-0.0806,  0.4565])\n",
      "Epoch 1105, Loss 3.558040\n",
      "  params :  tensor([  4.8946, -14.6266])\n",
      "  grad :  tensor([-0.0805,  0.4557])\n",
      "Epoch 1106, Loss 3.555901\n",
      "  params :  tensor([  4.8954, -14.6311])\n",
      "  grad :  tensor([-0.0804,  0.4550])\n",
      "Epoch 1107, Loss 3.553767\n",
      "  params :  tensor([  4.8962, -14.6357])\n",
      "  grad :  tensor([-0.0802,  0.4542])\n",
      "Epoch 1108, Loss 3.551641\n",
      "  params :  tensor([  4.8970, -14.6402])\n",
      "  grad :  tensor([-0.0801,  0.4534])\n",
      "Epoch 1109, Loss 3.549524\n",
      "  params :  tensor([  4.8978, -14.6447])\n",
      "  grad :  tensor([-0.0799,  0.4527])\n",
      "Epoch 1110, Loss 3.547411\n",
      "  params :  tensor([  4.8986, -14.6493])\n",
      "  grad :  tensor([-0.0798,  0.4519])\n",
      "Epoch 1111, Loss 3.545309\n",
      "  params :  tensor([  4.8994, -14.6538])\n",
      "  grad :  tensor([-0.0797,  0.4511])\n",
      "Epoch 1112, Loss 3.543211\n",
      "  params :  tensor([  4.9002, -14.6583])\n",
      "  grad :  tensor([-0.0796,  0.4503])\n",
      "Epoch 1113, Loss 3.541124\n",
      "  params :  tensor([  4.9010, -14.6628])\n",
      "  grad :  tensor([-0.0794,  0.4496])\n",
      "Epoch 1114, Loss 3.539041\n",
      "  params :  tensor([  4.9018, -14.6673])\n",
      "  grad :  tensor([-0.0793,  0.4488])\n",
      "Epoch 1115, Loss 3.536967\n",
      "  params :  tensor([  4.9026, -14.6717])\n",
      "  grad :  tensor([-0.0791,  0.4481])\n",
      "Epoch 1116, Loss 3.534896\n",
      "  params :  tensor([  4.9034, -14.6762])\n",
      "  grad :  tensor([-0.0790,  0.4473])\n",
      "Epoch 1117, Loss 3.532835\n",
      "  params :  tensor([  4.9042, -14.6807])\n",
      "  grad :  tensor([-0.0789,  0.4465])\n",
      "Epoch 1118, Loss 3.530781\n",
      "  params :  tensor([  4.9049, -14.6851])\n",
      "  grad :  tensor([-0.0787,  0.4458])\n",
      "Epoch 1119, Loss 3.528734\n",
      "  params :  tensor([  4.9057, -14.6896])\n",
      "  grad :  tensor([-0.0786,  0.4450])\n",
      "Epoch 1120, Loss 3.526694\n",
      "  params :  tensor([  4.9065, -14.6940])\n",
      "  grad :  tensor([-0.0785,  0.4443])\n",
      "Epoch 1121, Loss 3.524662\n",
      "  params :  tensor([  4.9073, -14.6985])\n",
      "  grad :  tensor([-0.0784,  0.4435])\n",
      "Epoch 1122, Loss 3.522633\n",
      "  params :  tensor([  4.9081, -14.7029])\n",
      "  grad :  tensor([-0.0782,  0.4428])\n",
      "Epoch 1123, Loss 3.520614\n",
      "  params :  tensor([  4.9089, -14.7073])\n",
      "  grad :  tensor([-0.0781,  0.4420])\n",
      "Epoch 1124, Loss 3.518601\n",
      "  params :  tensor([  4.9096, -14.7117])\n",
      "  grad :  tensor([-0.0779,  0.4413])\n",
      "Epoch 1125, Loss 3.516594\n",
      "  params :  tensor([  4.9104, -14.7161])\n",
      "  grad :  tensor([-0.0778,  0.4405])\n",
      "Epoch 1126, Loss 3.514594\n",
      "  params :  tensor([  4.9112, -14.7205])\n",
      "  grad :  tensor([-0.0777,  0.4398])\n",
      "Epoch 1127, Loss 3.512602\n",
      "  params :  tensor([  4.9120, -14.7249])\n",
      "  grad :  tensor([-0.0775,  0.4390])\n",
      "Epoch 1128, Loss 3.510619\n",
      "  params :  tensor([  4.9128, -14.7293])\n",
      "  grad :  tensor([-0.0774,  0.4383])\n",
      "Epoch 1129, Loss 3.508637\n",
      "  params :  tensor([  4.9135, -14.7337])\n",
      "  grad :  tensor([-0.0773,  0.4375])\n",
      "Epoch 1130, Loss 3.506665\n",
      "  params :  tensor([  4.9143, -14.7380])\n",
      "  grad :  tensor([-0.0772,  0.4368])\n",
      "Epoch 1131, Loss 3.504699\n",
      "  params :  tensor([  4.9151, -14.7424])\n",
      "  grad :  tensor([-0.0770,  0.4360])\n",
      "Epoch 1132, Loss 3.502741\n",
      "  params :  tensor([  4.9158, -14.7467])\n",
      "  grad :  tensor([-0.0769,  0.4353])\n",
      "Epoch 1133, Loss 3.500789\n",
      "  params :  tensor([  4.9166, -14.7511])\n",
      "  grad :  tensor([-0.0767,  0.4346])\n",
      "Epoch 1134, Loss 3.498843\n",
      "  params :  tensor([  4.9174, -14.7554])\n",
      "  grad :  tensor([-0.0766,  0.4338])\n",
      "Epoch 1135, Loss 3.496905\n",
      "  params :  tensor([  4.9181, -14.7598])\n",
      "  grad :  tensor([-0.0765,  0.4331])\n",
      "Epoch 1136, Loss 3.494972\n",
      "  params :  tensor([  4.9189, -14.7641])\n",
      "  grad :  tensor([-0.0764,  0.4323])\n",
      "Epoch 1137, Loss 3.493046\n",
      "  params :  tensor([  4.9197, -14.7684])\n",
      "  grad :  tensor([-0.0763,  0.4316])\n",
      "Epoch 1138, Loss 3.491127\n",
      "  params :  tensor([  4.9204, -14.7727])\n",
      "  grad :  tensor([-0.0761,  0.4309])\n",
      "Epoch 1139, Loss 3.489214\n",
      "  params :  tensor([  4.9212, -14.7770])\n",
      "  grad :  tensor([-0.0760,  0.4301])\n",
      "Epoch 1140, Loss 3.487308\n",
      "  params :  tensor([  4.9219, -14.7813])\n",
      "  grad :  tensor([-0.0759,  0.4294])\n",
      "Epoch 1141, Loss 3.485410\n",
      "  params :  tensor([  4.9227, -14.7856])\n",
      "  grad :  tensor([-0.0757,  0.4287])\n",
      "Epoch 1142, Loss 3.483515\n",
      "  params :  tensor([  4.9235, -14.7899])\n",
      "  grad :  tensor([-0.0756,  0.4280])\n",
      "Epoch 1143, Loss 3.481627\n",
      "  params :  tensor([  4.9242, -14.7941])\n",
      "  grad :  tensor([-0.0755,  0.4272])\n",
      "Epoch 1144, Loss 3.479746\n",
      "  params :  tensor([  4.9250, -14.7984])\n",
      "  grad :  tensor([-0.0753,  0.4265])\n",
      "Epoch 1145, Loss 3.477872\n",
      "  params :  tensor([  4.9257, -14.8027])\n",
      "  grad :  tensor([-0.0752,  0.4258])\n",
      "Epoch 1146, Loss 3.476005\n",
      "  params :  tensor([  4.9265, -14.8069])\n",
      "  grad :  tensor([-0.0751,  0.4250])\n",
      "Epoch 1147, Loss 3.474143\n",
      "  params :  tensor([  4.9272, -14.8112])\n",
      "  grad :  tensor([-0.0750,  0.4243])\n",
      "Epoch 1148, Loss 3.472288\n",
      "  params :  tensor([  4.9280, -14.8154])\n",
      "  grad :  tensor([-0.0748,  0.4236])\n",
      "Epoch 1149, Loss 3.470441\n",
      "  params :  tensor([  4.9287, -14.8196])\n",
      "  grad :  tensor([-0.0747,  0.4229])\n",
      "Epoch 1150, Loss 3.468597\n",
      "  params :  tensor([  4.9295, -14.8238])\n",
      "  grad :  tensor([-0.0746,  0.4222])\n",
      "Epoch 1151, Loss 3.466762\n",
      "  params :  tensor([  4.9302, -14.8281])\n",
      "  grad :  tensor([-0.0745,  0.4215])\n",
      "Epoch 1152, Loss 3.464930\n",
      "  params :  tensor([  4.9309, -14.8323])\n",
      "  grad :  tensor([-0.0743,  0.4207])\n",
      "Epoch 1153, Loss 3.463105\n",
      "  params :  tensor([  4.9317, -14.8365])\n",
      "  grad :  tensor([-0.0742,  0.4200])\n",
      "Epoch 1154, Loss 3.461290\n",
      "  params :  tensor([  4.9324, -14.8407])\n",
      "  grad :  tensor([-0.0741,  0.4193])\n",
      "Epoch 1155, Loss 3.459477\n",
      "  params :  tensor([  4.9332, -14.8448])\n",
      "  grad :  tensor([-0.0739,  0.4186])\n",
      "Epoch 1156, Loss 3.457671\n",
      "  params :  tensor([  4.9339, -14.8490])\n",
      "  grad :  tensor([-0.0738,  0.4179])\n",
      "Epoch 1157, Loss 3.455873\n",
      "  params :  tensor([  4.9346, -14.8532])\n",
      "  grad :  tensor([-0.0737,  0.4172])\n",
      "Epoch 1158, Loss 3.454080\n",
      "  params :  tensor([  4.9354, -14.8574])\n",
      "  grad :  tensor([-0.0736,  0.4165])\n",
      "Epoch 1159, Loss 3.452293\n",
      "  params :  tensor([  4.9361, -14.8615])\n",
      "  grad :  tensor([-0.0734,  0.4158])\n",
      "Epoch 1160, Loss 3.450513\n",
      "  params :  tensor([  4.9368, -14.8657])\n",
      "  grad :  tensor([-0.0733,  0.4151])\n",
      "Epoch 1161, Loss 3.448736\n",
      "  params :  tensor([  4.9376, -14.8698])\n",
      "  grad :  tensor([-0.0732,  0.4143])\n",
      "Epoch 1162, Loss 3.446968\n",
      "  params :  tensor([  4.9383, -14.8739])\n",
      "  grad :  tensor([-0.0731,  0.4136])\n",
      "Epoch 1163, Loss 3.445203\n",
      "  params :  tensor([  4.9390, -14.8781])\n",
      "  grad :  tensor([-0.0730,  0.4129])\n",
      "Epoch 1164, Loss 3.443449\n",
      "  params :  tensor([  4.9398, -14.8822])\n",
      "  grad :  tensor([-0.0728,  0.4122])\n",
      "Epoch 1165, Loss 3.441697\n",
      "  params :  tensor([  4.9405, -14.8863])\n",
      "  grad :  tensor([-0.0727,  0.4115])\n",
      "Epoch 1166, Loss 3.439952\n",
      "  params :  tensor([  4.9412, -14.8904])\n",
      "  grad :  tensor([-0.0726,  0.4108])\n",
      "Epoch 1167, Loss 3.438210\n",
      "  params :  tensor([  4.9419, -14.8945])\n",
      "  grad :  tensor([-0.0725,  0.4101])\n",
      "Epoch 1168, Loss 3.436479\n",
      "  params :  tensor([  4.9427, -14.8986])\n",
      "  grad :  tensor([-0.0723,  0.4094])\n",
      "Epoch 1169, Loss 3.434753\n",
      "  params :  tensor([  4.9434, -14.9027])\n",
      "  grad :  tensor([-0.0722,  0.4087])\n",
      "Epoch 1170, Loss 3.433030\n",
      "  params :  tensor([  4.9441, -14.9068])\n",
      "  grad :  tensor([-0.0721,  0.4081])\n",
      "Epoch 1171, Loss 3.431314\n",
      "  params :  tensor([  4.9448, -14.9109])\n",
      "  grad :  tensor([-0.0720,  0.4074])\n",
      "Epoch 1172, Loss 3.429607\n",
      "  params :  tensor([  4.9455, -14.9149])\n",
      "  grad :  tensor([-0.0719,  0.4067])\n",
      "Epoch 1173, Loss 3.427903\n",
      "  params :  tensor([  4.9463, -14.9190])\n",
      "  grad :  tensor([-0.0717,  0.4060])\n",
      "Epoch 1174, Loss 3.426204\n",
      "  params :  tensor([  4.9470, -14.9230])\n",
      "  grad :  tensor([-0.0716,  0.4053])\n",
      "Epoch 1175, Loss 3.424510\n",
      "  params :  tensor([  4.9477, -14.9271])\n",
      "  grad :  tensor([-0.0715,  0.4046])\n",
      "Epoch 1176, Loss 3.422823\n",
      "  params :  tensor([  4.9484, -14.9311])\n",
      "  grad :  tensor([-0.0714,  0.4039])\n",
      "Epoch 1177, Loss 3.421144\n",
      "  params :  tensor([  4.9491, -14.9352])\n",
      "  grad :  tensor([-0.0712,  0.4032])\n",
      "Epoch 1178, Loss 3.419468\n",
      "  params :  tensor([  4.9498, -14.9392])\n",
      "  grad :  tensor([-0.0711,  0.4025])\n",
      "Epoch 1179, Loss 3.417798\n",
      "  params :  tensor([  4.9505, -14.9432])\n",
      "  grad :  tensor([-0.0710,  0.4019])\n",
      "Epoch 1180, Loss 3.416134\n",
      "  params :  tensor([  4.9512, -14.9472])\n",
      "  grad :  tensor([-0.0709,  0.4012])\n",
      "Epoch 1181, Loss 3.414476\n",
      "  params :  tensor([  4.9520, -14.9512])\n",
      "  grad :  tensor([-0.0708,  0.4005])\n",
      "Epoch 1182, Loss 3.412824\n",
      "  params :  tensor([  4.9527, -14.9552])\n",
      "  grad :  tensor([-0.0706,  0.3998])\n",
      "Epoch 1183, Loss 3.411176\n",
      "  params :  tensor([  4.9534, -14.9592])\n",
      "  grad :  tensor([-0.0705,  0.3991])\n",
      "Epoch 1184, Loss 3.409534\n",
      "  params :  tensor([  4.9541, -14.9632])\n",
      "  grad :  tensor([-0.0704,  0.3985])\n",
      "Epoch 1185, Loss 3.407900\n",
      "  params :  tensor([  4.9548, -14.9672])\n",
      "  grad :  tensor([-0.0703,  0.3978])\n",
      "Epoch 1186, Loss 3.406271\n",
      "  params :  tensor([  4.9555, -14.9711])\n",
      "  grad :  tensor([-0.0701,  0.3971])\n",
      "Epoch 1187, Loss 3.404645\n",
      "  params :  tensor([  4.9562, -14.9751])\n",
      "  grad :  tensor([-0.0700,  0.3964])\n",
      "Epoch 1188, Loss 3.403024\n",
      "  params :  tensor([  4.9569, -14.9791])\n",
      "  grad :  tensor([-0.0699,  0.3958])\n",
      "Epoch 1189, Loss 3.401413\n",
      "  params :  tensor([  4.9576, -14.9830])\n",
      "  grad :  tensor([-0.0698,  0.3951])\n",
      "Epoch 1190, Loss 3.399802\n",
      "  params :  tensor([  4.9583, -14.9870])\n",
      "  grad :  tensor([-0.0697,  0.3944])\n",
      "Epoch 1191, Loss 3.398200\n",
      "  params :  tensor([  4.9590, -14.9909])\n",
      "  grad :  tensor([-0.0696,  0.3937])\n",
      "Epoch 1192, Loss 3.396602\n",
      "  params :  tensor([  4.9597, -14.9948])\n",
      "  grad :  tensor([-0.0694,  0.3931])\n",
      "Epoch 1193, Loss 3.395011\n",
      "  params :  tensor([  4.9604, -14.9988])\n",
      "  grad :  tensor([-0.0693,  0.3924])\n",
      "Epoch 1194, Loss 3.393425\n",
      "  params :  tensor([  4.9610, -15.0027])\n",
      "  grad :  tensor([-0.0692,  0.3917])\n",
      "Epoch 1195, Loss 3.391844\n",
      "  params :  tensor([  4.9617, -15.0066])\n",
      "  grad :  tensor([-0.0691,  0.3911])\n",
      "Epoch 1196, Loss 3.390266\n",
      "  params :  tensor([  4.9624, -15.0105])\n",
      "  grad :  tensor([-0.0690,  0.3904])\n",
      "Epoch 1197, Loss 3.388697\n",
      "  params :  tensor([  4.9631, -15.0144])\n",
      "  grad :  tensor([-0.0689,  0.3897])\n",
      "Epoch 1198, Loss 3.387131\n",
      "  params :  tensor([  4.9638, -15.0183])\n",
      "  grad :  tensor([-0.0687,  0.3891])\n",
      "Epoch 1199, Loss 3.385571\n",
      "  params :  tensor([  4.9645, -15.0222])\n",
      "  grad :  tensor([-0.0686,  0.3884])\n",
      "Epoch 1200, Loss 3.384018\n",
      "  params :  tensor([  4.9652, -15.0260])\n",
      "  grad :  tensor([-0.0685,  0.3878])\n",
      "Epoch 1201, Loss 3.382467\n",
      "  params :  tensor([  4.9659, -15.0299])\n",
      "  grad :  tensor([-0.0684,  0.3871])\n",
      "Epoch 1202, Loss 3.380925\n",
      "  params :  tensor([  4.9665, -15.0338])\n",
      "  grad :  tensor([-0.0683,  0.3864])\n",
      "Epoch 1203, Loss 3.379385\n",
      "  params :  tensor([  4.9672, -15.0376])\n",
      "  grad :  tensor([-0.0681,  0.3858])\n",
      "Epoch 1204, Loss 3.377851\n",
      "  params :  tensor([  4.9679, -15.0415])\n",
      "  grad :  tensor([-0.0680,  0.3851])\n",
      "Epoch 1205, Loss 3.376323\n",
      "  params :  tensor([  4.9686, -15.0453])\n",
      "  grad :  tensor([-0.0679,  0.3845])\n",
      "Epoch 1206, Loss 3.374800\n",
      "  params :  tensor([  4.9693, -15.0492])\n",
      "  grad :  tensor([-0.0678,  0.3838])\n",
      "Epoch 1207, Loss 3.373284\n",
      "  params :  tensor([  4.9699, -15.0530])\n",
      "  grad :  tensor([-0.0677,  0.3832])\n",
      "Epoch 1208, Loss 3.371769\n",
      "  params :  tensor([  4.9706, -15.0568])\n",
      "  grad :  tensor([-0.0676,  0.3825])\n",
      "Epoch 1209, Loss 3.370261\n",
      "  params :  tensor([  4.9713, -15.0606])\n",
      "  grad :  tensor([-0.0675,  0.3819])\n",
      "Epoch 1210, Loss 3.368760\n",
      "  params :  tensor([  4.9720, -15.0645])\n",
      "  grad :  tensor([-0.0673,  0.3812])\n",
      "Epoch 1211, Loss 3.367262\n",
      "  params :  tensor([  4.9726, -15.0683])\n",
      "  grad :  tensor([-0.0672,  0.3806])\n",
      "Epoch 1212, Loss 3.365771\n",
      "  params :  tensor([  4.9733, -15.0721])\n",
      "  grad :  tensor([-0.0671,  0.3799])\n",
      "Epoch 1213, Loss 3.364282\n",
      "  params :  tensor([  4.9740, -15.0758])\n",
      "  grad :  tensor([-0.0670,  0.3793])\n",
      "Epoch 1214, Loss 3.362800\n",
      "  params :  tensor([  4.9746, -15.0796])\n",
      "  grad :  tensor([-0.0669,  0.3786])\n",
      "Epoch 1215, Loss 3.361324\n",
      "  params :  tensor([  4.9753, -15.0834])\n",
      "  grad :  tensor([-0.0668,  0.3780])\n",
      "Epoch 1216, Loss 3.359850\n",
      "  params :  tensor([  4.9760, -15.0872])\n",
      "  grad :  tensor([-0.0667,  0.3774])\n",
      "Epoch 1217, Loss 3.358384\n",
      "  params :  tensor([  4.9766, -15.0910])\n",
      "  grad :  tensor([-0.0665,  0.3767])\n",
      "Epoch 1218, Loss 3.356921\n",
      "  params :  tensor([  4.9773, -15.0947])\n",
      "  grad :  tensor([-0.0664,  0.3761])\n",
      "Epoch 1219, Loss 3.355464\n",
      "  params :  tensor([  4.9780, -15.0985])\n",
      "  grad :  tensor([-0.0663,  0.3754])\n",
      "Epoch 1220, Loss 3.354012\n",
      "  params :  tensor([  4.9786, -15.1022])\n",
      "  grad :  tensor([-0.0662,  0.3748])\n",
      "Epoch 1221, Loss 3.352564\n",
      "  params :  tensor([  4.9793, -15.1060])\n",
      "  grad :  tensor([-0.0661,  0.3742])\n",
      "Epoch 1222, Loss 3.351122\n",
      "  params :  tensor([  4.9799, -15.1097])\n",
      "  grad :  tensor([-0.0660,  0.3735])\n",
      "Epoch 1223, Loss 3.349685\n",
      "  params :  tensor([  4.9806, -15.1134])\n",
      "  grad :  tensor([-0.0659,  0.3729])\n",
      "Epoch 1224, Loss 3.348251\n",
      "  params :  tensor([  4.9813, -15.1171])\n",
      "  grad :  tensor([-0.0657,  0.3723])\n",
      "Epoch 1225, Loss 3.346824\n",
      "  params :  tensor([  4.9819, -15.1209])\n",
      "  grad :  tensor([-0.0656,  0.3716])\n",
      "Epoch 1226, Loss 3.345403\n",
      "  params :  tensor([  4.9826, -15.1246])\n",
      "  grad :  tensor([-0.0655,  0.3710])\n",
      "Epoch 1227, Loss 3.343982\n",
      "  params :  tensor([  4.9832, -15.1283])\n",
      "  grad :  tensor([-0.0654,  0.3704])\n",
      "Epoch 1228, Loss 3.342571\n",
      "  params :  tensor([  4.9839, -15.1320])\n",
      "  grad :  tensor([-0.0653,  0.3697])\n",
      "Epoch 1229, Loss 3.341160\n",
      "  params :  tensor([  4.9845, -15.1357])\n",
      "  grad :  tensor([-0.0652,  0.3691])\n",
      "Epoch 1230, Loss 3.339758\n",
      "  params :  tensor([  4.9852, -15.1393])\n",
      "  grad :  tensor([-0.0651,  0.3685])\n",
      "Epoch 1231, Loss 3.338359\n",
      "  params :  tensor([  4.9858, -15.1430])\n",
      "  grad :  tensor([-0.0650,  0.3679])\n",
      "Epoch 1232, Loss 3.336965\n",
      "  params :  tensor([  4.9865, -15.1467])\n",
      "  grad :  tensor([-0.0649,  0.3672])\n",
      "Epoch 1233, Loss 3.335577\n",
      "  params :  tensor([  4.9871, -15.1504])\n",
      "  grad :  tensor([-0.0648,  0.3666])\n",
      "Epoch 1234, Loss 3.334192\n",
      "  params :  tensor([  4.9878, -15.1540])\n",
      "  grad :  tensor([-0.0646,  0.3660])\n",
      "Epoch 1235, Loss 3.332811\n",
      "  params :  tensor([  4.9884, -15.1577])\n",
      "  grad :  tensor([-0.0645,  0.3654])\n",
      "Epoch 1236, Loss 3.331436\n",
      "  params :  tensor([  4.9891, -15.1613])\n",
      "  grad :  tensor([-0.0644,  0.3647])\n",
      "Epoch 1237, Loss 3.330065\n",
      "  params :  tensor([  4.9897, -15.1650])\n",
      "  grad :  tensor([-0.0643,  0.3641])\n",
      "Epoch 1238, Loss 3.328699\n",
      "  params :  tensor([  4.9904, -15.1686])\n",
      "  grad :  tensor([-0.0642,  0.3635])\n",
      "Epoch 1239, Loss 3.327339\n",
      "  params :  tensor([  4.9910, -15.1722])\n",
      "  grad :  tensor([-0.0641,  0.3629])\n",
      "Epoch 1240, Loss 3.325980\n",
      "  params :  tensor([  4.9916, -15.1759])\n",
      "  grad :  tensor([-0.0640,  0.3623])\n",
      "Epoch 1241, Loss 3.324628\n",
      "  params :  tensor([  4.9923, -15.1795])\n",
      "  grad :  tensor([-0.0639,  0.3617])\n",
      "Epoch 1242, Loss 3.323279\n",
      "  params :  tensor([  4.9929, -15.1831])\n",
      "  grad :  tensor([-0.0638,  0.3610])\n",
      "Epoch 1243, Loss 3.321935\n",
      "  params :  tensor([  4.9936, -15.1867])\n",
      "  grad :  tensor([-0.0637,  0.3604])\n",
      "Epoch 1244, Loss 3.320600\n",
      "  params :  tensor([  4.9942, -15.1903])\n",
      "  grad :  tensor([-0.0636,  0.3598])\n",
      "Epoch 1245, Loss 3.319264\n",
      "  params :  tensor([  4.9948, -15.1939])\n",
      "  grad :  tensor([-0.0635,  0.3592])\n",
      "Epoch 1246, Loss 3.317935\n",
      "  params :  tensor([  4.9955, -15.1975])\n",
      "  grad :  tensor([-0.0633,  0.3586])\n",
      "Epoch 1247, Loss 3.316611\n",
      "  params :  tensor([  4.9961, -15.2010])\n",
      "  grad :  tensor([-0.0633,  0.3580])\n",
      "Epoch 1248, Loss 3.315289\n",
      "  params :  tensor([  4.9967, -15.2046])\n",
      "  grad :  tensor([-0.0631,  0.3574])\n",
      "Epoch 1249, Loss 3.313973\n",
      "  params :  tensor([  4.9973, -15.2082])\n",
      "  grad :  tensor([-0.0630,  0.3568])\n",
      "Epoch 1250, Loss 3.312663\n",
      "  params :  tensor([  4.9980, -15.2117])\n",
      "  grad :  tensor([-0.0629,  0.3562])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1251, Loss 3.311353\n",
      "  params :  tensor([  4.9986, -15.2153])\n",
      "  grad :  tensor([-0.0628,  0.3556])\n",
      "Epoch 1252, Loss 3.310053\n",
      "  params :  tensor([  4.9992, -15.2189])\n",
      "  grad :  tensor([-0.0627,  0.3550])\n",
      "Epoch 1253, Loss 3.308756\n",
      "  params :  tensor([  4.9999, -15.2224])\n",
      "  grad :  tensor([-0.0626,  0.3543])\n",
      "Epoch 1254, Loss 3.307463\n",
      "  params :  tensor([  5.0005, -15.2259])\n",
      "  grad :  tensor([-0.0625,  0.3537])\n",
      "Epoch 1255, Loss 3.306170\n",
      "  params :  tensor([  5.0011, -15.2295])\n",
      "  grad :  tensor([-0.0624,  0.3531])\n",
      "Epoch 1256, Loss 3.304887\n",
      "  params :  tensor([  5.0017, -15.2330])\n",
      "  grad :  tensor([-0.0623,  0.3525])\n",
      "Epoch 1257, Loss 3.303605\n",
      "  params :  tensor([  5.0024, -15.2365])\n",
      "  grad :  tensor([-0.0622,  0.3519])\n",
      "Epoch 1258, Loss 3.302329\n",
      "  params :  tensor([  5.0030, -15.2400])\n",
      "  grad :  tensor([-0.0620,  0.3514])\n",
      "Epoch 1259, Loss 3.301057\n",
      "  params :  tensor([  5.0036, -15.2435])\n",
      "  grad :  tensor([-0.0620,  0.3508])\n",
      "Epoch 1260, Loss 3.299791\n",
      "  params :  tensor([  5.0042, -15.2470])\n",
      "  grad :  tensor([-0.0619,  0.3502])\n",
      "Epoch 1261, Loss 3.298528\n",
      "  params :  tensor([  5.0048, -15.2505])\n",
      "  grad :  tensor([-0.0618,  0.3496])\n",
      "Epoch 1262, Loss 3.297267\n",
      "  params :  tensor([  5.0054, -15.2540])\n",
      "  grad :  tensor([-0.0616,  0.3490])\n",
      "Epoch 1263, Loss 3.296014\n",
      "  params :  tensor([  5.0061, -15.2575])\n",
      "  grad :  tensor([-0.0615,  0.3484])\n",
      "Epoch 1264, Loss 3.294762\n",
      "  params :  tensor([  5.0067, -15.2610])\n",
      "  grad :  tensor([-0.0614,  0.3478])\n",
      "Epoch 1265, Loss 3.293517\n",
      "  params :  tensor([  5.0073, -15.2645])\n",
      "  grad :  tensor([-0.0613,  0.3472])\n",
      "Epoch 1266, Loss 3.292276\n",
      "  params :  tensor([  5.0079, -15.2679])\n",
      "  grad :  tensor([-0.0612,  0.3466])\n",
      "Epoch 1267, Loss 3.291036\n",
      "  params :  tensor([  5.0085, -15.2714])\n",
      "  grad :  tensor([-0.0611,  0.3460])\n",
      "Epoch 1268, Loss 3.289804\n",
      "  params :  tensor([  5.0091, -15.2748])\n",
      "  grad :  tensor([-0.0610,  0.3454])\n",
      "Epoch 1269, Loss 3.288573\n",
      "  params :  tensor([  5.0097, -15.2783])\n",
      "  grad :  tensor([-0.0609,  0.3448])\n",
      "Epoch 1270, Loss 3.287347\n",
      "  params :  tensor([  5.0103, -15.2817])\n",
      "  grad :  tensor([-0.0608,  0.3443])\n",
      "Epoch 1271, Loss 3.286129\n",
      "  params :  tensor([  5.0109, -15.2852])\n",
      "  grad :  tensor([-0.0607,  0.3437])\n",
      "Epoch 1272, Loss 3.284911\n",
      "  params :  tensor([  5.0116, -15.2886])\n",
      "  grad :  tensor([-0.0606,  0.3431])\n",
      "Epoch 1273, Loss 3.283698\n",
      "  params :  tensor([  5.0122, -15.2920])\n",
      "  grad :  tensor([-0.0605,  0.3425])\n",
      "Epoch 1274, Loss 3.282488\n",
      "  params :  tensor([  5.0128, -15.2954])\n",
      "  grad :  tensor([-0.0604,  0.3419])\n",
      "Epoch 1275, Loss 3.281284\n",
      "  params :  tensor([  5.0134, -15.2988])\n",
      "  grad :  tensor([-0.0603,  0.3413])\n",
      "Epoch 1276, Loss 3.280085\n",
      "  params :  tensor([  5.0140, -15.3023])\n",
      "  grad :  tensor([-0.0602,  0.3408])\n",
      "Epoch 1277, Loss 3.278888\n",
      "  params :  tensor([  5.0146, -15.3057])\n",
      "  grad :  tensor([-0.0601,  0.3402])\n",
      "Epoch 1278, Loss 3.277696\n",
      "  params :  tensor([  5.0152, -15.3091])\n",
      "  grad :  tensor([-0.0600,  0.3396])\n",
      "Epoch 1279, Loss 3.276506\n",
      "  params :  tensor([  5.0158, -15.3124])\n",
      "  grad :  tensor([-0.0599,  0.3390])\n",
      "Epoch 1280, Loss 3.275322\n",
      "  params :  tensor([  5.0164, -15.3158])\n",
      "  grad :  tensor([-0.0598,  0.3384])\n",
      "Epoch 1281, Loss 3.274142\n",
      "  params :  tensor([  5.0170, -15.3192])\n",
      "  grad :  tensor([-0.0597,  0.3379])\n",
      "Epoch 1282, Loss 3.272968\n",
      "  params :  tensor([  5.0176, -15.3226])\n",
      "  grad :  tensor([-0.0596,  0.3373])\n",
      "Epoch 1283, Loss 3.271793\n",
      "  params :  tensor([  5.0182, -15.3259])\n",
      "  grad :  tensor([-0.0595,  0.3367])\n",
      "Epoch 1284, Loss 3.270625\n",
      "  params :  tensor([  5.0187, -15.3293])\n",
      "  grad :  tensor([-0.0594,  0.3362])\n",
      "Epoch 1285, Loss 3.269460\n",
      "  params :  tensor([  5.0193, -15.3327])\n",
      "  grad :  tensor([-0.0593,  0.3356])\n",
      "Epoch 1286, Loss 3.268301\n",
      "  params :  tensor([  5.0199, -15.3360])\n",
      "  grad :  tensor([-0.0592,  0.3350])\n",
      "Epoch 1287, Loss 3.267143\n",
      "  params :  tensor([  5.0205, -15.3394])\n",
      "  grad :  tensor([-0.0591,  0.3344])\n",
      "Epoch 1288, Loss 3.265991\n",
      "  params :  tensor([  5.0211, -15.3427])\n",
      "  grad :  tensor([-0.0590,  0.3339])\n",
      "Epoch 1289, Loss 3.264842\n",
      "  params :  tensor([  5.0217, -15.3460])\n",
      "  grad :  tensor([-0.0589,  0.3333])\n",
      "Epoch 1290, Loss 3.263700\n",
      "  params :  tensor([  5.0223, -15.3494])\n",
      "  grad :  tensor([-0.0588,  0.3327])\n",
      "Epoch 1291, Loss 3.262556\n",
      "  params :  tensor([  5.0229, -15.3527])\n",
      "  grad :  tensor([-0.0587,  0.3322])\n",
      "Epoch 1292, Loss 3.261421\n",
      "  params :  tensor([  5.0235, -15.3560])\n",
      "  grad :  tensor([-0.0586,  0.3316])\n",
      "Epoch 1293, Loss 3.260287\n",
      "  params :  tensor([  5.0240, -15.3593])\n",
      "  grad :  tensor([-0.0585,  0.3311])\n",
      "Epoch 1294, Loss 3.259160\n",
      "  params :  tensor([  5.0246, -15.3626])\n",
      "  grad :  tensor([-0.0584,  0.3305])\n",
      "Epoch 1295, Loss 3.258033\n",
      "  params :  tensor([  5.0252, -15.3659])\n",
      "  grad :  tensor([-0.0583,  0.3299])\n",
      "Epoch 1296, Loss 3.256912\n",
      "  params :  tensor([  5.0258, -15.3692])\n",
      "  grad :  tensor([-0.0582,  0.3294])\n",
      "Epoch 1297, Loss 3.255795\n",
      "  params :  tensor([  5.0264, -15.3725])\n",
      "  grad :  tensor([-0.0581,  0.3288])\n",
      "Epoch 1298, Loss 3.254681\n",
      "  params :  tensor([  5.0270, -15.3758])\n",
      "  grad :  tensor([-0.0580,  0.3282])\n",
      "Epoch 1299, Loss 3.253569\n",
      "  params :  tensor([  5.0275, -15.3791])\n",
      "  grad :  tensor([-0.0579,  0.3277])\n",
      "Epoch 1300, Loss 3.252462\n",
      "  params :  tensor([  5.0281, -15.3823])\n",
      "  grad :  tensor([-0.0578,  0.3271])\n",
      "Epoch 1301, Loss 3.251362\n",
      "  params :  tensor([  5.0287, -15.3856])\n",
      "  grad :  tensor([-0.0577,  0.3266])\n",
      "Epoch 1302, Loss 3.250263\n",
      "  params :  tensor([  5.0293, -15.3888])\n",
      "  grad :  tensor([-0.0576,  0.3260])\n",
      "Epoch 1303, Loss 3.249168\n",
      "  params :  tensor([  5.0298, -15.3921])\n",
      "  grad :  tensor([-0.0575,  0.3255])\n",
      "Epoch 1304, Loss 3.248077\n",
      "  params :  tensor([  5.0304, -15.3954])\n",
      "  grad :  tensor([-0.0574,  0.3249])\n",
      "Epoch 1305, Loss 3.246988\n",
      "  params :  tensor([  5.0310, -15.3986])\n",
      "  grad :  tensor([-0.0573,  0.3244])\n",
      "Epoch 1306, Loss 3.245904\n",
      "  params :  tensor([  5.0316, -15.4018])\n",
      "  grad :  tensor([-0.0572,  0.3238])\n",
      "Epoch 1307, Loss 3.244824\n",
      "  params :  tensor([  5.0321, -15.4051])\n",
      "  grad :  tensor([-0.0571,  0.3233])\n",
      "Epoch 1308, Loss 3.243747\n",
      "  params :  tensor([  5.0327, -15.4083])\n",
      "  grad :  tensor([-0.0570,  0.3227])\n",
      "Epoch 1309, Loss 3.242674\n",
      "  params :  tensor([  5.0333, -15.4115])\n",
      "  grad :  tensor([-0.0569,  0.3222])\n",
      "Epoch 1310, Loss 3.241606\n",
      "  params :  tensor([  5.0338, -15.4147])\n",
      "  grad :  tensor([-0.0568,  0.3216])\n",
      "Epoch 1311, Loss 3.240538\n",
      "  params :  tensor([  5.0344, -15.4179])\n",
      "  grad :  tensor([-0.0567,  0.3211])\n",
      "Epoch 1312, Loss 3.239475\n",
      "  params :  tensor([  5.0350, -15.4211])\n",
      "  grad :  tensor([-0.0566,  0.3205])\n",
      "Epoch 1313, Loss 3.238419\n",
      "  params :  tensor([  5.0355, -15.4243])\n",
      "  grad :  tensor([-0.0565,  0.3200])\n",
      "Epoch 1314, Loss 3.237363\n",
      "  params :  tensor([  5.0361, -15.4275])\n",
      "  grad :  tensor([-0.0564,  0.3194])\n",
      "Epoch 1315, Loss 3.236314\n",
      "  params :  tensor([  5.0367, -15.4307])\n",
      "  grad :  tensor([-0.0563,  0.3189])\n",
      "Epoch 1316, Loss 3.235265\n",
      "  params :  tensor([  5.0372, -15.4339])\n",
      "  grad :  tensor([-0.0562,  0.3184])\n",
      "Epoch 1317, Loss 3.234218\n",
      "  params :  tensor([  5.0378, -15.4371])\n",
      "  grad :  tensor([-0.0561,  0.3178])\n",
      "Epoch 1318, Loss 3.233179\n",
      "  params :  tensor([  5.0383, -15.4403])\n",
      "  grad :  tensor([-0.0561,  0.3173])\n",
      "Epoch 1319, Loss 3.232143\n",
      "  params :  tensor([  5.0389, -15.4434])\n",
      "  grad :  tensor([-0.0560,  0.3167])\n",
      "Epoch 1320, Loss 3.231109\n",
      "  params :  tensor([  5.0395, -15.4466])\n",
      "  grad :  tensor([-0.0558,  0.3162])\n",
      "Epoch 1321, Loss 3.230078\n",
      "  params :  tensor([  5.0400, -15.4498])\n",
      "  grad :  tensor([-0.0558,  0.3157])\n",
      "Epoch 1322, Loss 3.229051\n",
      "  params :  tensor([  5.0406, -15.4529])\n",
      "  grad :  tensor([-0.0557,  0.3151])\n",
      "Epoch 1323, Loss 3.228027\n",
      "  params :  tensor([  5.0411, -15.4560])\n",
      "  grad :  tensor([-0.0556,  0.3146])\n",
      "Epoch 1324, Loss 3.227010\n",
      "  params :  tensor([  5.0417, -15.4592])\n",
      "  grad :  tensor([-0.0555,  0.3141])\n",
      "Epoch 1325, Loss 3.225992\n",
      "  params :  tensor([  5.0422, -15.4623])\n",
      "  grad :  tensor([-0.0554,  0.3135])\n",
      "Epoch 1326, Loss 3.224979\n",
      "  params :  tensor([  5.0428, -15.4655])\n",
      "  grad :  tensor([-0.0553,  0.3130])\n",
      "Epoch 1327, Loss 3.223971\n",
      "  params :  tensor([  5.0433, -15.4686])\n",
      "  grad :  tensor([-0.0552,  0.3125])\n",
      "Epoch 1328, Loss 3.222965\n",
      "  params :  tensor([  5.0439, -15.4717])\n",
      "  grad :  tensor([-0.0551,  0.3119])\n",
      "Epoch 1329, Loss 3.221960\n",
      "  params :  tensor([  5.0444, -15.4748])\n",
      "  grad :  tensor([-0.0550,  0.3114])\n",
      "Epoch 1330, Loss 3.220962\n",
      "  params :  tensor([  5.0450, -15.4779])\n",
      "  grad :  tensor([-0.0549,  0.3109])\n",
      "Epoch 1331, Loss 3.219967\n",
      "  params :  tensor([  5.0455, -15.4810])\n",
      "  grad :  tensor([-0.0548,  0.3103])\n",
      "Epoch 1332, Loss 3.218975\n",
      "  params :  tensor([  5.0461, -15.4841])\n",
      "  grad :  tensor([-0.0547,  0.3098])\n",
      "Epoch 1333, Loss 3.217986\n",
      "  params :  tensor([  5.0466, -15.4872])\n",
      "  grad :  tensor([-0.0546,  0.3093])\n",
      "Epoch 1334, Loss 3.217000\n",
      "  params :  tensor([  5.0472, -15.4903])\n",
      "  grad :  tensor([-0.0545,  0.3088])\n",
      "Epoch 1335, Loss 3.216017\n",
      "  params :  tensor([  5.0477, -15.4934])\n",
      "  grad :  tensor([-0.0544,  0.3082])\n",
      "Epoch 1336, Loss 3.215038\n",
      "  params :  tensor([  5.0483, -15.4965])\n",
      "  grad :  tensor([-0.0543,  0.3077])\n",
      "Epoch 1337, Loss 3.214062\n",
      "  params :  tensor([  5.0488, -15.4995])\n",
      "  grad :  tensor([-0.0543,  0.3072])\n",
      "Epoch 1338, Loss 3.213092\n",
      "  params :  tensor([  5.0494, -15.5026])\n",
      "  grad :  tensor([-0.0542,  0.3067])\n",
      "Epoch 1339, Loss 3.212122\n",
      "  params :  tensor([  5.0499, -15.5057])\n",
      "  grad :  tensor([-0.0541,  0.3061])\n",
      "Epoch 1340, Loss 3.211157\n",
      "  params :  tensor([  5.0504, -15.5087])\n",
      "  grad :  tensor([-0.0540,  0.3056])\n",
      "Epoch 1341, Loss 3.210192\n",
      "  params :  tensor([  5.0510, -15.5118])\n",
      "  grad :  tensor([-0.0539,  0.3051])\n",
      "Epoch 1342, Loss 3.209235\n",
      "  params :  tensor([  5.0515, -15.5148])\n",
      "  grad :  tensor([-0.0538,  0.3046])\n",
      "Epoch 1343, Loss 3.208279\n",
      "  params :  tensor([  5.0521, -15.5179])\n",
      "  grad :  tensor([-0.0537,  0.3041])\n",
      "Epoch 1344, Loss 3.207326\n",
      "  params :  tensor([  5.0526, -15.5209])\n",
      "  grad :  tensor([-0.0536,  0.3036])\n",
      "Epoch 1345, Loss 3.206376\n",
      "  params :  tensor([  5.0531, -15.5239])\n",
      "  grad :  tensor([-0.0535,  0.3030])\n",
      "Epoch 1346, Loss 3.205430\n",
      "  params :  tensor([  5.0537, -15.5269])\n",
      "  grad :  tensor([-0.0534,  0.3025])\n",
      "Epoch 1347, Loss 3.204488\n",
      "  params :  tensor([  5.0542, -15.5300])\n",
      "  grad :  tensor([-0.0533,  0.3020])\n",
      "Epoch 1348, Loss 3.203547\n",
      "  params :  tensor([  5.0547, -15.5330])\n",
      "  grad :  tensor([-0.0532,  0.3015])\n",
      "Epoch 1349, Loss 3.202610\n",
      "  params :  tensor([  5.0553, -15.5360])\n",
      "  grad :  tensor([-0.0532,  0.3010])\n",
      "Epoch 1350, Loss 3.201678\n",
      "  params :  tensor([  5.0558, -15.5390])\n",
      "  grad :  tensor([-0.0531,  0.3005])\n",
      "Epoch 1351, Loss 3.200747\n",
      "  params :  tensor([  5.0563, -15.5420])\n",
      "  grad :  tensor([-0.0530,  0.3000])\n",
      "Epoch 1352, Loss 3.199820\n",
      "  params :  tensor([  5.0568, -15.5450])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad :  tensor([-0.0529,  0.2995])\n",
      "Epoch 1353, Loss 3.198897\n",
      "  params :  tensor([  5.0574, -15.5480])\n",
      "  grad :  tensor([-0.0528,  0.2989])\n",
      "Epoch 1354, Loss 3.197976\n",
      "  params :  tensor([  5.0579, -15.5510])\n",
      "  grad :  tensor([-0.0527,  0.2984])\n",
      "Epoch 1355, Loss 3.197060\n",
      "  params :  tensor([  5.0584, -15.5539])\n",
      "  grad :  tensor([-0.0526,  0.2979])\n",
      "Epoch 1356, Loss 3.196143\n",
      "  params :  tensor([  5.0590, -15.5569])\n",
      "  grad :  tensor([-0.0525,  0.2974])\n",
      "Epoch 1357, Loss 3.195231\n",
      "  params :  tensor([  5.0595, -15.5599])\n",
      "  grad :  tensor([-0.0524,  0.2969])\n",
      "Epoch 1358, Loss 3.194324\n",
      "  params :  tensor([  5.0600, -15.5629])\n",
      "  grad :  tensor([-0.0524,  0.2964])\n",
      "Epoch 1359, Loss 3.193420\n",
      "  params :  tensor([  5.0605, -15.5658])\n",
      "  grad :  tensor([-0.0523,  0.2959])\n",
      "Epoch 1360, Loss 3.192517\n",
      "  params :  tensor([  5.0610, -15.5688])\n",
      "  grad :  tensor([-0.0522,  0.2954])\n",
      "Epoch 1361, Loss 3.191616\n",
      "  params :  tensor([  5.0616, -15.5717])\n",
      "  grad :  tensor([-0.0521,  0.2949])\n",
      "Epoch 1362, Loss 3.190720\n",
      "  params :  tensor([  5.0621, -15.5747])\n",
      "  grad :  tensor([-0.0520,  0.2944])\n",
      "Epoch 1363, Loss 3.189829\n",
      "  params :  tensor([  5.0626, -15.5776])\n",
      "  grad :  tensor([-0.0519,  0.2939])\n",
      "Epoch 1364, Loss 3.188938\n",
      "  params :  tensor([  5.0631, -15.5805])\n",
      "  grad :  tensor([-0.0518,  0.2934])\n",
      "Epoch 1365, Loss 3.188051\n",
      "  params :  tensor([  5.0636, -15.5835])\n",
      "  grad :  tensor([-0.0517,  0.2929])\n",
      "Epoch 1366, Loss 3.187166\n",
      "  params :  tensor([  5.0642, -15.5864])\n",
      "  grad :  tensor([-0.0516,  0.2924])\n",
      "Epoch 1367, Loss 3.186287\n",
      "  params :  tensor([  5.0647, -15.5893])\n",
      "  grad :  tensor([-0.0516,  0.2919])\n",
      "Epoch 1368, Loss 3.185409\n",
      "  params :  tensor([  5.0652, -15.5922])\n",
      "  grad :  tensor([-0.0515,  0.2914])\n",
      "Epoch 1369, Loss 3.184534\n",
      "  params :  tensor([  5.0657, -15.5951])\n",
      "  grad :  tensor([-0.0514,  0.2909])\n",
      "Epoch 1370, Loss 3.183662\n",
      "  params :  tensor([  5.0662, -15.5980])\n",
      "  grad :  tensor([-0.0513,  0.2904])\n",
      "Epoch 1371, Loss 3.182792\n",
      "  params :  tensor([  5.0667, -15.6009])\n",
      "  grad :  tensor([-0.0512,  0.2899])\n",
      "Epoch 1372, Loss 3.181925\n",
      "  params :  tensor([  5.0672, -15.6038])\n",
      "  grad :  tensor([-0.0511,  0.2894])\n",
      "Epoch 1373, Loss 3.181063\n",
      "  params :  tensor([  5.0678, -15.6067])\n",
      "  grad :  tensor([-0.0510,  0.2890])\n",
      "Epoch 1374, Loss 3.180201\n",
      "  params :  tensor([  5.0683, -15.6096])\n",
      "  grad :  tensor([-0.0509,  0.2885])\n",
      "Epoch 1375, Loss 3.179347\n",
      "  params :  tensor([  5.0688, -15.6125])\n",
      "  grad :  tensor([-0.0509,  0.2880])\n",
      "Epoch 1376, Loss 3.178490\n",
      "  params :  tensor([  5.0693, -15.6154])\n",
      "  grad :  tensor([-0.0508,  0.2875])\n",
      "Epoch 1377, Loss 3.177638\n",
      "  params :  tensor([  5.0698, -15.6182])\n",
      "  grad :  tensor([-0.0507,  0.2870])\n",
      "Epoch 1378, Loss 3.176789\n",
      "  params :  tensor([  5.0703, -15.6211])\n",
      "  grad :  tensor([-0.0506,  0.2865])\n",
      "Epoch 1379, Loss 3.175945\n",
      "  params :  tensor([  5.0708, -15.6240])\n",
      "  grad :  tensor([-0.0505,  0.2860])\n",
      "Epoch 1380, Loss 3.175101\n",
      "  params :  tensor([  5.0713, -15.6268])\n",
      "  grad :  tensor([-0.0504,  0.2855])\n",
      "Epoch 1381, Loss 3.174262\n",
      "  params :  tensor([  5.0718, -15.6297])\n",
      "  grad :  tensor([-0.0504,  0.2850])\n",
      "Epoch 1382, Loss 3.173425\n",
      "  params :  tensor([  5.0723, -15.6325])\n",
      "  grad :  tensor([-0.0503,  0.2846])\n",
      "Epoch 1383, Loss 3.172590\n",
      "  params :  tensor([  5.0728, -15.6353])\n",
      "  grad :  tensor([-0.0502,  0.2841])\n",
      "Epoch 1384, Loss 3.171759\n",
      "  params :  tensor([  5.0733, -15.6382])\n",
      "  grad :  tensor([-0.0501,  0.2836])\n",
      "Epoch 1385, Loss 3.170929\n",
      "  params :  tensor([  5.0738, -15.6410])\n",
      "  grad :  tensor([-0.0500,  0.2831])\n",
      "Epoch 1386, Loss 3.170103\n",
      "  params :  tensor([  5.0743, -15.6438])\n",
      "  grad :  tensor([-0.0499,  0.2826])\n",
      "Epoch 1387, Loss 3.169280\n",
      "  params :  tensor([  5.0748, -15.6467])\n",
      "  grad :  tensor([-0.0498,  0.2822])\n",
      "Epoch 1388, Loss 3.168462\n",
      "  params :  tensor([  5.0753, -15.6495])\n",
      "  grad :  tensor([-0.0498,  0.2817])\n",
      "Epoch 1389, Loss 3.167644\n",
      "  params :  tensor([  5.0758, -15.6523])\n",
      "  grad :  tensor([-0.0497,  0.2812])\n",
      "Epoch 1390, Loss 3.166827\n",
      "  params :  tensor([  5.0763, -15.6551])\n",
      "  grad :  tensor([-0.0496,  0.2807])\n",
      "Epoch 1391, Loss 3.166017\n",
      "  params :  tensor([  5.0768, -15.6579])\n",
      "  grad :  tensor([-0.0495,  0.2802])\n",
      "Epoch 1392, Loss 3.165207\n",
      "  params :  tensor([  5.0773, -15.6607])\n",
      "  grad :  tensor([-0.0494,  0.2798])\n",
      "Epoch 1393, Loss 3.164401\n",
      "  params :  tensor([  5.0778, -15.6635])\n",
      "  grad :  tensor([-0.0493,  0.2793])\n",
      "Epoch 1394, Loss 3.163594\n",
      "  params :  tensor([  5.0783, -15.6663])\n",
      "  grad :  tensor([-0.0492,  0.2788])\n",
      "Epoch 1395, Loss 3.162795\n",
      "  params :  tensor([  5.0788, -15.6691])\n",
      "  grad :  tensor([-0.0492,  0.2783])\n",
      "Epoch 1396, Loss 3.161996\n",
      "  params :  tensor([  5.0793, -15.6718])\n",
      "  grad :  tensor([-0.0491,  0.2779])\n",
      "Epoch 1397, Loss 3.161201\n",
      "  params :  tensor([  5.0797, -15.6746])\n",
      "  grad :  tensor([-0.0490,  0.2774])\n",
      "Epoch 1398, Loss 3.160410\n",
      "  params :  tensor([  5.0802, -15.6774])\n",
      "  grad :  tensor([-0.0489,  0.2769])\n",
      "Epoch 1399, Loss 3.159618\n",
      "  params :  tensor([  5.0807, -15.6802])\n",
      "  grad :  tensor([-0.0488,  0.2765])\n",
      "Epoch 1400, Loss 3.158830\n",
      "  params :  tensor([  5.0812, -15.6829])\n",
      "  grad :  tensor([-0.0488,  0.2760])\n",
      "Epoch 1401, Loss 3.158046\n",
      "  params :  tensor([  5.0817, -15.6857])\n",
      "  grad :  tensor([-0.0487,  0.2755])\n",
      "Epoch 1402, Loss 3.157263\n",
      "  params :  tensor([  5.0822, -15.6884])\n",
      "  grad :  tensor([-0.0486,  0.2751])\n",
      "Epoch 1403, Loss 3.156484\n",
      "  params :  tensor([  5.0827, -15.6912])\n",
      "  grad :  tensor([-0.0485,  0.2746])\n",
      "Epoch 1404, Loss 3.155708\n",
      "  params :  tensor([  5.0832, -15.6939])\n",
      "  grad :  tensor([-0.0484,  0.2741])\n",
      "Epoch 1405, Loss 3.154933\n",
      "  params :  tensor([  5.0836, -15.6966])\n",
      "  grad :  tensor([-0.0483,  0.2736])\n",
      "Epoch 1406, Loss 3.154162\n",
      "  params :  tensor([  5.0841, -15.6994])\n",
      "  grad :  tensor([-0.0483,  0.2732])\n",
      "Epoch 1407, Loss 3.153393\n",
      "  params :  tensor([  5.0846, -15.7021])\n",
      "  grad :  tensor([-0.0482,  0.2727])\n",
      "Epoch 1408, Loss 3.152628\n",
      "  params :  tensor([  5.0851, -15.7048])\n",
      "  grad :  tensor([-0.0481,  0.2723])\n",
      "Epoch 1409, Loss 3.151865\n",
      "  params :  tensor([  5.0856, -15.7075])\n",
      "  grad :  tensor([-0.0480,  0.2718])\n",
      "Epoch 1410, Loss 3.151101\n",
      "  params :  tensor([  5.0860, -15.7103])\n",
      "  grad :  tensor([-0.0479,  0.2713])\n",
      "Epoch 1411, Loss 3.150343\n",
      "  params :  tensor([  5.0865, -15.7130])\n",
      "  grad :  tensor([-0.0479,  0.2709])\n",
      "Epoch 1412, Loss 3.149587\n",
      "  params :  tensor([  5.0870, -15.7157])\n",
      "  grad :  tensor([-0.0478,  0.2704])\n",
      "Epoch 1413, Loss 3.148833\n",
      "  params :  tensor([  5.0875, -15.7184])\n",
      "  grad :  tensor([-0.0477,  0.2700])\n",
      "Epoch 1414, Loss 3.148083\n",
      "  params :  tensor([  5.0879, -15.7211])\n",
      "  grad :  tensor([-0.0476,  0.2695])\n",
      "Epoch 1415, Loss 3.147335\n",
      "  params :  tensor([  5.0884, -15.7238])\n",
      "  grad :  tensor([-0.0475,  0.2690])\n",
      "Epoch 1416, Loss 3.146588\n",
      "  params :  tensor([  5.0889, -15.7264])\n",
      "  grad :  tensor([-0.0474,  0.2686])\n",
      "Epoch 1417, Loss 3.145845\n",
      "  params :  tensor([  5.0894, -15.7291])\n",
      "  grad :  tensor([-0.0474,  0.2681])\n",
      "Epoch 1418, Loss 3.145105\n",
      "  params :  tensor([  5.0898, -15.7318])\n",
      "  grad :  tensor([-0.0473,  0.2677])\n",
      "Epoch 1419, Loss 3.144367\n",
      "  params :  tensor([  5.0903, -15.7345])\n",
      "  grad :  tensor([-0.0472,  0.2672])\n",
      "Epoch 1420, Loss 3.143630\n",
      "  params :  tensor([  5.0908, -15.7371])\n",
      "  grad :  tensor([-0.0471,  0.2668])\n",
      "Epoch 1421, Loss 3.142899\n",
      "  params :  tensor([  5.0913, -15.7398])\n",
      "  grad :  tensor([-0.0470,  0.2663])\n",
      "Epoch 1422, Loss 3.142166\n",
      "  params :  tensor([  5.0917, -15.7425])\n",
      "  grad :  tensor([-0.0469,  0.2659])\n",
      "Epoch 1423, Loss 3.141439\n",
      "  params :  tensor([  5.0922, -15.7451])\n",
      "  grad :  tensor([-0.0469,  0.2654])\n",
      "Epoch 1424, Loss 3.140712\n",
      "  params :  tensor([  5.0927, -15.7478])\n",
      "  grad :  tensor([-0.0468,  0.2649])\n",
      "Epoch 1425, Loss 3.139989\n",
      "  params :  tensor([  5.0931, -15.7504])\n",
      "  grad :  tensor([-0.0467,  0.2645])\n",
      "Epoch 1426, Loss 3.139271\n",
      "  params :  tensor([  5.0936, -15.7530])\n",
      "  grad :  tensor([-0.0466,  0.2641])\n",
      "Epoch 1427, Loss 3.138551\n",
      "  params :  tensor([  5.0941, -15.7557])\n",
      "  grad :  tensor([-0.0466,  0.2636])\n",
      "Epoch 1428, Loss 3.137835\n",
      "  params :  tensor([  5.0945, -15.7583])\n",
      "  grad :  tensor([-0.0465,  0.2632])\n",
      "Epoch 1429, Loss 3.137121\n",
      "  params :  tensor([  5.0950, -15.7609])\n",
      "  grad :  tensor([-0.0464,  0.2627])\n",
      "Epoch 1430, Loss 3.136409\n",
      "  params :  tensor([  5.0955, -15.7636])\n",
      "  grad :  tensor([-0.0463,  0.2623])\n",
      "Epoch 1431, Loss 3.135702\n",
      "  params :  tensor([  5.0959, -15.7662])\n",
      "  grad :  tensor([-0.0462,  0.2618])\n",
      "Epoch 1432, Loss 3.134994\n",
      "  params :  tensor([  5.0964, -15.7688])\n",
      "  grad :  tensor([-0.0461,  0.2614])\n",
      "Epoch 1433, Loss 3.134292\n",
      "  params :  tensor([  5.0968, -15.7714])\n",
      "  grad :  tensor([-0.0461,  0.2609])\n",
      "Epoch 1434, Loss 3.133590\n",
      "  params :  tensor([  5.0973, -15.7740])\n",
      "  grad :  tensor([-0.0460,  0.2605])\n",
      "Epoch 1435, Loss 3.132889\n",
      "  params :  tensor([  5.0978, -15.7766])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad :  tensor([-0.0459,  0.2600])\n",
      "Epoch 1436, Loss 3.132194\n",
      "  params :  tensor([  5.0982, -15.7792])\n",
      "  grad :  tensor([-0.0459,  0.2596])\n",
      "Epoch 1437, Loss 3.131500\n",
      "  params :  tensor([  5.0987, -15.7818])\n",
      "  grad :  tensor([-0.0458,  0.2592])\n",
      "Epoch 1438, Loss 3.130810\n",
      "  params :  tensor([  5.0991, -15.7844])\n",
      "  grad :  tensor([-0.0457,  0.2587])\n",
      "Epoch 1439, Loss 3.130119\n",
      "  params :  tensor([  5.0996, -15.7870])\n",
      "  grad :  tensor([-0.0456,  0.2583])\n",
      "Epoch 1440, Loss 3.129432\n",
      "  params :  tensor([  5.1000, -15.7895])\n",
      "  grad :  tensor([-0.0455,  0.2578])\n",
      "Epoch 1441, Loss 3.128746\n",
      "  params :  tensor([  5.1005, -15.7921])\n",
      "  grad :  tensor([-0.0455,  0.2574])\n",
      "Epoch 1442, Loss 3.128064\n",
      "  params :  tensor([  5.1010, -15.7947])\n",
      "  grad :  tensor([-0.0454,  0.2570])\n",
      "Epoch 1443, Loss 3.127382\n",
      "  params :  tensor([  5.1014, -15.7973])\n",
      "  grad :  tensor([-0.0453,  0.2565])\n",
      "Epoch 1444, Loss 3.126705\n",
      "  params :  tensor([  5.1019, -15.7998])\n",
      "  grad :  tensor([-0.0453,  0.2561])\n",
      "Epoch 1445, Loss 3.126030\n",
      "  params :  tensor([  5.1023, -15.8024])\n",
      "  grad :  tensor([-0.0452,  0.2557])\n",
      "Epoch 1446, Loss 3.125356\n",
      "  params :  tensor([  5.1028, -15.8049])\n",
      "  grad :  tensor([-0.0451,  0.2552])\n",
      "Epoch 1447, Loss 3.124683\n",
      "  params :  tensor([  5.1032, -15.8075])\n",
      "  grad :  tensor([-0.0450,  0.2548])\n",
      "Epoch 1448, Loss 3.124016\n",
      "  params :  tensor([  5.1037, -15.8100])\n",
      "  grad :  tensor([-0.0449,  0.2544])\n",
      "Epoch 1449, Loss 3.123349\n",
      "  params :  tensor([  5.1041, -15.8126])\n",
      "  grad :  tensor([-0.0449,  0.2539])\n",
      "Epoch 1450, Loss 3.122686\n",
      "  params :  tensor([  5.1046, -15.8151])\n",
      "  grad :  tensor([-0.0448,  0.2535])\n",
      "Epoch 1451, Loss 3.122022\n",
      "  params :  tensor([  5.1050, -15.8176])\n",
      "  grad :  tensor([-0.0447,  0.2531])\n",
      "Epoch 1452, Loss 3.121362\n",
      "  params :  tensor([  5.1055, -15.8201])\n",
      "  grad :  tensor([-0.0446,  0.2526])\n",
      "Epoch 1453, Loss 3.120707\n",
      "  params :  tensor([  5.1059, -15.8227])\n",
      "  grad :  tensor([-0.0445,  0.2522])\n",
      "Epoch 1454, Loss 3.120049\n",
      "  params :  tensor([  5.1063, -15.8252])\n",
      "  grad :  tensor([-0.0445,  0.2518])\n",
      "Epoch 1455, Loss 3.119397\n",
      "  params :  tensor([  5.1068, -15.8277])\n",
      "  grad :  tensor([-0.0444,  0.2513])\n",
      "Epoch 1456, Loss 3.118746\n",
      "  params :  tensor([  5.1072, -15.8302])\n",
      "  grad :  tensor([-0.0443,  0.2509])\n",
      "Epoch 1457, Loss 3.118098\n",
      "  params :  tensor([  5.1077, -15.8327])\n",
      "  grad :  tensor([-0.0442,  0.2505])\n",
      "Epoch 1458, Loss 3.117451\n",
      "  params :  tensor([  5.1081, -15.8352])\n",
      "  grad :  tensor([-0.0442,  0.2501])\n",
      "Epoch 1459, Loss 3.116805\n",
      "  params :  tensor([  5.1086, -15.8377])\n",
      "  grad :  tensor([-0.0441,  0.2496])\n",
      "Epoch 1460, Loss 3.116164\n",
      "  params :  tensor([  5.1090, -15.8402])\n",
      "  grad :  tensor([-0.0440,  0.2492])\n",
      "Epoch 1461, Loss 3.115525\n",
      "  params :  tensor([  5.1094, -15.8427])\n",
      "  grad :  tensor([-0.0439,  0.2488])\n",
      "Epoch 1462, Loss 3.114886\n",
      "  params :  tensor([  5.1099, -15.8452])\n",
      "  grad :  tensor([-0.0439,  0.2484])\n",
      "Epoch 1463, Loss 3.114251\n",
      "  params :  tensor([  5.1103, -15.8477])\n",
      "  grad :  tensor([-0.0438,  0.2480])\n",
      "Epoch 1464, Loss 3.113617\n",
      "  params :  tensor([  5.1107, -15.8501])\n",
      "  grad :  tensor([-0.0437,  0.2475])\n",
      "Epoch 1465, Loss 3.112985\n",
      "  params :  tensor([  5.1112, -15.8526])\n",
      "  grad :  tensor([-0.0437,  0.2471])\n",
      "Epoch 1466, Loss 3.112358\n",
      "  params :  tensor([  5.1116, -15.8551])\n",
      "  grad :  tensor([-0.0436,  0.2467])\n",
      "Epoch 1467, Loss 3.111731\n",
      "  params :  tensor([  5.1121, -15.8575])\n",
      "  grad :  tensor([-0.0435,  0.2463])\n",
      "Epoch 1468, Loss 3.111103\n",
      "  params :  tensor([  5.1125, -15.8600])\n",
      "  grad :  tensor([-0.0434,  0.2459])\n",
      "Epoch 1469, Loss 3.110484\n",
      "  params :  tensor([  5.1129, -15.8624])\n",
      "  grad :  tensor([-0.0433,  0.2454])\n",
      "Epoch 1470, Loss 3.109859\n",
      "  params :  tensor([  5.1134, -15.8649])\n",
      "  grad :  tensor([-0.0433,  0.2450])\n",
      "Epoch 1471, Loss 3.109243\n",
      "  params :  tensor([  5.1138, -15.8673])\n",
      "  grad :  tensor([-0.0432,  0.2446])\n",
      "Epoch 1472, Loss 3.108627\n",
      "  params :  tensor([  5.1142, -15.8698])\n",
      "  grad :  tensor([-0.0431,  0.2442])\n",
      "Epoch 1473, Loss 3.108011\n",
      "  params :  tensor([  5.1147, -15.8722])\n",
      "  grad :  tensor([-0.0430,  0.2438])\n",
      "Epoch 1474, Loss 3.107401\n",
      "  params :  tensor([  5.1151, -15.8747])\n",
      "  grad :  tensor([-0.0430,  0.2434])\n",
      "Epoch 1475, Loss 3.106791\n",
      "  params :  tensor([  5.1155, -15.8771])\n",
      "  grad :  tensor([-0.0429,  0.2429])\n",
      "Epoch 1476, Loss 3.106180\n",
      "  params :  tensor([  5.1159, -15.8795])\n",
      "  grad :  tensor([-0.0428,  0.2425])\n",
      "Epoch 1477, Loss 3.105575\n",
      "  params :  tensor([  5.1164, -15.8819])\n",
      "  grad :  tensor([-0.0428,  0.2421])\n",
      "Epoch 1478, Loss 3.104972\n",
      "  params :  tensor([  5.1168, -15.8843])\n",
      "  grad :  tensor([-0.0427,  0.2417])\n",
      "Epoch 1479, Loss 3.104370\n",
      "  params :  tensor([  5.1172, -15.8868])\n",
      "  grad :  tensor([-0.0426,  0.2413])\n",
      "Epoch 1480, Loss 3.103770\n",
      "  params :  tensor([  5.1176, -15.8892])\n",
      "  grad :  tensor([-0.0425,  0.2409])\n",
      "Epoch 1481, Loss 3.103172\n",
      "  params :  tensor([  5.1181, -15.8916])\n",
      "  grad :  tensor([-0.0425,  0.2405])\n",
      "Epoch 1482, Loss 3.102576\n",
      "  params :  tensor([  5.1185, -15.8940])\n",
      "  grad :  tensor([-0.0424,  0.2401])\n",
      "Epoch 1483, Loss 3.101982\n",
      "  params :  tensor([  5.1189, -15.8964])\n",
      "  grad :  tensor([-0.0423,  0.2397])\n",
      "Epoch 1484, Loss 3.101390\n",
      "  params :  tensor([  5.1193, -15.8988])\n",
      "  grad :  tensor([-0.0423,  0.2393])\n",
      "Epoch 1485, Loss 3.100802\n",
      "  params :  tensor([  5.1198, -15.9011])\n",
      "  grad :  tensor([-0.0422,  0.2388])\n",
      "Epoch 1486, Loss 3.100213\n",
      "  params :  tensor([  5.1202, -15.9035])\n",
      "  grad :  tensor([-0.0421,  0.2384])\n",
      "Epoch 1487, Loss 3.099627\n",
      "  params :  tensor([  5.1206, -15.9059])\n",
      "  grad :  tensor([-0.0421,  0.2380])\n",
      "Epoch 1488, Loss 3.099044\n",
      "  params :  tensor([  5.1210, -15.9083])\n",
      "  grad :  tensor([-0.0420,  0.2376])\n",
      "Epoch 1489, Loss 3.098463\n",
      "  params :  tensor([  5.1214, -15.9107])\n",
      "  grad :  tensor([-0.0419,  0.2372])\n",
      "Epoch 1490, Loss 3.097883\n",
      "  params :  tensor([  5.1219, -15.9130])\n",
      "  grad :  tensor([-0.0418,  0.2368])\n",
      "Epoch 1491, Loss 3.097302\n",
      "  params :  tensor([  5.1223, -15.9154])\n",
      "  grad :  tensor([-0.0418,  0.2364])\n",
      "Epoch 1492, Loss 3.096727\n",
      "  params :  tensor([  5.1227, -15.9178])\n",
      "  grad :  tensor([-0.0417,  0.2360])\n",
      "Epoch 1493, Loss 3.096153\n",
      "  params :  tensor([  5.1231, -15.9201])\n",
      "  grad :  tensor([-0.0416,  0.2356])\n",
      "Epoch 1494, Loss 3.095583\n",
      "  params :  tensor([  5.1235, -15.9225])\n",
      "  grad :  tensor([-0.0416,  0.2352])\n",
      "Epoch 1495, Loss 3.095011\n",
      "  params :  tensor([  5.1239, -15.9248])\n",
      "  grad :  tensor([-0.0415,  0.2348])\n",
      "Epoch 1496, Loss 3.094444\n",
      "  params :  tensor([  5.1244, -15.9272])\n",
      "  grad :  tensor([-0.0414,  0.2344])\n",
      "Epoch 1497, Loss 3.093877\n",
      "  params :  tensor([  5.1248, -15.9295])\n",
      "  grad :  tensor([-0.0413,  0.2340])\n",
      "Epoch 1498, Loss 3.093314\n",
      "  params :  tensor([  5.1252, -15.9318])\n",
      "  grad :  tensor([-0.0413,  0.2336])\n",
      "Epoch 1499, Loss 3.092751\n",
      "  params :  tensor([  5.1256, -15.9342])\n",
      "  grad :  tensor([-0.0412,  0.2332])\n",
      "Epoch 1500, Loss 3.092191\n",
      "  params :  tensor([  5.1260, -15.9365])\n",
      "  grad :  tensor([-0.0411,  0.2328])\n",
      "Epoch 1501, Loss 3.091630\n",
      "  params :  tensor([  5.1264, -15.9388])\n",
      "  grad :  tensor([-0.0411,  0.2324])\n",
      "Epoch 1502, Loss 3.091074\n",
      "  params :  tensor([  5.1268, -15.9411])\n",
      "  grad :  tensor([-0.0410,  0.2320])\n",
      "Epoch 1503, Loss 3.090520\n",
      "  params :  tensor([  5.1272, -15.9435])\n",
      "  grad :  tensor([-0.0409,  0.2317])\n",
      "Epoch 1504, Loss 3.089969\n",
      "  params :  tensor([  5.1276, -15.9458])\n",
      "  grad :  tensor([-0.0408,  0.2313])\n",
      "Epoch 1505, Loss 3.089417\n",
      "  params :  tensor([  5.1281, -15.9481])\n",
      "  grad :  tensor([-0.0408,  0.2309])\n",
      "Epoch 1506, Loss 3.088867\n",
      "  params :  tensor([  5.1285, -15.9504])\n",
      "  grad :  tensor([-0.0407,  0.2305])\n",
      "Epoch 1507, Loss 3.088320\n",
      "  params :  tensor([  5.1289, -15.9527])\n",
      "  grad :  tensor([-0.0406,  0.2301])\n",
      "Epoch 1508, Loss 3.087775\n",
      "  params :  tensor([  5.1293, -15.9550])\n",
      "  grad :  tensor([-0.0406,  0.2297])\n",
      "Epoch 1509, Loss 3.087232\n",
      "  params :  tensor([  5.1297, -15.9573])\n",
      "  grad :  tensor([-0.0405,  0.2293])\n",
      "Epoch 1510, Loss 3.086690\n",
      "  params :  tensor([  5.1301, -15.9596])\n",
      "  grad :  tensor([-0.0404,  0.2289])\n",
      "Epoch 1511, Loss 3.086150\n",
      "  params :  tensor([  5.1305, -15.9618])\n",
      "  grad :  tensor([-0.0404,  0.2285])\n",
      "Epoch 1512, Loss 3.085612\n",
      "  params :  tensor([  5.1309, -15.9641])\n",
      "  grad :  tensor([-0.0403,  0.2281])\n",
      "Epoch 1513, Loss 3.085075\n",
      "  params :  tensor([  5.1313, -15.9664])\n",
      "  grad :  tensor([-0.0402,  0.2277])\n",
      "Epoch 1514, Loss 3.084542\n",
      "  params :  tensor([  5.1317, -15.9687])\n",
      "  grad :  tensor([-0.0402,  0.2274])\n",
      "Epoch 1515, Loss 3.084009\n",
      "  params :  tensor([  5.1321, -15.9709])\n",
      "  grad :  tensor([-0.0401,  0.2270])\n",
      "Epoch 1516, Loss 3.083478\n",
      "  params :  tensor([  5.1325, -15.9732])\n",
      "  grad :  tensor([-0.0400,  0.2266])\n",
      "Epoch 1517, Loss 3.082948\n",
      "  params :  tensor([  5.1329, -15.9755])\n",
      "  grad :  tensor([-0.0400,  0.2262])\n",
      "Epoch 1518, Loss 3.082422\n",
      "  params :  tensor([  5.1333, -15.9777])\n",
      "  grad :  tensor([-0.0399,  0.2258])\n",
      "Epoch 1519, Loss 3.081897\n",
      "  params :  tensor([  5.1337, -15.9800])\n",
      "  grad :  tensor([-0.0398,  0.2254])\n",
      "Epoch 1520, Loss 3.081373\n",
      "  params :  tensor([  5.1341, -15.9822])\n",
      "  grad :  tensor([-0.0398,  0.2250])\n",
      "Epoch 1521, Loss 3.080850\n",
      "  params :  tensor([  5.1345, -15.9845])\n",
      "  grad :  tensor([-0.0397,  0.2247])\n",
      "Epoch 1522, Loss 3.080331\n",
      "  params :  tensor([  5.1349, -15.9867])\n",
      "  grad :  tensor([-0.0396,  0.2243])\n",
      "Epoch 1523, Loss 3.079811\n",
      "  params :  tensor([  5.1353, -15.9890])\n",
      "  grad :  tensor([-0.0396,  0.2239])\n",
      "Epoch 1524, Loss 3.079296\n",
      "  params :  tensor([  5.1357, -15.9912])\n",
      "  grad :  tensor([-0.0395,  0.2235])\n",
      "Epoch 1525, Loss 3.078781\n",
      "  params :  tensor([  5.1361, -15.9934])\n",
      "  grad :  tensor([-0.0394,  0.2231])\n",
      "Epoch 1526, Loss 3.078268\n",
      "  params :  tensor([  5.1365, -15.9957])\n",
      "  grad :  tensor([-0.0394,  0.2228])\n",
      "Epoch 1527, Loss 3.077758\n",
      "  params :  tensor([  5.1369, -15.9979])\n",
      "  grad :  tensor([-0.0393,  0.2224])\n",
      "Epoch 1528, Loss 3.077248\n",
      "  params :  tensor([  5.1372, -16.0001])\n",
      "  grad :  tensor([-0.0392,  0.2220])\n",
      "Epoch 1529, Loss 3.076739\n",
      "  params :  tensor([  5.1376, -16.0023])\n",
      "  grad :  tensor([-0.0391,  0.2216])\n",
      "Epoch 1530, Loss 3.076232\n",
      "  params :  tensor([  5.1380, -16.0045])\n",
      "  grad :  tensor([-0.0391,  0.2213])\n",
      "Epoch 1531, Loss 3.075729\n",
      "  params :  tensor([  5.1384, -16.0067])\n",
      "  grad :  tensor([-0.0390,  0.2209])\n",
      "Epoch 1532, Loss 3.075225\n",
      "  params :  tensor([  5.1388, -16.0089])\n",
      "  grad :  tensor([-0.0390,  0.2205])\n",
      "Epoch 1533, Loss 3.074724\n",
      "  params :  tensor([  5.1392, -16.0111])\n",
      "  grad :  tensor([-0.0389,  0.2201])\n",
      "Epoch 1534, Loss 3.074227\n",
      "  params :  tensor([  5.1396, -16.0133])\n",
      "  grad :  tensor([-0.0388,  0.2198])\n",
      "Epoch 1535, Loss 3.073726\n",
      "  params :  tensor([  5.1400, -16.0155])\n",
      "  grad :  tensor([-0.0387,  0.2194])\n",
      "Epoch 1536, Loss 3.073232\n",
      "  params :  tensor([  5.1404, -16.0177])\n",
      "  grad :  tensor([-0.0387,  0.2190])\n",
      "Epoch 1537, Loss 3.072739\n",
      "  params :  tensor([  5.1407, -16.0199])\n",
      "  grad :  tensor([-0.0386,  0.2186])\n",
      "Epoch 1538, Loss 3.072245\n",
      "  params :  tensor([  5.1411, -16.0221])\n",
      "  grad :  tensor([-0.0385,  0.2183])\n",
      "Epoch 1539, Loss 3.071753\n",
      "  params :  tensor([  5.1415, -16.0243])\n",
      "  grad :  tensor([-0.0385,  0.2179])\n",
      "Epoch 1540, Loss 3.071265\n",
      "  params :  tensor([  5.1419, -16.0264])\n",
      "  grad :  tensor([-0.0384,  0.2175])\n",
      "Epoch 1541, Loss 3.070778\n",
      "  params :  tensor([  5.1423, -16.0286])\n",
      "  grad :  tensor([-0.0383,  0.2172])\n",
      "Epoch 1542, Loss 3.070293\n",
      "  params :  tensor([  5.1427, -16.0308])\n",
      "  grad :  tensor([-0.0383,  0.2168])\n",
      "Epoch 1543, Loss 3.069808\n",
      "  params :  tensor([  5.1430, -16.0330])\n",
      "  grad :  tensor([-0.0382,  0.2164])\n",
      "Epoch 1544, Loss 3.069326\n",
      "  params :  tensor([  5.1434, -16.0351])\n",
      "  grad :  tensor([-0.0382,  0.2161])\n",
      "Epoch 1545, Loss 3.068845\n",
      "  params :  tensor([  5.1438, -16.0373])\n",
      "  grad :  tensor([-0.0381,  0.2157])\n",
      "Epoch 1546, Loss 3.068366\n",
      "  params :  tensor([  5.1442, -16.0394])\n",
      "  grad :  tensor([-0.0380,  0.2153])\n",
      "Epoch 1547, Loss 3.067887\n",
      "  params :  tensor([  5.1446, -16.0416])\n",
      "  grad :  tensor([-0.0380,  0.2150])\n",
      "Epoch 1548, Loss 3.067412\n",
      "  params :  tensor([  5.1449, -16.0437])\n",
      "  grad :  tensor([-0.0379,  0.2146])\n",
      "Epoch 1549, Loss 3.066937\n",
      "  params :  tensor([  5.1453, -16.0459])\n",
      "  grad :  tensor([-0.0378,  0.2142])\n",
      "Epoch 1550, Loss 3.066463\n",
      "  params :  tensor([  5.1457, -16.0480])\n",
      "  grad :  tensor([-0.0378,  0.2139])\n",
      "Epoch 1551, Loss 3.065993\n",
      "  params :  tensor([  5.1461, -16.0501])\n",
      "  grad :  tensor([-0.0377,  0.2135])\n",
      "Epoch 1552, Loss 3.065524\n",
      "  params :  tensor([  5.1465, -16.0523])\n",
      "  grad :  tensor([-0.0376,  0.2131])\n",
      "Epoch 1553, Loss 3.065055\n",
      "  params :  tensor([  5.1468, -16.0544])\n",
      "  grad :  tensor([-0.0376,  0.2128])\n",
      "Epoch 1554, Loss 3.064588\n",
      "  params :  tensor([  5.1472, -16.0565])\n",
      "  grad :  tensor([-0.0375,  0.2124])\n",
      "Epoch 1555, Loss 3.064123\n",
      "  params :  tensor([  5.1476, -16.0586])\n",
      "  grad :  tensor([-0.0375,  0.2120])\n",
      "Epoch 1556, Loss 3.063660\n",
      "  params :  tensor([  5.1480, -16.0608])\n",
      "  grad :  tensor([-0.0374,  0.2117])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1557, Loss 3.063199\n",
      "  params :  tensor([  5.1483, -16.0629])\n",
      "  grad :  tensor([-0.0373,  0.2113])\n",
      "Epoch 1558, Loss 3.062738\n",
      "  params :  tensor([  5.1487, -16.0650])\n",
      "  grad :  tensor([-0.0373,  0.2110])\n",
      "Epoch 1559, Loss 3.062280\n",
      "  params :  tensor([  5.1491, -16.0671])\n",
      "  grad :  tensor([-0.0372,  0.2106])\n",
      "Epoch 1560, Loss 3.061822\n",
      "  params :  tensor([  5.1494, -16.0692])\n",
      "  grad :  tensor([-0.0371,  0.2103])\n",
      "Epoch 1561, Loss 3.061367\n",
      "  params :  tensor([  5.1498, -16.0713])\n",
      "  grad :  tensor([-0.0371,  0.2099])\n",
      "Epoch 1562, Loss 3.060913\n",
      "  params :  tensor([  5.1502, -16.0734])\n",
      "  grad :  tensor([-0.0370,  0.2095])\n",
      "Epoch 1563, Loss 3.060462\n",
      "  params :  tensor([  5.1506, -16.0755])\n",
      "  grad :  tensor([-0.0370,  0.2092])\n",
      "Epoch 1564, Loss 3.060011\n",
      "  params :  tensor([  5.1509, -16.0776])\n",
      "  grad :  tensor([-0.0369,  0.2088])\n",
      "Epoch 1565, Loss 3.059561\n",
      "  params :  tensor([  5.1513, -16.0796])\n",
      "  grad :  tensor([-0.0368,  0.2085])\n",
      "Epoch 1566, Loss 3.059114\n",
      "  params :  tensor([  5.1517, -16.0817])\n",
      "  grad :  tensor([-0.0368,  0.2081])\n",
      "Epoch 1567, Loss 3.058668\n",
      "  params :  tensor([  5.1520, -16.0838])\n",
      "  grad :  tensor([-0.0367,  0.2078])\n",
      "Epoch 1568, Loss 3.058221\n",
      "  params :  tensor([  5.1524, -16.0859])\n",
      "  grad :  tensor([-0.0366,  0.2074])\n",
      "Epoch 1569, Loss 3.057781\n",
      "  params :  tensor([  5.1528, -16.0880])\n",
      "  grad :  tensor([-0.0366,  0.2071])\n",
      "Epoch 1570, Loss 3.057338\n",
      "  params :  tensor([  5.1531, -16.0900])\n",
      "  grad :  tensor([-0.0365,  0.2067])\n",
      "Epoch 1571, Loss 3.056898\n",
      "  params :  tensor([  5.1535, -16.0921])\n",
      "  grad :  tensor([-0.0364,  0.2064])\n",
      "Epoch 1572, Loss 3.056458\n",
      "  params :  tensor([  5.1539, -16.0941])\n",
      "  grad :  tensor([-0.0364,  0.2060])\n",
      "Epoch 1573, Loss 3.056019\n",
      "  params :  tensor([  5.1542, -16.0962])\n",
      "  grad :  tensor([-0.0363,  0.2057])\n",
      "Epoch 1574, Loss 3.055585\n",
      "  params :  tensor([  5.1546, -16.0983])\n",
      "  grad :  tensor([-0.0363,  0.2053])\n",
      "Epoch 1575, Loss 3.055151\n",
      "  params :  tensor([  5.1549, -16.1003])\n",
      "  grad :  tensor([-0.0362,  0.2050])\n",
      "Epoch 1576, Loss 3.054717\n",
      "  params :  tensor([  5.1553, -16.1023])\n",
      "  grad :  tensor([-0.0361,  0.2046])\n",
      "Epoch 1577, Loss 3.054286\n",
      "  params :  tensor([  5.1557, -16.1044])\n",
      "  grad :  tensor([-0.0361,  0.2043])\n",
      "Epoch 1578, Loss 3.053857\n",
      "  params :  tensor([  5.1560, -16.1064])\n",
      "  grad :  tensor([-0.0360,  0.2039])\n",
      "Epoch 1579, Loss 3.053427\n",
      "  params :  tensor([  5.1564, -16.1085])\n",
      "  grad :  tensor([-0.0360,  0.2036])\n",
      "Epoch 1580, Loss 3.053000\n",
      "  params :  tensor([  5.1567, -16.1105])\n",
      "  grad :  tensor([-0.0359,  0.2032])\n",
      "Epoch 1581, Loss 3.052576\n",
      "  params :  tensor([  5.1571, -16.1125])\n",
      "  grad :  tensor([-0.0358,  0.2029])\n",
      "Epoch 1582, Loss 3.052152\n",
      "  params :  tensor([  5.1575, -16.1146])\n",
      "  grad :  tensor([-0.0358,  0.2025])\n",
      "Epoch 1583, Loss 3.051730\n",
      "  params :  tensor([  5.1578, -16.1166])\n",
      "  grad :  tensor([-0.0357,  0.2022])\n",
      "Epoch 1584, Loss 3.051306\n",
      "  params :  tensor([  5.1582, -16.1186])\n",
      "  grad :  tensor([-0.0357,  0.2018])\n",
      "Epoch 1585, Loss 3.050888\n",
      "  params :  tensor([  5.1585, -16.1206])\n",
      "  grad :  tensor([-0.0356,  0.2015])\n",
      "Epoch 1586, Loss 3.050471\n",
      "  params :  tensor([  5.1589, -16.1226])\n",
      "  grad :  tensor([-0.0355,  0.2012])\n",
      "Epoch 1587, Loss 3.050052\n",
      "  params :  tensor([  5.1592, -16.1246])\n",
      "  grad :  tensor([-0.0355,  0.2008])\n",
      "Epoch 1588, Loss 3.049639\n",
      "  params :  tensor([  5.1596, -16.1266])\n",
      "  grad :  tensor([-0.0354,  0.2005])\n",
      "Epoch 1589, Loss 3.049223\n",
      "  params :  tensor([  5.1599, -16.1286])\n",
      "  grad :  tensor([-0.0354,  0.2001])\n",
      "Epoch 1590, Loss 3.048811\n",
      "  params :  tensor([  5.1603, -16.1306])\n",
      "  grad :  tensor([-0.0353,  0.1998])\n",
      "Epoch 1591, Loss 3.048398\n",
      "  params :  tensor([  5.1607, -16.1326])\n",
      "  grad :  tensor([-0.0353,  0.1995])\n",
      "Epoch 1592, Loss 3.047991\n",
      "  params :  tensor([  5.1610, -16.1346])\n",
      "  grad :  tensor([-0.0352,  0.1991])\n",
      "Epoch 1593, Loss 3.047581\n",
      "  params :  tensor([  5.1614, -16.1366])\n",
      "  grad :  tensor([-0.0351,  0.1988])\n",
      "Epoch 1594, Loss 3.047173\n",
      "  params :  tensor([  5.1617, -16.1386])\n",
      "  grad :  tensor([-0.0351,  0.1984])\n",
      "Epoch 1595, Loss 3.046768\n",
      "  params :  tensor([  5.1621, -16.1406])\n",
      "  grad :  tensor([-0.0350,  0.1981])\n",
      "Epoch 1596, Loss 3.046362\n",
      "  params :  tensor([  5.1624, -16.1425])\n",
      "  grad :  tensor([-0.0349,  0.1978])\n",
      "Epoch 1597, Loss 3.045960\n",
      "  params :  tensor([  5.1628, -16.1445])\n",
      "  grad :  tensor([-0.0349,  0.1974])\n",
      "Epoch 1598, Loss 3.045559\n",
      "  params :  tensor([  5.1631, -16.1465])\n",
      "  grad :  tensor([-0.0348,  0.1971])\n",
      "Epoch 1599, Loss 3.045160\n",
      "  params :  tensor([  5.1635, -16.1485])\n",
      "  grad :  tensor([-0.0348,  0.1968])\n",
      "Epoch 1600, Loss 3.044759\n",
      "  params :  tensor([  5.1638, -16.1504])\n",
      "  grad :  tensor([-0.0347,  0.1964])\n",
      "Epoch 1601, Loss 3.044361\n",
      "  params :  tensor([  5.1641, -16.1524])\n",
      "  grad :  tensor([-0.0346,  0.1961])\n",
      "Epoch 1602, Loss 3.043966\n",
      "  params :  tensor([  5.1645, -16.1543])\n",
      "  grad :  tensor([-0.0346,  0.1958])\n",
      "Epoch 1603, Loss 3.043571\n",
      "  params :  tensor([  5.1648, -16.1563])\n",
      "  grad :  tensor([-0.0345,  0.1954])\n",
      "Epoch 1604, Loss 3.043176\n",
      "  params :  tensor([  5.1652, -16.1582])\n",
      "  grad :  tensor([-0.0345,  0.1951])\n",
      "Epoch 1605, Loss 3.042785\n",
      "  params :  tensor([  5.1655, -16.1602])\n",
      "  grad :  tensor([-0.0344,  0.1948])\n",
      "Epoch 1606, Loss 3.042395\n",
      "  params :  tensor([  5.1659, -16.1621])\n",
      "  grad :  tensor([-0.0343,  0.1944])\n",
      "Epoch 1607, Loss 3.042005\n",
      "  params :  tensor([  5.1662, -16.1641])\n",
      "  grad :  tensor([-0.0343,  0.1941])\n",
      "Epoch 1608, Loss 3.041615\n",
      "  params :  tensor([  5.1666, -16.1660])\n",
      "  grad :  tensor([-0.0342,  0.1938])\n",
      "Epoch 1609, Loss 3.041230\n",
      "  params :  tensor([  5.1669, -16.1680])\n",
      "  grad :  tensor([-0.0342,  0.1934])\n",
      "Epoch 1610, Loss 3.040844\n",
      "  params :  tensor([  5.1672, -16.1699])\n",
      "  grad :  tensor([-0.0341,  0.1931])\n",
      "Epoch 1611, Loss 3.040461\n",
      "  params :  tensor([  5.1676, -16.1718])\n",
      "  grad :  tensor([-0.0341,  0.1928])\n",
      "Epoch 1612, Loss 3.040077\n",
      "  params :  tensor([  5.1679, -16.1737])\n",
      "  grad :  tensor([-0.0340,  0.1925])\n",
      "Epoch 1613, Loss 3.039695\n",
      "  params :  tensor([  5.1683, -16.1757])\n",
      "  grad :  tensor([-0.0339,  0.1921])\n",
      "Epoch 1614, Loss 3.039314\n",
      "  params :  tensor([  5.1686, -16.1776])\n",
      "  grad :  tensor([-0.0339,  0.1918])\n",
      "Epoch 1615, Loss 3.038934\n",
      "  params :  tensor([  5.1689, -16.1795])\n",
      "  grad :  tensor([-0.0338,  0.1915])\n",
      "Epoch 1616, Loss 3.038557\n",
      "  params :  tensor([  5.1693, -16.1814])\n",
      "  grad :  tensor([-0.0338,  0.1912])\n",
      "Epoch 1617, Loss 3.038181\n",
      "  params :  tensor([  5.1696, -16.1833])\n",
      "  grad :  tensor([-0.0337,  0.1908])\n",
      "Epoch 1618, Loss 3.037805\n",
      "  params :  tensor([  5.1699, -16.1852])\n",
      "  grad :  tensor([-0.0337,  0.1905])\n",
      "Epoch 1619, Loss 3.037432\n",
      "  params :  tensor([  5.1703, -16.1871])\n",
      "  grad :  tensor([-0.0336,  0.1902])\n",
      "Epoch 1620, Loss 3.037059\n",
      "  params :  tensor([  5.1706, -16.1890])\n",
      "  grad :  tensor([-0.0335,  0.1899])\n",
      "Epoch 1621, Loss 3.036689\n",
      "  params :  tensor([  5.1710, -16.1909])\n",
      "  grad :  tensor([-0.0335,  0.1895])\n",
      "Epoch 1622, Loss 3.036319\n",
      "  params :  tensor([  5.1713, -16.1928])\n",
      "  grad :  tensor([-0.0334,  0.1892])\n",
      "Epoch 1623, Loss 3.035949\n",
      "  params :  tensor([  5.1716, -16.1947])\n",
      "  grad :  tensor([-0.0334,  0.1889])\n",
      "Epoch 1624, Loss 3.035583\n",
      "  params :  tensor([  5.1720, -16.1966])\n",
      "  grad :  tensor([-0.0333,  0.1886])\n",
      "Epoch 1625, Loss 3.035216\n",
      "  params :  tensor([  5.1723, -16.1985])\n",
      "  grad :  tensor([-0.0333,  0.1883])\n",
      "Epoch 1626, Loss 3.034849\n",
      "  params :  tensor([  5.1726, -16.2003])\n",
      "  grad :  tensor([-0.0332,  0.1879])\n",
      "Epoch 1627, Loss 3.034485\n",
      "  params :  tensor([  5.1729, -16.2022])\n",
      "  grad :  tensor([-0.0331,  0.1876])\n",
      "Epoch 1628, Loss 3.034123\n",
      "  params :  tensor([  5.1733, -16.2041])\n",
      "  grad :  tensor([-0.0331,  0.1873])\n",
      "Epoch 1629, Loss 3.033762\n",
      "  params :  tensor([  5.1736, -16.2060])\n",
      "  grad :  tensor([-0.0330,  0.1870])\n",
      "Epoch 1630, Loss 3.033402\n",
      "  params :  tensor([  5.1739, -16.2078])\n",
      "  grad :  tensor([-0.0330,  0.1867])\n",
      "Epoch 1631, Loss 3.033041\n",
      "  params :  tensor([  5.1743, -16.2097])\n",
      "  grad :  tensor([-0.0329,  0.1863])\n",
      "Epoch 1632, Loss 3.032685\n",
      "  params :  tensor([  5.1746, -16.2116])\n",
      "  grad :  tensor([-0.0329,  0.1860])\n",
      "Epoch 1633, Loss 3.032329\n",
      "  params :  tensor([  5.1749, -16.2134])\n",
      "  grad :  tensor([-0.0328,  0.1857])\n",
      "Epoch 1634, Loss 3.031973\n",
      "  params :  tensor([  5.1753, -16.2153])\n",
      "  grad :  tensor([-0.0327,  0.1854])\n",
      "Epoch 1635, Loss 3.031619\n",
      "  params :  tensor([  5.1756, -16.2171])\n",
      "  grad :  tensor([-0.0327,  0.1851])\n",
      "Epoch 1636, Loss 3.031265\n",
      "  params :  tensor([  5.1759, -16.2190])\n",
      "  grad :  tensor([-0.0326,  0.1848])\n",
      "Epoch 1637, Loss 3.030913\n",
      "  params :  tensor([  5.1762, -16.2208])\n",
      "  grad :  tensor([-0.0326,  0.1845])\n",
      "Epoch 1638, Loss 3.030564\n",
      "  params :  tensor([  5.1766, -16.2226])\n",
      "  grad :  tensor([-0.0325,  0.1841])\n",
      "Epoch 1639, Loss 3.030215\n",
      "  params :  tensor([  5.1769, -16.2245])\n",
      "  grad :  tensor([-0.0325,  0.1838])\n",
      "Epoch 1640, Loss 3.029867\n",
      "  params :  tensor([  5.1772, -16.2263])\n",
      "  grad :  tensor([-0.0324,  0.1835])\n",
      "Epoch 1641, Loss 3.029518\n",
      "  params :  tensor([  5.1775, -16.2282])\n",
      "  grad :  tensor([-0.0324,  0.1832])\n",
      "Epoch 1642, Loss 3.029173\n",
      "  params :  tensor([  5.1779, -16.2300])\n",
      "  grad :  tensor([-0.0323,  0.1829])\n",
      "Epoch 1643, Loss 3.028828\n",
      "  params :  tensor([  5.1782, -16.2318])\n",
      "  grad :  tensor([-0.0323,  0.1826])\n",
      "Epoch 1644, Loss 3.028486\n",
      "  params :  tensor([  5.1785, -16.2336])\n",
      "  grad :  tensor([-0.0322,  0.1823])\n",
      "Epoch 1645, Loss 3.028142\n",
      "  params :  tensor([  5.1788, -16.2355])\n",
      "  grad :  tensor([-0.0321,  0.1820])\n",
      "Epoch 1646, Loss 3.027802\n",
      "  params :  tensor([  5.1791, -16.2373])\n",
      "  grad :  tensor([-0.0321,  0.1817])\n",
      "Epoch 1647, Loss 3.027463\n",
      "  params :  tensor([  5.1795, -16.2391])\n",
      "  grad :  tensor([-0.0320,  0.1813])\n",
      "Epoch 1648, Loss 3.027122\n",
      "  params :  tensor([  5.1798, -16.2409])\n",
      "  grad :  tensor([-0.0320,  0.1810])\n",
      "Epoch 1649, Loss 3.026784\n",
      "  params :  tensor([  5.1801, -16.2427])\n",
      "  grad :  tensor([-0.0319,  0.1807])\n",
      "Epoch 1650, Loss 3.026447\n",
      "  params :  tensor([  5.1804, -16.2445])\n",
      "  grad :  tensor([-0.0319,  0.1804])\n",
      "Epoch 1651, Loss 3.026111\n",
      "  params :  tensor([  5.1807, -16.2463])\n",
      "  grad :  tensor([-0.0318,  0.1801])\n",
      "Epoch 1652, Loss 3.025780\n",
      "  params :  tensor([  5.1811, -16.2481])\n",
      "  grad :  tensor([-0.0318,  0.1798])\n",
      "Epoch 1653, Loss 3.025447\n",
      "  params :  tensor([  5.1814, -16.2499])\n",
      "  grad :  tensor([-0.0317,  0.1795])\n",
      "Epoch 1654, Loss 3.025114\n",
      "  params :  tensor([  5.1817, -16.2517])\n",
      "  grad :  tensor([-0.0317,  0.1792])\n",
      "Epoch 1655, Loss 3.024782\n",
      "  params :  tensor([  5.1820, -16.2535])\n",
      "  grad :  tensor([-0.0316,  0.1789])\n",
      "Epoch 1656, Loss 3.024452\n",
      "  params :  tensor([  5.1823, -16.2553])\n",
      "  grad :  tensor([-0.0316,  0.1786])\n",
      "Epoch 1657, Loss 3.024125\n",
      "  params :  tensor([  5.1826, -16.2570])\n",
      "  grad :  tensor([-0.0315,  0.1783])\n",
      "Epoch 1658, Loss 3.023796\n",
      "  params :  tensor([  5.1829, -16.2588])\n",
      "  grad :  tensor([-0.0315,  0.1780])\n",
      "Epoch 1659, Loss 3.023471\n",
      "  params :  tensor([  5.1833, -16.2606])\n",
      "  grad :  tensor([-0.0314,  0.1777])\n",
      "Epoch 1660, Loss 3.023145\n",
      "  params :  tensor([  5.1836, -16.2624])\n",
      "  grad :  tensor([-0.0313,  0.1774])\n",
      "Epoch 1661, Loss 3.022820\n",
      "  params :  tensor([  5.1839, -16.2641])\n",
      "  grad :  tensor([-0.0313,  0.1771])\n",
      "Epoch 1662, Loss 3.022498\n",
      "  params :  tensor([  5.1842, -16.2659])\n",
      "  grad :  tensor([-0.0312,  0.1768])\n",
      "Epoch 1663, Loss 3.022177\n",
      "  params :  tensor([  5.1845, -16.2677])\n",
      "  grad :  tensor([-0.0312,  0.1765])\n",
      "Epoch 1664, Loss 3.021855\n",
      "  params :  tensor([  5.1848, -16.2694])\n",
      "  grad :  tensor([-0.0311,  0.1762])\n",
      "Epoch 1665, Loss 3.021534\n",
      "  params :  tensor([  5.1851, -16.2712])\n",
      "  grad :  tensor([-0.0311,  0.1759])\n",
      "Epoch 1666, Loss 3.021217\n",
      "  params :  tensor([  5.1854, -16.2730])\n",
      "  grad :  tensor([-0.0310,  0.1756])\n",
      "Epoch 1667, Loss 3.020898\n",
      "  params :  tensor([  5.1858, -16.2747])\n",
      "  grad :  tensor([-0.0310,  0.1753])\n",
      "Epoch 1668, Loss 3.020582\n",
      "  params :  tensor([  5.1861, -16.2765])\n",
      "  grad :  tensor([-0.0309,  0.1750])\n",
      "Epoch 1669, Loss 3.020265\n",
      "  params :  tensor([  5.1864, -16.2782])\n",
      "  grad :  tensor([-0.0309,  0.1747])\n",
      "Epoch 1670, Loss 3.019952\n",
      "  params :  tensor([  5.1867, -16.2800])\n",
      "  grad :  tensor([-0.0308,  0.1744])\n",
      "Epoch 1671, Loss 3.019639\n",
      "  params :  tensor([  5.1870, -16.2817])\n",
      "  grad :  tensor([-0.0308,  0.1741])\n",
      "Epoch 1672, Loss 3.019325\n",
      "  params :  tensor([  5.1873, -16.2834])\n",
      "  grad :  tensor([-0.0307,  0.1738])\n",
      "Epoch 1673, Loss 3.019016\n",
      "  params :  tensor([  5.1876, -16.2852])\n",
      "  grad :  tensor([-0.0307,  0.1735])\n",
      "Epoch 1674, Loss 3.018706\n",
      "  params :  tensor([  5.1879, -16.2869])\n",
      "  grad :  tensor([-0.0306,  0.1732])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1675, Loss 3.018395\n",
      "  params :  tensor([  5.1882, -16.2886])\n",
      "  grad :  tensor([-0.0305,  0.1729])\n",
      "Epoch 1676, Loss 3.018089\n",
      "  params :  tensor([  5.1885, -16.2904])\n",
      "  grad :  tensor([-0.0305,  0.1726])\n",
      "Epoch 1677, Loss 3.017780\n",
      "  params :  tensor([  5.1888, -16.2921])\n",
      "  grad :  tensor([-0.0304,  0.1723])\n",
      "Epoch 1678, Loss 3.017475\n",
      "  params :  tensor([  5.1891, -16.2938])\n",
      "  grad :  tensor([-0.0304,  0.1720])\n",
      "Epoch 1679, Loss 3.017170\n",
      "  params :  tensor([  5.1894, -16.2955])\n",
      "  grad :  tensor([-0.0303,  0.1717])\n",
      "Epoch 1680, Loss 3.016867\n",
      "  params :  tensor([  5.1897, -16.2972])\n",
      "  grad :  tensor([-0.0303,  0.1715])\n",
      "Epoch 1681, Loss 3.016564\n",
      "  params :  tensor([  5.1900, -16.2989])\n",
      "  grad :  tensor([-0.0302,  0.1712])\n",
      "Epoch 1682, Loss 3.016262\n",
      "  params :  tensor([  5.1903, -16.3006])\n",
      "  grad :  tensor([-0.0302,  0.1709])\n",
      "Epoch 1683, Loss 3.015959\n",
      "  params :  tensor([  5.1906, -16.3024])\n",
      "  grad :  tensor([-0.0301,  0.1706])\n",
      "Epoch 1684, Loss 3.015662\n",
      "  params :  tensor([  5.1909, -16.3041])\n",
      "  grad :  tensor([-0.0301,  0.1703])\n",
      "Epoch 1685, Loss 3.015361\n",
      "  params :  tensor([  5.1912, -16.3058])\n",
      "  grad :  tensor([-0.0300,  0.1700])\n",
      "Epoch 1686, Loss 3.015064\n",
      "  params :  tensor([  5.1915, -16.3075])\n",
      "  grad :  tensor([-0.0300,  0.1697])\n",
      "Epoch 1687, Loss 3.014768\n",
      "  params :  tensor([  5.1918, -16.3091])\n",
      "  grad :  tensor([-0.0299,  0.1694])\n",
      "Epoch 1688, Loss 3.014472\n",
      "  params :  tensor([  5.1921, -16.3108])\n",
      "  grad :  tensor([-0.0299,  0.1691])\n",
      "Epoch 1689, Loss 3.014179\n",
      "  params :  tensor([  5.1924, -16.3125])\n",
      "  grad :  tensor([-0.0298,  0.1688])\n",
      "Epoch 1690, Loss 3.013884\n",
      "  params :  tensor([  5.1927, -16.3142])\n",
      "  grad :  tensor([-0.0298,  0.1686])\n",
      "Epoch 1691, Loss 3.013591\n",
      "  params :  tensor([  5.1930, -16.3159])\n",
      "  grad :  tensor([-0.0297,  0.1683])\n",
      "Epoch 1692, Loss 3.013299\n",
      "  params :  tensor([  5.1933, -16.3176])\n",
      "  grad :  tensor([-0.0297,  0.1680])\n",
      "Epoch 1693, Loss 3.013008\n",
      "  params :  tensor([  5.1936, -16.3193])\n",
      "  grad :  tensor([-0.0296,  0.1677])\n",
      "Epoch 1694, Loss 3.012719\n",
      "  params :  tensor([  5.1939, -16.3209])\n",
      "  grad :  tensor([-0.0296,  0.1674])\n",
      "Epoch 1695, Loss 3.012431\n",
      "  params :  tensor([  5.1942, -16.3226])\n",
      "  grad :  tensor([-0.0295,  0.1671])\n",
      "Epoch 1696, Loss 3.012141\n",
      "  params :  tensor([  5.1945, -16.3243])\n",
      "  grad :  tensor([-0.0295,  0.1668])\n",
      "Epoch 1697, Loss 3.011855\n",
      "  params :  tensor([  5.1948, -16.3259])\n",
      "  grad :  tensor([-0.0294,  0.1666])\n",
      "Epoch 1698, Loss 3.011570\n",
      "  params :  tensor([  5.1951, -16.3276])\n",
      "  grad :  tensor([-0.0294,  0.1663])\n",
      "Epoch 1699, Loss 3.011284\n",
      "  params :  tensor([  5.1954, -16.3293])\n",
      "  grad :  tensor([-0.0293,  0.1660])\n",
      "Epoch 1700, Loss 3.011001\n",
      "  params :  tensor([  5.1957, -16.3309])\n",
      "  grad :  tensor([-0.0293,  0.1657])\n",
      "Epoch 1701, Loss 3.010718\n",
      "  params :  tensor([  5.1960, -16.3326])\n",
      "  grad :  tensor([-0.0292,  0.1654])\n",
      "Epoch 1702, Loss 3.010436\n",
      "  params :  tensor([  5.1963, -16.3342])\n",
      "  grad :  tensor([-0.0292,  0.1652])\n",
      "Epoch 1703, Loss 3.010156\n",
      "  params :  tensor([  5.1966, -16.3359])\n",
      "  grad :  tensor([-0.0291,  0.1649])\n",
      "Epoch 1704, Loss 3.009876\n",
      "  params :  tensor([  5.1968, -16.3375])\n",
      "  grad :  tensor([-0.0291,  0.1646])\n",
      "Epoch 1705, Loss 3.009595\n",
      "  params :  tensor([  5.1971, -16.3392])\n",
      "  grad :  tensor([-0.0290,  0.1643])\n",
      "Epoch 1706, Loss 3.009319\n",
      "  params :  tensor([  5.1974, -16.3408])\n",
      "  grad :  tensor([-0.0290,  0.1640])\n",
      "Epoch 1707, Loss 3.009040\n",
      "  params :  tensor([  5.1977, -16.3424])\n",
      "  grad :  tensor([-0.0289,  0.1638])\n",
      "Epoch 1708, Loss 3.008763\n",
      "  params :  tensor([  5.1980, -16.3441])\n",
      "  grad :  tensor([-0.0289,  0.1635])\n",
      "Epoch 1709, Loss 3.008487\n",
      "  params :  tensor([  5.1983, -16.3457])\n",
      "  grad :  tensor([-0.0288,  0.1632])\n",
      "Epoch 1710, Loss 3.008215\n",
      "  params :  tensor([  5.1986, -16.3473])\n",
      "  grad :  tensor([-0.0288,  0.1629])\n",
      "Epoch 1711, Loss 3.007941\n",
      "  params :  tensor([  5.1989, -16.3490])\n",
      "  grad :  tensor([-0.0287,  0.1626])\n",
      "Epoch 1712, Loss 3.007668\n",
      "  params :  tensor([  5.1992, -16.3506])\n",
      "  grad :  tensor([-0.0287,  0.1624])\n",
      "Epoch 1713, Loss 3.007397\n",
      "  params :  tensor([  5.1994, -16.3522])\n",
      "  grad :  tensor([-0.0286,  0.1621])\n",
      "Epoch 1714, Loss 3.007126\n",
      "  params :  tensor([  5.1997, -16.3538])\n",
      "  grad :  tensor([-0.0286,  0.1618])\n",
      "Epoch 1715, Loss 3.006857\n",
      "  params :  tensor([  5.2000, -16.3554])\n",
      "  grad :  tensor([-0.0285,  0.1615])\n",
      "Epoch 1716, Loss 3.006586\n",
      "  params :  tensor([  5.2003, -16.3570])\n",
      "  grad :  tensor([-0.0285,  0.1613])\n",
      "Epoch 1717, Loss 3.006318\n",
      "  params :  tensor([  5.2006, -16.3587])\n",
      "  grad :  tensor([-0.0284,  0.1610])\n",
      "Epoch 1718, Loss 3.006052\n",
      "  params :  tensor([  5.2009, -16.3603])\n",
      "  grad :  tensor([-0.0284,  0.1607])\n",
      "Epoch 1719, Loss 3.005785\n",
      "  params :  tensor([  5.2012, -16.3619])\n",
      "  grad :  tensor([-0.0284,  0.1604])\n",
      "Epoch 1720, Loss 3.005521\n",
      "  params :  tensor([  5.2014, -16.3635])\n",
      "  grad :  tensor([-0.0283,  0.1602])\n",
      "Epoch 1721, Loss 3.005256\n",
      "  params :  tensor([  5.2017, -16.3651])\n",
      "  grad :  tensor([-0.0283,  0.1599])\n",
      "Epoch 1722, Loss 3.004993\n",
      "  params :  tensor([  5.2020, -16.3667])\n",
      "  grad :  tensor([-0.0282,  0.1596])\n",
      "Epoch 1723, Loss 3.004729\n",
      "  params :  tensor([  5.2023, -16.3683])\n",
      "  grad :  tensor([-0.0281,  0.1594])\n",
      "Epoch 1724, Loss 3.004467\n",
      "  params :  tensor([  5.2026, -16.3699])\n",
      "  grad :  tensor([-0.0281,  0.1591])\n",
      "Epoch 1725, Loss 3.004207\n",
      "  params :  tensor([  5.2028, -16.3714])\n",
      "  grad :  tensor([-0.0280,  0.1588])\n",
      "Epoch 1726, Loss 3.003947\n",
      "  params :  tensor([  5.2031, -16.3730])\n",
      "  grad :  tensor([-0.0280,  0.1586])\n",
      "Epoch 1727, Loss 3.003690\n",
      "  params :  tensor([  5.2034, -16.3746])\n",
      "  grad :  tensor([-0.0280,  0.1583])\n",
      "Epoch 1728, Loss 3.003431\n",
      "  params :  tensor([  5.2037, -16.3762])\n",
      "  grad :  tensor([-0.0279,  0.1580])\n",
      "Epoch 1729, Loss 3.003174\n",
      "  params :  tensor([  5.2040, -16.3778])\n",
      "  grad :  tensor([-0.0279,  0.1577])\n",
      "Epoch 1730, Loss 3.002918\n",
      "  params :  tensor([  5.2042, -16.3793])\n",
      "  grad :  tensor([-0.0278,  0.1575])\n",
      "Epoch 1731, Loss 3.002661\n",
      "  params :  tensor([  5.2045, -16.3809])\n",
      "  grad :  tensor([-0.0278,  0.1572])\n",
      "Epoch 1732, Loss 3.002406\n",
      "  params :  tensor([  5.2048, -16.3825])\n",
      "  grad :  tensor([-0.0277,  0.1569])\n",
      "Epoch 1733, Loss 3.002152\n",
      "  params :  tensor([  5.2051, -16.3840])\n",
      "  grad :  tensor([-0.0277,  0.1567])\n",
      "Epoch 1734, Loss 3.001901\n",
      "  params :  tensor([  5.2053, -16.3856])\n",
      "  grad :  tensor([-0.0276,  0.1564])\n",
      "Epoch 1735, Loss 3.001649\n",
      "  params :  tensor([  5.2056, -16.3872])\n",
      "  grad :  tensor([-0.0276,  0.1561])\n",
      "Epoch 1736, Loss 3.001395\n",
      "  params :  tensor([  5.2059, -16.3887])\n",
      "  grad :  tensor([-0.0275,  0.1559])\n",
      "Epoch 1737, Loss 3.001145\n",
      "  params :  tensor([  5.2062, -16.3903])\n",
      "  grad :  tensor([-0.0275,  0.1556])\n",
      "Epoch 1738, Loss 3.000898\n",
      "  params :  tensor([  5.2064, -16.3918])\n",
      "  grad :  tensor([-0.0274,  0.1553])\n",
      "Epoch 1739, Loss 3.000648\n",
      "  params :  tensor([  5.2067, -16.3934])\n",
      "  grad :  tensor([-0.0274,  0.1551])\n",
      "Epoch 1740, Loss 3.000400\n",
      "  params :  tensor([  5.2070, -16.3949])\n",
      "  grad :  tensor([-0.0273,  0.1548])\n",
      "Epoch 1741, Loss 3.000154\n",
      "  params :  tensor([  5.2073, -16.3965])\n",
      "  grad :  tensor([-0.0273,  0.1546])\n",
      "Epoch 1742, Loss 2.999907\n",
      "  params :  tensor([  5.2075, -16.3980])\n",
      "  grad :  tensor([-0.0273,  0.1543])\n",
      "Epoch 1743, Loss 2.999662\n",
      "  params :  tensor([  5.2078, -16.3996])\n",
      "  grad :  tensor([-0.0272,  0.1540])\n",
      "Epoch 1744, Loss 2.999417\n",
      "  params :  tensor([  5.2081, -16.4011])\n",
      "  grad :  tensor([-0.0272,  0.1538])\n",
      "Epoch 1745, Loss 2.999174\n",
      "  params :  tensor([  5.2084, -16.4026])\n",
      "  grad :  tensor([-0.0271,  0.1535])\n",
      "Epoch 1746, Loss 2.998930\n",
      "  params :  tensor([  5.2086, -16.4042])\n",
      "  grad :  tensor([-0.0271,  0.1533])\n",
      "Epoch 1747, Loss 2.998688\n",
      "  params :  tensor([  5.2089, -16.4057])\n",
      "  grad :  tensor([-0.0270,  0.1530])\n",
      "Epoch 1748, Loss 2.998448\n",
      "  params :  tensor([  5.2092, -16.4072])\n",
      "  grad :  tensor([-0.0270,  0.1527])\n",
      "Epoch 1749, Loss 2.998208\n",
      "  params :  tensor([  5.2094, -16.4088])\n",
      "  grad :  tensor([-0.0269,  0.1525])\n",
      "Epoch 1750, Loss 2.997968\n",
      "  params :  tensor([  5.2097, -16.4103])\n",
      "  grad :  tensor([-0.0269,  0.1522])\n",
      "Epoch 1751, Loss 2.997730\n",
      "  params :  tensor([  5.2100, -16.4118])\n",
      "  grad :  tensor([-0.0268,  0.1520])\n",
      "Epoch 1752, Loss 2.997490\n",
      "  params :  tensor([  5.2102, -16.4133])\n",
      "  grad :  tensor([-0.0268,  0.1517])\n",
      "Epoch 1753, Loss 2.997254\n",
      "  params :  tensor([  5.2105, -16.4148])\n",
      "  grad :  tensor([-0.0267,  0.1514])\n",
      "Epoch 1754, Loss 2.997018\n",
      "  params :  tensor([  5.2108, -16.4163])\n",
      "  grad :  tensor([-0.0267,  0.1512])\n",
      "Epoch 1755, Loss 2.996782\n",
      "  params :  tensor([  5.2110, -16.4179])\n",
      "  grad :  tensor([-0.0266,  0.1509])\n",
      "Epoch 1756, Loss 2.996548\n",
      "  params :  tensor([  5.2113, -16.4194])\n",
      "  grad :  tensor([-0.0266,  0.1507])\n",
      "Epoch 1757, Loss 2.996313\n",
      "  params :  tensor([  5.2116, -16.4209])\n",
      "  grad :  tensor([-0.0266,  0.1504])\n",
      "Epoch 1758, Loss 2.996081\n",
      "  params :  tensor([  5.2118, -16.4224])\n",
      "  grad :  tensor([-0.0265,  0.1502])\n",
      "Epoch 1759, Loss 2.995847\n",
      "  params :  tensor([  5.2121, -16.4239])\n",
      "  grad :  tensor([-0.0265,  0.1499])\n",
      "Epoch 1760, Loss 2.995615\n",
      "  params :  tensor([  5.2124, -16.4254])\n",
      "  grad :  tensor([-0.0264,  0.1496])\n",
      "Epoch 1761, Loss 2.995387\n",
      "  params :  tensor([  5.2126, -16.4269])\n",
      "  grad :  tensor([-0.0264,  0.1494])\n",
      "Epoch 1762, Loss 2.995156\n",
      "  params :  tensor([  5.2129, -16.4283])\n",
      "  grad :  tensor([-0.0263,  0.1491])\n",
      "Epoch 1763, Loss 2.994928\n",
      "  params :  tensor([  5.2132, -16.4298])\n",
      "  grad :  tensor([-0.0263,  0.1489])\n",
      "Epoch 1764, Loss 2.994699\n",
      "  params :  tensor([  5.2134, -16.4313])\n",
      "  grad :  tensor([-0.0263,  0.1486])\n",
      "Epoch 1765, Loss 2.994471\n",
      "  params :  tensor([  5.2137, -16.4328])\n",
      "  grad :  tensor([-0.0262,  0.1484])\n",
      "Epoch 1766, Loss 2.994245\n",
      "  params :  tensor([  5.2139, -16.4343])\n",
      "  grad :  tensor([-0.0262,  0.1481])\n",
      "Epoch 1767, Loss 2.994019\n",
      "  params :  tensor([  5.2142, -16.4358])\n",
      "  grad :  tensor([-0.0261,  0.1479])\n",
      "Epoch 1768, Loss 2.993794\n",
      "  params :  tensor([  5.2145, -16.4372])\n",
      "  grad :  tensor([-0.0261,  0.1476])\n",
      "Epoch 1769, Loss 2.993569\n",
      "  params :  tensor([  5.2147, -16.4387])\n",
      "  grad :  tensor([-0.0260,  0.1474])\n",
      "Epoch 1770, Loss 2.993344\n",
      "  params :  tensor([  5.2150, -16.4402])\n",
      "  grad :  tensor([-0.0260,  0.1471])\n",
      "Epoch 1771, Loss 2.993121\n",
      "  params :  tensor([  5.2152, -16.4417])\n",
      "  grad :  tensor([-0.0260,  0.1469])\n",
      "Epoch 1772, Loss 2.992900\n",
      "  params :  tensor([  5.2155, -16.4431])\n",
      "  grad :  tensor([-0.0259,  0.1466])\n",
      "Epoch 1773, Loss 2.992678\n",
      "  params :  tensor([  5.2158, -16.4446])\n",
      "  grad :  tensor([-0.0259,  0.1464])\n",
      "Epoch 1774, Loss 2.992457\n",
      "  params :  tensor([  5.2160, -16.4460])\n",
      "  grad :  tensor([-0.0258,  0.1461])\n",
      "Epoch 1775, Loss 2.992237\n",
      "  params :  tensor([  5.2163, -16.4475])\n",
      "  grad :  tensor([-0.0258,  0.1459])\n",
      "Epoch 1776, Loss 2.992017\n",
      "  params :  tensor([  5.2165, -16.4490])\n",
      "  grad :  tensor([-0.0257,  0.1456])\n",
      "Epoch 1777, Loss 2.991798\n",
      "  params :  tensor([  5.2168, -16.4504])\n",
      "  grad :  tensor([-0.0257,  0.1454])\n",
      "Epoch 1778, Loss 2.991582\n",
      "  params :  tensor([  5.2170, -16.4519])\n",
      "  grad :  tensor([-0.0256,  0.1451])\n",
      "Epoch 1779, Loss 2.991366\n",
      "  params :  tensor([  5.2173, -16.4533])\n",
      "  grad :  tensor([-0.0256,  0.1449])\n",
      "Epoch 1780, Loss 2.991146\n",
      "  params :  tensor([  5.2176, -16.4548])\n",
      "  grad :  tensor([-0.0256,  0.1446])\n",
      "Epoch 1781, Loss 2.990932\n",
      "  params :  tensor([  5.2178, -16.4562])\n",
      "  grad :  tensor([-0.0255,  0.1444])\n",
      "Epoch 1782, Loss 2.990719\n",
      "  params :  tensor([  5.2181, -16.4576])\n",
      "  grad :  tensor([-0.0255,  0.1442])\n",
      "Epoch 1783, Loss 2.990503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.2183, -16.4591])\n",
      "  grad :  tensor([-0.0254,  0.1439])\n",
      "Epoch 1784, Loss 2.990288\n",
      "  params :  tensor([  5.2186, -16.4605])\n",
      "  grad :  tensor([-0.0254,  0.1437])\n",
      "Epoch 1785, Loss 2.990078\n",
      "  params :  tensor([  5.2188, -16.4620])\n",
      "  grad :  tensor([-0.0253,  0.1434])\n",
      "Epoch 1786, Loss 2.989866\n",
      "  params :  tensor([  5.2191, -16.4634])\n",
      "  grad :  tensor([-0.0253,  0.1432])\n",
      "Epoch 1787, Loss 2.989655\n",
      "  params :  tensor([  5.2193, -16.4648])\n",
      "  grad :  tensor([-0.0252,  0.1429])\n",
      "Epoch 1788, Loss 2.989443\n",
      "  params :  tensor([  5.2196, -16.4662])\n",
      "  grad :  tensor([-0.0252,  0.1427])\n",
      "Epoch 1789, Loss 2.989233\n",
      "  params :  tensor([  5.2198, -16.4677])\n",
      "  grad :  tensor([-0.0252,  0.1424])\n",
      "Epoch 1790, Loss 2.989025\n",
      "  params :  tensor([  5.2201, -16.4691])\n",
      "  grad :  tensor([-0.0251,  0.1422])\n",
      "Epoch 1791, Loss 2.988817\n",
      "  params :  tensor([  5.2203, -16.4705])\n",
      "  grad :  tensor([-0.0251,  0.1420])\n",
      "Epoch 1792, Loss 2.988609\n",
      "  params :  tensor([  5.2206, -16.4719])\n",
      "  grad :  tensor([-0.0250,  0.1417])\n",
      "Epoch 1793, Loss 2.988401\n",
      "  params :  tensor([  5.2208, -16.4733])\n",
      "  grad :  tensor([-0.0250,  0.1415])\n",
      "Epoch 1794, Loss 2.988195\n",
      "  params :  tensor([  5.2211, -16.4748])\n",
      "  grad :  tensor([-0.0249,  0.1412])\n",
      "Epoch 1795, Loss 2.987989\n",
      "  params :  tensor([  5.2213, -16.4762])\n",
      "  grad :  tensor([-0.0249,  0.1410])\n",
      "Epoch 1796, Loss 2.987785\n",
      "  params :  tensor([  5.2216, -16.4776])\n",
      "  grad :  tensor([-0.0249,  0.1408])\n",
      "Epoch 1797, Loss 2.987582\n",
      "  params :  tensor([  5.2218, -16.4790])\n",
      "  grad :  tensor([-0.0248,  0.1405])\n",
      "Epoch 1798, Loss 2.987377\n",
      "  params :  tensor([  5.2221, -16.4804])\n",
      "  grad :  tensor([-0.0248,  0.1403])\n",
      "Epoch 1799, Loss 2.987174\n",
      "  params :  tensor([  5.2223, -16.4818])\n",
      "  grad :  tensor([-0.0247,  0.1400])\n",
      "Epoch 1800, Loss 2.986974\n",
      "  params :  tensor([  5.2226, -16.4832])\n",
      "  grad :  tensor([-0.0247,  0.1398])\n",
      "Epoch 1801, Loss 2.986771\n",
      "  params :  tensor([  5.2228, -16.4846])\n",
      "  grad :  tensor([-0.0246,  0.1396])\n",
      "Epoch 1802, Loss 2.986570\n",
      "  params :  tensor([  5.2231, -16.4860])\n",
      "  grad :  tensor([-0.0246,  0.1393])\n",
      "Epoch 1803, Loss 2.986371\n",
      "  params :  tensor([  5.2233, -16.4874])\n",
      "  grad :  tensor([-0.0246,  0.1391])\n",
      "Epoch 1804, Loss 2.986171\n",
      "  params :  tensor([  5.2236, -16.4888])\n",
      "  grad :  tensor([-0.0245,  0.1389])\n",
      "Epoch 1805, Loss 2.985972\n",
      "  params :  tensor([  5.2238, -16.4901])\n",
      "  grad :  tensor([-0.0245,  0.1386])\n",
      "Epoch 1806, Loss 2.985774\n",
      "  params :  tensor([  5.2241, -16.4915])\n",
      "  grad :  tensor([-0.0245,  0.1384])\n",
      "Epoch 1807, Loss 2.985578\n",
      "  params :  tensor([  5.2243, -16.4929])\n",
      "  grad :  tensor([-0.0244,  0.1382])\n",
      "Epoch 1808, Loss 2.985381\n",
      "  params :  tensor([  5.2245, -16.4943])\n",
      "  grad :  tensor([-0.0244,  0.1379])\n",
      "Epoch 1809, Loss 2.985184\n",
      "  params :  tensor([  5.2248, -16.4957])\n",
      "  grad :  tensor([-0.0243,  0.1377])\n",
      "Epoch 1810, Loss 2.984989\n",
      "  params :  tensor([  5.2250, -16.4970])\n",
      "  grad :  tensor([-0.0243,  0.1374])\n",
      "Epoch 1811, Loss 2.984793\n",
      "  params :  tensor([  5.2253, -16.4984])\n",
      "  grad :  tensor([-0.0243,  0.1372])\n",
      "Epoch 1812, Loss 2.984601\n",
      "  params :  tensor([  5.2255, -16.4998])\n",
      "  grad :  tensor([-0.0242,  0.1370])\n",
      "Epoch 1813, Loss 2.984407\n",
      "  params :  tensor([  5.2258, -16.5011])\n",
      "  grad :  tensor([-0.0242,  0.1368])\n",
      "Epoch 1814, Loss 2.984215\n",
      "  params :  tensor([  5.2260, -16.5025])\n",
      "  grad :  tensor([-0.0241,  0.1365])\n",
      "Epoch 1815, Loss 2.984022\n",
      "  params :  tensor([  5.2262, -16.5039])\n",
      "  grad :  tensor([-0.0241,  0.1363])\n",
      "Epoch 1816, Loss 2.983831\n",
      "  params :  tensor([  5.2265, -16.5052])\n",
      "  grad :  tensor([-0.0240,  0.1361])\n",
      "Epoch 1817, Loss 2.983639\n",
      "  params :  tensor([  5.2267, -16.5066])\n",
      "  grad :  tensor([-0.0240,  0.1358])\n",
      "Epoch 1818, Loss 2.983449\n",
      "  params :  tensor([  5.2270, -16.5079])\n",
      "  grad :  tensor([-0.0239,  0.1356])\n",
      "Epoch 1819, Loss 2.983259\n",
      "  params :  tensor([  5.2272, -16.5093])\n",
      "  grad :  tensor([-0.0239,  0.1354])\n",
      "Epoch 1820, Loss 2.983073\n",
      "  params :  tensor([  5.2274, -16.5107])\n",
      "  grad :  tensor([-0.0239,  0.1351])\n",
      "Epoch 1821, Loss 2.982884\n",
      "  params :  tensor([  5.2277, -16.5120])\n",
      "  grad :  tensor([-0.0238,  0.1349])\n",
      "Epoch 1822, Loss 2.982697\n",
      "  params :  tensor([  5.2279, -16.5133])\n",
      "  grad :  tensor([-0.0238,  0.1347])\n",
      "Epoch 1823, Loss 2.982510\n",
      "  params :  tensor([  5.2281, -16.5147])\n",
      "  grad :  tensor([-0.0237,  0.1344])\n",
      "Epoch 1824, Loss 2.982322\n",
      "  params :  tensor([  5.2284, -16.5160])\n",
      "  grad :  tensor([-0.0237,  0.1342])\n",
      "Epoch 1825, Loss 2.982137\n",
      "  params :  tensor([  5.2286, -16.5174])\n",
      "  grad :  tensor([-0.0237,  0.1340])\n",
      "Epoch 1826, Loss 2.981953\n",
      "  params :  tensor([  5.2289, -16.5187])\n",
      "  grad :  tensor([-0.0236,  0.1338])\n",
      "Epoch 1827, Loss 2.981769\n",
      "  params :  tensor([  5.2291, -16.5200])\n",
      "  grad :  tensor([-0.0236,  0.1335])\n",
      "Epoch 1828, Loss 2.981586\n",
      "  params :  tensor([  5.2293, -16.5214])\n",
      "  grad :  tensor([-0.0236,  0.1333])\n",
      "Epoch 1829, Loss 2.981402\n",
      "  params :  tensor([  5.2296, -16.5227])\n",
      "  grad :  tensor([-0.0235,  0.1331])\n",
      "Epoch 1830, Loss 2.981219\n",
      "  params :  tensor([  5.2298, -16.5240])\n",
      "  grad :  tensor([-0.0235,  0.1329])\n",
      "Epoch 1831, Loss 2.981037\n",
      "  params :  tensor([  5.2300, -16.5254])\n",
      "  grad :  tensor([-0.0235,  0.1326])\n",
      "Epoch 1832, Loss 2.980856\n",
      "  params :  tensor([  5.2303, -16.5267])\n",
      "  grad :  tensor([-0.0234,  0.1324])\n",
      "Epoch 1833, Loss 2.980675\n",
      "  params :  tensor([  5.2305, -16.5280])\n",
      "  grad :  tensor([-0.0234,  0.1322])\n",
      "Epoch 1834, Loss 2.980495\n",
      "  params :  tensor([  5.2307, -16.5293])\n",
      "  grad :  tensor([-0.0233,  0.1320])\n",
      "Epoch 1835, Loss 2.980315\n",
      "  params :  tensor([  5.2310, -16.5306])\n",
      "  grad :  tensor([-0.0233,  0.1317])\n",
      "Epoch 1836, Loss 2.980137\n",
      "  params :  tensor([  5.2312, -16.5320])\n",
      "  grad :  tensor([-0.0232,  0.1315])\n",
      "Epoch 1837, Loss 2.979958\n",
      "  params :  tensor([  5.2314, -16.5333])\n",
      "  grad :  tensor([-0.0232,  0.1313])\n",
      "Epoch 1838, Loss 2.979782\n",
      "  params :  tensor([  5.2317, -16.5346])\n",
      "  grad :  tensor([-0.0232,  0.1311])\n",
      "Epoch 1839, Loss 2.979604\n",
      "  params :  tensor([  5.2319, -16.5359])\n",
      "  grad :  tensor([-0.0231,  0.1308])\n",
      "Epoch 1840, Loss 2.979428\n",
      "  params :  tensor([  5.2321, -16.5372])\n",
      "  grad :  tensor([-0.0231,  0.1306])\n",
      "Epoch 1841, Loss 2.979253\n",
      "  params :  tensor([  5.2324, -16.5385])\n",
      "  grad :  tensor([-0.0230,  0.1304])\n",
      "Epoch 1842, Loss 2.979078\n",
      "  params :  tensor([  5.2326, -16.5398])\n",
      "  grad :  tensor([-0.0230,  0.1302])\n",
      "Epoch 1843, Loss 2.978902\n",
      "  params :  tensor([  5.2328, -16.5411])\n",
      "  grad :  tensor([-0.0229,  0.1300])\n",
      "Epoch 1844, Loss 2.978729\n",
      "  params :  tensor([  5.2330, -16.5424])\n",
      "  grad :  tensor([-0.0229,  0.1297])\n",
      "Epoch 1845, Loss 2.978556\n",
      "  params :  tensor([  5.2333, -16.5437])\n",
      "  grad :  tensor([-0.0229,  0.1295])\n",
      "Epoch 1846, Loss 2.978382\n",
      "  params :  tensor([  5.2335, -16.5450])\n",
      "  grad :  tensor([-0.0228,  0.1293])\n",
      "Epoch 1847, Loss 2.978211\n",
      "  params :  tensor([  5.2337, -16.5463])\n",
      "  grad :  tensor([-0.0228,  0.1291])\n",
      "Epoch 1848, Loss 2.978039\n",
      "  params :  tensor([  5.2340, -16.5476])\n",
      "  grad :  tensor([-0.0228,  0.1288])\n",
      "Epoch 1849, Loss 2.977867\n",
      "  params :  tensor([  5.2342, -16.5489])\n",
      "  grad :  tensor([-0.0227,  0.1286])\n",
      "Epoch 1850, Loss 2.977696\n",
      "  params :  tensor([  5.2344, -16.5501])\n",
      "  grad :  tensor([-0.0227,  0.1284])\n",
      "Epoch 1851, Loss 2.977527\n",
      "  params :  tensor([  5.2346, -16.5514])\n",
      "  grad :  tensor([-0.0227,  0.1282])\n",
      "Epoch 1852, Loss 2.977357\n",
      "  params :  tensor([  5.2349, -16.5527])\n",
      "  grad :  tensor([-0.0226,  0.1280])\n",
      "Epoch 1853, Loss 2.977188\n",
      "  params :  tensor([  5.2351, -16.5540])\n",
      "  grad :  tensor([-0.0226,  0.1278])\n",
      "Epoch 1854, Loss 2.977021\n",
      "  params :  tensor([  5.2353, -16.5553])\n",
      "  grad :  tensor([-0.0225,  0.1275])\n",
      "Epoch 1855, Loss 2.976853\n",
      "  params :  tensor([  5.2355, -16.5565])\n",
      "  grad :  tensor([-0.0225,  0.1273])\n",
      "Epoch 1856, Loss 2.976687\n",
      "  params :  tensor([  5.2358, -16.5578])\n",
      "  grad :  tensor([-0.0225,  0.1271])\n",
      "Epoch 1857, Loss 2.976520\n",
      "  params :  tensor([  5.2360, -16.5591])\n",
      "  grad :  tensor([-0.0224,  0.1269])\n",
      "Epoch 1858, Loss 2.976354\n",
      "  params :  tensor([  5.2362, -16.5603])\n",
      "  grad :  tensor([-0.0224,  0.1267])\n",
      "Epoch 1859, Loss 2.976189\n",
      "  params :  tensor([  5.2364, -16.5616])\n",
      "  grad :  tensor([-0.0223,  0.1265])\n",
      "Epoch 1860, Loss 2.976023\n",
      "  params :  tensor([  5.2367, -16.5629])\n",
      "  grad :  tensor([-0.0223,  0.1263])\n",
      "Epoch 1861, Loss 2.975860\n",
      "  params :  tensor([  5.2369, -16.5641])\n",
      "  grad :  tensor([-0.0223,  0.1260])\n",
      "Epoch 1862, Loss 2.975697\n",
      "  params :  tensor([  5.2371, -16.5654])\n",
      "  grad :  tensor([-0.0222,  0.1258])\n",
      "Epoch 1863, Loss 2.975533\n",
      "  params :  tensor([  5.2373, -16.5666])\n",
      "  grad :  tensor([-0.0222,  0.1256])\n",
      "Epoch 1864, Loss 2.975369\n",
      "  params :  tensor([  5.2375, -16.5679])\n",
      "  grad :  tensor([-0.0222,  0.1254])\n",
      "Epoch 1865, Loss 2.975208\n",
      "  params :  tensor([  5.2378, -16.5691])\n",
      "  grad :  tensor([-0.0221,  0.1252])\n",
      "Epoch 1866, Loss 2.975046\n",
      "  params :  tensor([  5.2380, -16.5704])\n",
      "  grad :  tensor([-0.0221,  0.1250])\n",
      "Epoch 1867, Loss 2.974886\n",
      "  params :  tensor([  5.2382, -16.5716])\n",
      "  grad :  tensor([-0.0220,  0.1248])\n",
      "Epoch 1868, Loss 2.974725\n",
      "  params :  tensor([  5.2384, -16.5729])\n",
      "  grad :  tensor([-0.0220,  0.1245])\n",
      "Epoch 1869, Loss 2.974565\n",
      "  params :  tensor([  5.2386, -16.5741])\n",
      "  grad :  tensor([-0.0220,  0.1243])\n",
      "Epoch 1870, Loss 2.974406\n",
      "  params :  tensor([  5.2389, -16.5754])\n",
      "  grad :  tensor([-0.0219,  0.1241])\n",
      "Epoch 1871, Loss 2.974248\n",
      "  params :  tensor([  5.2391, -16.5766])\n",
      "  grad :  tensor([-0.0219,  0.1239])\n",
      "Epoch 1872, Loss 2.974088\n",
      "  params :  tensor([  5.2393, -16.5778])\n",
      "  grad :  tensor([-0.0219,  0.1237])\n",
      "Epoch 1873, Loss 2.973930\n",
      "  params :  tensor([  5.2395, -16.5791])\n",
      "  grad :  tensor([-0.0218,  0.1235])\n",
      "Epoch 1874, Loss 2.973776\n",
      "  params :  tensor([  5.2397, -16.5803])\n",
      "  grad :  tensor([-0.0218,  0.1233])\n",
      "Epoch 1875, Loss 2.973618\n",
      "  params :  tensor([  5.2400, -16.5815])\n",
      "  grad :  tensor([-0.0217,  0.1231])\n",
      "Epoch 1876, Loss 2.973463\n",
      "  params :  tensor([  5.2402, -16.5828])\n",
      "  grad :  tensor([-0.0217,  0.1229])\n",
      "Epoch 1877, Loss 2.973307\n",
      "  params :  tensor([  5.2404, -16.5840])\n",
      "  grad :  tensor([-0.0217,  0.1227])\n",
      "Epoch 1878, Loss 2.973151\n",
      "  params :  tensor([  5.2406, -16.5852])\n",
      "  grad :  tensor([-0.0216,  0.1224])\n",
      "Epoch 1879, Loss 2.972996\n",
      "  params :  tensor([  5.2408, -16.5864])\n",
      "  grad :  tensor([-0.0216,  0.1222])\n",
      "Epoch 1880, Loss 2.972843\n",
      "  params :  tensor([  5.2410, -16.5877])\n",
      "  grad :  tensor([-0.0215,  0.1220])\n",
      "Epoch 1881, Loss 2.972690\n",
      "  params :  tensor([  5.2413, -16.5889])\n",
      "  grad :  tensor([-0.0215,  0.1218])\n",
      "Epoch 1882, Loss 2.972536\n",
      "  params :  tensor([  5.2415, -16.5901])\n",
      "  grad :  tensor([-0.0215,  0.1216])\n",
      "Epoch 1883, Loss 2.972383\n",
      "  params :  tensor([  5.2417, -16.5913])\n",
      "  grad :  tensor([-0.0214,  0.1214])\n",
      "Epoch 1884, Loss 2.972232\n",
      "  params :  tensor([  5.2419, -16.5925])\n",
      "  grad :  tensor([-0.0214,  0.1212])\n",
      "Epoch 1885, Loss 2.972081\n",
      "  params :  tensor([  5.2421, -16.5937])\n",
      "  grad :  tensor([-0.0214,  0.1210])\n",
      "Epoch 1886, Loss 2.971931\n",
      "  params :  tensor([  5.2423, -16.5949])\n",
      "  grad :  tensor([-0.0213,  0.1208])\n",
      "Epoch 1887, Loss 2.971780\n",
      "  params :  tensor([  5.2425, -16.5961])\n",
      "  grad :  tensor([-0.0213,  0.1206])\n",
      "Epoch 1888, Loss 2.971630\n",
      "  params :  tensor([  5.2427, -16.5974])\n",
      "  grad :  tensor([-0.0213,  0.1204])\n",
      "Epoch 1889, Loss 2.971481\n",
      "  params :  tensor([  5.2430, -16.5986])\n",
      "  grad :  tensor([-0.0212,  0.1202])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1890, Loss 2.971332\n",
      "  params :  tensor([  5.2432, -16.5998])\n",
      "  grad :  tensor([-0.0212,  0.1200])\n",
      "Epoch 1891, Loss 2.971184\n",
      "  params :  tensor([  5.2434, -16.6010])\n",
      "  grad :  tensor([-0.0212,  0.1198])\n",
      "Epoch 1892, Loss 2.971035\n",
      "  params :  tensor([  5.2436, -16.6021])\n",
      "  grad :  tensor([-0.0211,  0.1196])\n",
      "Epoch 1893, Loss 2.970888\n",
      "  params :  tensor([  5.2438, -16.6033])\n",
      "  grad :  tensor([-0.0211,  0.1194])\n",
      "Epoch 1894, Loss 2.970741\n",
      "  params :  tensor([  5.2440, -16.6045])\n",
      "  grad :  tensor([-0.0211,  0.1192])\n",
      "Epoch 1895, Loss 2.970596\n",
      "  params :  tensor([  5.2442, -16.6057])\n",
      "  grad :  tensor([-0.0210,  0.1190])\n",
      "Epoch 1896, Loss 2.970449\n",
      "  params :  tensor([  5.2444, -16.6069])\n",
      "  grad :  tensor([-0.0210,  0.1188])\n",
      "Epoch 1897, Loss 2.970304\n",
      "  params :  tensor([  5.2446, -16.6081])\n",
      "  grad :  tensor([-0.0209,  0.1186])\n",
      "Epoch 1898, Loss 2.970159\n",
      "  params :  tensor([  5.2449, -16.6093])\n",
      "  grad :  tensor([-0.0209,  0.1183])\n",
      "Epoch 1899, Loss 2.970016\n",
      "  params :  tensor([  5.2451, -16.6105])\n",
      "  grad :  tensor([-0.0209,  0.1182])\n",
      "Epoch 1900, Loss 2.969871\n",
      "  params :  tensor([  5.2453, -16.6116])\n",
      "  grad :  tensor([-0.0208,  0.1180])\n",
      "Epoch 1901, Loss 2.969727\n",
      "  params :  tensor([  5.2455, -16.6128])\n",
      "  grad :  tensor([-0.0208,  0.1178])\n",
      "Epoch 1902, Loss 2.969586\n",
      "  params :  tensor([  5.2457, -16.6140])\n",
      "  grad :  tensor([-0.0208,  0.1175])\n",
      "Epoch 1903, Loss 2.969443\n",
      "  params :  tensor([  5.2459, -16.6152])\n",
      "  grad :  tensor([-0.0207,  0.1173])\n",
      "Epoch 1904, Loss 2.969302\n",
      "  params :  tensor([  5.2461, -16.6163])\n",
      "  grad :  tensor([-0.0207,  0.1172])\n",
      "Epoch 1905, Loss 2.969160\n",
      "  params :  tensor([  5.2463, -16.6175])\n",
      "  grad :  tensor([-0.0206,  0.1170])\n",
      "Epoch 1906, Loss 2.969017\n",
      "  params :  tensor([  5.2465, -16.6187])\n",
      "  grad :  tensor([-0.0206,  0.1168])\n",
      "Epoch 1907, Loss 2.968879\n",
      "  params :  tensor([  5.2467, -16.6198])\n",
      "  grad :  tensor([-0.0206,  0.1166])\n",
      "Epoch 1908, Loss 2.968739\n",
      "  params :  tensor([  5.2469, -16.6210])\n",
      "  grad :  tensor([-0.0205,  0.1164])\n",
      "Epoch 1909, Loss 2.968599\n",
      "  params :  tensor([  5.2471, -16.6222])\n",
      "  grad :  tensor([-0.0205,  0.1162])\n",
      "Epoch 1910, Loss 2.968460\n",
      "  params :  tensor([  5.2473, -16.6233])\n",
      "  grad :  tensor([-0.0205,  0.1160])\n",
      "Epoch 1911, Loss 2.968321\n",
      "  params :  tensor([  5.2475, -16.6245])\n",
      "  grad :  tensor([-0.0204,  0.1158])\n",
      "Epoch 1912, Loss 2.968183\n",
      "  params :  tensor([  5.2477, -16.6256])\n",
      "  grad :  tensor([-0.0204,  0.1156])\n",
      "Epoch 1913, Loss 2.968046\n",
      "  params :  tensor([  5.2479, -16.6268])\n",
      "  grad :  tensor([-0.0204,  0.1154])\n",
      "Epoch 1914, Loss 2.967908\n",
      "  params :  tensor([  5.2482, -16.6279])\n",
      "  grad :  tensor([-0.0204,  0.1152])\n",
      "Epoch 1915, Loss 2.967772\n",
      "  params :  tensor([  5.2484, -16.6291])\n",
      "  grad :  tensor([-0.0203,  0.1150])\n",
      "Epoch 1916, Loss 2.967636\n",
      "  params :  tensor([  5.2486, -16.6302])\n",
      "  grad :  tensor([-0.0203,  0.1148])\n",
      "Epoch 1917, Loss 2.967499\n",
      "  params :  tensor([  5.2488, -16.6314])\n",
      "  grad :  tensor([-0.0202,  0.1146])\n",
      "Epoch 1918, Loss 2.967365\n",
      "  params :  tensor([  5.2490, -16.6325])\n",
      "  grad :  tensor([-0.0202,  0.1144])\n",
      "Epoch 1919, Loss 2.967230\n",
      "  params :  tensor([  5.2492, -16.6337])\n",
      "  grad :  tensor([-0.0202,  0.1142])\n",
      "Epoch 1920, Loss 2.967095\n",
      "  params :  tensor([  5.2494, -16.6348])\n",
      "  grad :  tensor([-0.0202,  0.1140])\n",
      "Epoch 1921, Loss 2.966961\n",
      "  params :  tensor([  5.2496, -16.6360])\n",
      "  grad :  tensor([-0.0201,  0.1138])\n",
      "Epoch 1922, Loss 2.966828\n",
      "  params :  tensor([  5.2498, -16.6371])\n",
      "  grad :  tensor([-0.0201,  0.1136])\n",
      "Epoch 1923, Loss 2.966693\n",
      "  params :  tensor([  5.2500, -16.6382])\n",
      "  grad :  tensor([-0.0200,  0.1134])\n",
      "Epoch 1924, Loss 2.966561\n",
      "  params :  tensor([  5.2502, -16.6394])\n",
      "  grad :  tensor([-0.0200,  0.1132])\n",
      "Epoch 1925, Loss 2.966429\n",
      "  params :  tensor([  5.2504, -16.6405])\n",
      "  grad :  tensor([-0.0200,  0.1130])\n",
      "Epoch 1926, Loss 2.966297\n",
      "  params :  tensor([  5.2506, -16.6416])\n",
      "  grad :  tensor([-0.0199,  0.1128])\n",
      "Epoch 1927, Loss 2.966168\n",
      "  params :  tensor([  5.2508, -16.6427])\n",
      "  grad :  tensor([-0.0199,  0.1127])\n",
      "Epoch 1928, Loss 2.966036\n",
      "  params :  tensor([  5.2510, -16.6439])\n",
      "  grad :  tensor([-0.0199,  0.1125])\n",
      "Epoch 1929, Loss 2.965904\n",
      "  params :  tensor([  5.2512, -16.6450])\n",
      "  grad :  tensor([-0.0198,  0.1123])\n",
      "Epoch 1930, Loss 2.965777\n",
      "  params :  tensor([  5.2514, -16.6461])\n",
      "  grad :  tensor([-0.0198,  0.1121])\n",
      "Epoch 1931, Loss 2.965647\n",
      "  params :  tensor([  5.2516, -16.6472])\n",
      "  grad :  tensor([-0.0198,  0.1119])\n",
      "Epoch 1932, Loss 2.965516\n",
      "  params :  tensor([  5.2518, -16.6484])\n",
      "  grad :  tensor([-0.0197,  0.1117])\n",
      "Epoch 1933, Loss 2.965388\n",
      "  params :  tensor([  5.2520, -16.6495])\n",
      "  grad :  tensor([-0.0197,  0.1115])\n",
      "Epoch 1934, Loss 2.965261\n",
      "  params :  tensor([  5.2522, -16.6506])\n",
      "  grad :  tensor([-0.0197,  0.1113])\n",
      "Epoch 1935, Loss 2.965131\n",
      "  params :  tensor([  5.2523, -16.6517])\n",
      "  grad :  tensor([-0.0196,  0.1111])\n",
      "Epoch 1936, Loss 2.965006\n",
      "  params :  tensor([  5.2525, -16.6528])\n",
      "  grad :  tensor([-0.0196,  0.1109])\n",
      "Epoch 1937, Loss 2.964877\n",
      "  params :  tensor([  5.2527, -16.6539])\n",
      "  grad :  tensor([-0.0196,  0.1108])\n",
      "Epoch 1938, Loss 2.964751\n",
      "  params :  tensor([  5.2529, -16.6550])\n",
      "  grad :  tensor([-0.0195,  0.1106])\n",
      "Epoch 1939, Loss 2.964625\n",
      "  params :  tensor([  5.2531, -16.6561])\n",
      "  grad :  tensor([-0.0195,  0.1104])\n",
      "Epoch 1940, Loss 2.964500\n",
      "  params :  tensor([  5.2533, -16.6572])\n",
      "  grad :  tensor([-0.0195,  0.1102])\n",
      "Epoch 1941, Loss 2.964375\n",
      "  params :  tensor([  5.2535, -16.6583])\n",
      "  grad :  tensor([-0.0195,  0.1100])\n",
      "Epoch 1942, Loss 2.964250\n",
      "  params :  tensor([  5.2537, -16.6594])\n",
      "  grad :  tensor([-0.0194,  0.1098])\n",
      "Epoch 1943, Loss 2.964126\n",
      "  params :  tensor([  5.2539, -16.6605])\n",
      "  grad :  tensor([-0.0194,  0.1096])\n",
      "Epoch 1944, Loss 2.964001\n",
      "  params :  tensor([  5.2541, -16.6616])\n",
      "  grad :  tensor([-0.0194,  0.1094])\n",
      "Epoch 1945, Loss 2.963879\n",
      "  params :  tensor([  5.2543, -16.6627])\n",
      "  grad :  tensor([-0.0193,  0.1093])\n",
      "Epoch 1946, Loss 2.963756\n",
      "  params :  tensor([  5.2545, -16.6638])\n",
      "  grad :  tensor([-0.0193,  0.1091])\n",
      "Epoch 1947, Loss 2.963632\n",
      "  params :  tensor([  5.2547, -16.6649])\n",
      "  grad :  tensor([-0.0192,  0.1089])\n",
      "Epoch 1948, Loss 2.963511\n",
      "  params :  tensor([  5.2549, -16.6660])\n",
      "  grad :  tensor([-0.0192,  0.1087])\n",
      "Epoch 1949, Loss 2.963388\n",
      "  params :  tensor([  5.2551, -16.6671])\n",
      "  grad :  tensor([-0.0192,  0.1085])\n",
      "Epoch 1950, Loss 2.963266\n",
      "  params :  tensor([  5.2553, -16.6681])\n",
      "  grad :  tensor([-0.0191,  0.1083])\n",
      "Epoch 1951, Loss 2.963149\n",
      "  params :  tensor([  5.2554, -16.6692])\n",
      "  grad :  tensor([-0.0191,  0.1081])\n",
      "Epoch 1952, Loss 2.963026\n",
      "  params :  tensor([  5.2556, -16.6703])\n",
      "  grad :  tensor([-0.0191,  0.1080])\n",
      "Epoch 1953, Loss 2.962907\n",
      "  params :  tensor([  5.2558, -16.6714])\n",
      "  grad :  tensor([-0.0190,  0.1078])\n",
      "Epoch 1954, Loss 2.962788\n",
      "  params :  tensor([  5.2560, -16.6725])\n",
      "  grad :  tensor([-0.0190,  0.1076])\n",
      "Epoch 1955, Loss 2.962667\n",
      "  params :  tensor([  5.2562, -16.6735])\n",
      "  grad :  tensor([-0.0190,  0.1074])\n",
      "Epoch 1956, Loss 2.962547\n",
      "  params :  tensor([  5.2564, -16.6746])\n",
      "  grad :  tensor([-0.0189,  0.1072])\n",
      "Epoch 1957, Loss 2.962429\n",
      "  params :  tensor([  5.2566, -16.6757])\n",
      "  grad :  tensor([-0.0189,  0.1071])\n",
      "Epoch 1958, Loss 2.962312\n",
      "  params :  tensor([  5.2568, -16.6767])\n",
      "  grad :  tensor([-0.0189,  0.1069])\n",
      "Epoch 1959, Loss 2.962195\n",
      "  params :  tensor([  5.2570, -16.6778])\n",
      "  grad :  tensor([-0.0188,  0.1067])\n",
      "Epoch 1960, Loss 2.962078\n",
      "  params :  tensor([  5.2572, -16.6789])\n",
      "  grad :  tensor([-0.0188,  0.1065])\n",
      "Epoch 1961, Loss 2.961959\n",
      "  params :  tensor([  5.2573, -16.6799])\n",
      "  grad :  tensor([-0.0188,  0.1063])\n",
      "Epoch 1962, Loss 2.961843\n",
      "  params :  tensor([  5.2575, -16.6810])\n",
      "  grad :  tensor([-0.0187,  0.1062])\n",
      "Epoch 1963, Loss 2.961728\n",
      "  params :  tensor([  5.2577, -16.6821])\n",
      "  grad :  tensor([-0.0187,  0.1060])\n",
      "Epoch 1964, Loss 2.961611\n",
      "  params :  tensor([  5.2579, -16.6831])\n",
      "  grad :  tensor([-0.0187,  0.1058])\n",
      "Epoch 1965, Loss 2.961496\n",
      "  params :  tensor([  5.2581, -16.6842])\n",
      "  grad :  tensor([-0.0187,  0.1056])\n",
      "Epoch 1966, Loss 2.961382\n",
      "  params :  tensor([  5.2583, -16.6852])\n",
      "  grad :  tensor([-0.0186,  0.1054])\n",
      "Epoch 1967, Loss 2.961267\n",
      "  params :  tensor([  5.2585, -16.6863])\n",
      "  grad :  tensor([-0.0186,  0.1052])\n",
      "Epoch 1968, Loss 2.961153\n",
      "  params :  tensor([  5.2586, -16.6873])\n",
      "  grad :  tensor([-0.0186,  0.1051])\n",
      "Epoch 1969, Loss 2.961038\n",
      "  params :  tensor([  5.2588, -16.6884])\n",
      "  grad :  tensor([-0.0185,  0.1049])\n",
      "Epoch 1970, Loss 2.960926\n",
      "  params :  tensor([  5.2590, -16.6894])\n",
      "  grad :  tensor([-0.0185,  0.1047])\n",
      "Epoch 1971, Loss 2.960813\n",
      "  params :  tensor([  5.2592, -16.6905])\n",
      "  grad :  tensor([-0.0185,  0.1045])\n",
      "Epoch 1972, Loss 2.960700\n",
      "  params :  tensor([  5.2594, -16.6915])\n",
      "  grad :  tensor([-0.0184,  0.1044])\n",
      "Epoch 1973, Loss 2.960587\n",
      "  params :  tensor([  5.2596, -16.6926])\n",
      "  grad :  tensor([-0.0184,  0.1042])\n",
      "Epoch 1974, Loss 2.960475\n",
      "  params :  tensor([  5.2598, -16.6936])\n",
      "  grad :  tensor([-0.0184,  0.1040])\n",
      "Epoch 1975, Loss 2.960365\n",
      "  params :  tensor([  5.2599, -16.6946])\n",
      "  grad :  tensor([-0.0183,  0.1038])\n",
      "Epoch 1976, Loss 2.960255\n",
      "  params :  tensor([  5.2601, -16.6957])\n",
      "  grad :  tensor([-0.0183,  0.1037])\n",
      "Epoch 1977, Loss 2.960143\n",
      "  params :  tensor([  5.2603, -16.6967])\n",
      "  grad :  tensor([-0.0183,  0.1035])\n",
      "Epoch 1978, Loss 2.960033\n",
      "  params :  tensor([  5.2605, -16.6977])\n",
      "  grad :  tensor([-0.0182,  0.1033])\n",
      "Epoch 1979, Loss 2.959923\n",
      "  params :  tensor([  5.2607, -16.6988])\n",
      "  grad :  tensor([-0.0182,  0.1031])\n",
      "Epoch 1980, Loss 2.959812\n",
      "  params :  tensor([  5.2608, -16.6998])\n",
      "  grad :  tensor([-0.0182,  0.1029])\n",
      "Epoch 1981, Loss 2.959703\n",
      "  params :  tensor([  5.2610, -16.7008])\n",
      "  grad :  tensor([-0.0182,  0.1028])\n",
      "Epoch 1982, Loss 2.959594\n",
      "  params :  tensor([  5.2612, -16.7019])\n",
      "  grad :  tensor([-0.0181,  0.1026])\n",
      "Epoch 1983, Loss 2.959486\n",
      "  params :  tensor([  5.2614, -16.7029])\n",
      "  grad :  tensor([-0.0181,  0.1024])\n",
      "Epoch 1984, Loss 2.959378\n",
      "  params :  tensor([  5.2616, -16.7039])\n",
      "  grad :  tensor([-0.0181,  0.1022])\n",
      "Epoch 1985, Loss 2.959271\n",
      "  params :  tensor([  5.2618, -16.7049])\n",
      "  grad :  tensor([-0.0180,  0.1021])\n",
      "Epoch 1986, Loss 2.959162\n",
      "  params :  tensor([  5.2619, -16.7059])\n",
      "  grad :  tensor([-0.0180,  0.1019])\n",
      "Epoch 1987, Loss 2.959055\n",
      "  params :  tensor([  5.2621, -16.7070])\n",
      "  grad :  tensor([-0.0180,  0.1017])\n",
      "Epoch 1988, Loss 2.958950\n",
      "  params :  tensor([  5.2623, -16.7080])\n",
      "  grad :  tensor([-0.0179,  0.1016])\n",
      "Epoch 1989, Loss 2.958842\n",
      "  params :  tensor([  5.2625, -16.7090])\n",
      "  grad :  tensor([-0.0179,  0.1014])\n",
      "Epoch 1990, Loss 2.958738\n",
      "  params :  tensor([  5.2626, -16.7100])\n",
      "  grad :  tensor([-0.0179,  0.1012])\n",
      "Epoch 1991, Loss 2.958632\n",
      "  params :  tensor([  5.2628, -16.7110])\n",
      "  grad :  tensor([-0.0179,  0.1010])\n",
      "Epoch 1992, Loss 2.958526\n",
      "  params :  tensor([  5.2630, -16.7120])\n",
      "  grad :  tensor([-0.0178,  0.1009])\n",
      "Epoch 1993, Loss 2.958422\n",
      "  params :  tensor([  5.2632, -16.7130])\n",
      "  grad :  tensor([-0.0178,  0.1007])\n",
      "Epoch 1994, Loss 2.958317\n",
      "  params :  tensor([  5.2634, -16.7140])\n",
      "  grad :  tensor([-0.0178,  0.1005])\n",
      "Epoch 1995, Loss 2.958212\n",
      "  params :  tensor([  5.2635, -16.7150])\n",
      "  grad :  tensor([-0.0177,  0.1004])\n",
      "Epoch 1996, Loss 2.958109\n",
      "  params :  tensor([  5.2637, -16.7160])\n",
      "  grad :  tensor([-0.0177,  0.1002])\n",
      "Epoch 1997, Loss 2.958006\n",
      "  params :  tensor([  5.2639, -16.7170])\n",
      "  grad :  tensor([-0.0176,  0.1000])\n",
      "Epoch 1998, Loss 2.957904\n",
      "  params :  tensor([  5.2641, -16.7180])\n",
      "  grad :  tensor([-0.0176,  0.0998])\n",
      "Epoch 1999, Loss 2.957801\n",
      "  params :  tensor([  5.2642, -16.7190])\n",
      "  grad :  tensor([-0.0176,  0.0997])\n",
      "Epoch 2000, Loss 2.957698\n",
      "  params :  tensor([  5.2644, -16.7200])\n",
      "  grad :  tensor([-0.0176,  0.0995])\n",
      "Epoch 2001, Loss 2.957596\n",
      "  params :  tensor([  5.2646, -16.7210])\n",
      "  grad :  tensor([-0.0176,  0.0993])\n",
      "Epoch 2002, Loss 2.957494\n",
      "  params :  tensor([  5.2648, -16.7220])\n",
      "  grad :  tensor([-0.0175,  0.0992])\n",
      "Epoch 2003, Loss 2.957393\n",
      "  params :  tensor([  5.2649, -16.7230])\n",
      "  grad :  tensor([-0.0175,  0.0990])\n",
      "Epoch 2004, Loss 2.957292\n",
      "  params :  tensor([  5.2651, -16.7240])\n",
      "  grad :  tensor([-0.0174,  0.0988])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2005, Loss 2.957193\n",
      "  params :  tensor([  5.2653, -16.7250])\n",
      "  grad :  tensor([-0.0174,  0.0987])\n",
      "Epoch 2006, Loss 2.957091\n",
      "  params :  tensor([  5.2655, -16.7260])\n",
      "  grad :  tensor([-0.0174,  0.0985])\n",
      "Epoch 2007, Loss 2.956992\n",
      "  params :  tensor([  5.2656, -16.7269])\n",
      "  grad :  tensor([-0.0174,  0.0983])\n",
      "Epoch 2008, Loss 2.956892\n",
      "  params :  tensor([  5.2658, -16.7279])\n",
      "  grad :  tensor([-0.0173,  0.0982])\n",
      "Epoch 2009, Loss 2.956792\n",
      "  params :  tensor([  5.2660, -16.7289])\n",
      "  grad :  tensor([-0.0173,  0.0980])\n",
      "Epoch 2010, Loss 2.956694\n",
      "  params :  tensor([  5.2662, -16.7299])\n",
      "  grad :  tensor([-0.0173,  0.0978])\n",
      "Epoch 2011, Loss 2.956595\n",
      "  params :  tensor([  5.2663, -16.7309])\n",
      "  grad :  tensor([-0.0172,  0.0977])\n",
      "Epoch 2012, Loss 2.956496\n",
      "  params :  tensor([  5.2665, -16.7318])\n",
      "  grad :  tensor([-0.0172,  0.0975])\n",
      "Epoch 2013, Loss 2.956397\n",
      "  params :  tensor([  5.2667, -16.7328])\n",
      "  grad :  tensor([-0.0172,  0.0973])\n",
      "Epoch 2014, Loss 2.956300\n",
      "  params :  tensor([  5.2668, -16.7338])\n",
      "  grad :  tensor([-0.0172,  0.0972])\n",
      "Epoch 2015, Loss 2.956204\n",
      "  params :  tensor([  5.2670, -16.7348])\n",
      "  grad :  tensor([-0.0171,  0.0970])\n",
      "Epoch 2016, Loss 2.956108\n",
      "  params :  tensor([  5.2672, -16.7357])\n",
      "  grad :  tensor([-0.0171,  0.0968])\n",
      "Epoch 2017, Loss 2.956010\n",
      "  params :  tensor([  5.2674, -16.7367])\n",
      "  grad :  tensor([-0.0171,  0.0967])\n",
      "Epoch 2018, Loss 2.955914\n",
      "  params :  tensor([  5.2675, -16.7377])\n",
      "  grad :  tensor([-0.0171,  0.0965])\n",
      "Epoch 2019, Loss 2.955817\n",
      "  params :  tensor([  5.2677, -16.7386])\n",
      "  grad :  tensor([-0.0170,  0.0963])\n",
      "Epoch 2020, Loss 2.955722\n",
      "  params :  tensor([  5.2679, -16.7396])\n",
      "  grad :  tensor([-0.0170,  0.0962])\n",
      "Epoch 2021, Loss 2.955627\n",
      "  params :  tensor([  5.2680, -16.7405])\n",
      "  grad :  tensor([-0.0170,  0.0960])\n",
      "Epoch 2022, Loss 2.955533\n",
      "  params :  tensor([  5.2682, -16.7415])\n",
      "  grad :  tensor([-0.0169,  0.0959])\n",
      "Epoch 2023, Loss 2.955436\n",
      "  params :  tensor([  5.2684, -16.7425])\n",
      "  grad :  tensor([-0.0169,  0.0957])\n",
      "Epoch 2024, Loss 2.955343\n",
      "  params :  tensor([  5.2686, -16.7434])\n",
      "  grad :  tensor([-0.0169,  0.0955])\n",
      "Epoch 2025, Loss 2.955250\n",
      "  params :  tensor([  5.2687, -16.7444])\n",
      "  grad :  tensor([-0.0169,  0.0954])\n",
      "Epoch 2026, Loss 2.955154\n",
      "  params :  tensor([  5.2689, -16.7453])\n",
      "  grad :  tensor([-0.0168,  0.0952])\n",
      "Epoch 2027, Loss 2.955062\n",
      "  params :  tensor([  5.2691, -16.7463])\n",
      "  grad :  tensor([-0.0168,  0.0950])\n",
      "Epoch 2028, Loss 2.954969\n",
      "  params :  tensor([  5.2692, -16.7472])\n",
      "  grad :  tensor([-0.0168,  0.0949])\n",
      "Epoch 2029, Loss 2.954875\n",
      "  params :  tensor([  5.2694, -16.7482])\n",
      "  grad :  tensor([-0.0167,  0.0947])\n",
      "Epoch 2030, Loss 2.954783\n",
      "  params :  tensor([  5.2696, -16.7491])\n",
      "  grad :  tensor([-0.0167,  0.0946])\n",
      "Epoch 2031, Loss 2.954691\n",
      "  params :  tensor([  5.2697, -16.7501])\n",
      "  grad :  tensor([-0.0167,  0.0944])\n",
      "Epoch 2032, Loss 2.954600\n",
      "  params :  tensor([  5.2699, -16.7510])\n",
      "  grad :  tensor([-0.0167,  0.0942])\n",
      "Epoch 2033, Loss 2.954507\n",
      "  params :  tensor([  5.2701, -16.7519])\n",
      "  grad :  tensor([-0.0166,  0.0941])\n",
      "Epoch 2034, Loss 2.954417\n",
      "  params :  tensor([  5.2702, -16.7529])\n",
      "  grad :  tensor([-0.0166,  0.0939])\n",
      "Epoch 2035, Loss 2.954326\n",
      "  params :  tensor([  5.2704, -16.7538])\n",
      "  grad :  tensor([-0.0165,  0.0938])\n",
      "Epoch 2036, Loss 2.954235\n",
      "  params :  tensor([  5.2706, -16.7547])\n",
      "  grad :  tensor([-0.0165,  0.0936])\n",
      "Epoch 2037, Loss 2.954145\n",
      "  params :  tensor([  5.2707, -16.7557])\n",
      "  grad :  tensor([-0.0165,  0.0934])\n",
      "Epoch 2038, Loss 2.954055\n",
      "  params :  tensor([  5.2709, -16.7566])\n",
      "  grad :  tensor([-0.0165,  0.0933])\n",
      "Epoch 2039, Loss 2.953966\n",
      "  params :  tensor([  5.2710, -16.7575])\n",
      "  grad :  tensor([-0.0164,  0.0931])\n",
      "Epoch 2040, Loss 2.953876\n",
      "  params :  tensor([  5.2712, -16.7585])\n",
      "  grad :  tensor([-0.0164,  0.0930])\n",
      "Epoch 2041, Loss 2.953787\n",
      "  params :  tensor([  5.2714, -16.7594])\n",
      "  grad :  tensor([-0.0164,  0.0928])\n",
      "Epoch 2042, Loss 2.953698\n",
      "  params :  tensor([  5.2715, -16.7603])\n",
      "  grad :  tensor([-0.0164,  0.0926])\n",
      "Epoch 2043, Loss 2.953610\n",
      "  params :  tensor([  5.2717, -16.7613])\n",
      "  grad :  tensor([-0.0163,  0.0925])\n",
      "Epoch 2044, Loss 2.953521\n",
      "  params :  tensor([  5.2719, -16.7622])\n",
      "  grad :  tensor([-0.0163,  0.0923])\n",
      "Epoch 2045, Loss 2.953434\n",
      "  params :  tensor([  5.2720, -16.7631])\n",
      "  grad :  tensor([-0.0163,  0.0922])\n",
      "Epoch 2046, Loss 2.953346\n",
      "  params :  tensor([  5.2722, -16.7640])\n",
      "  grad :  tensor([-0.0163,  0.0920])\n",
      "Epoch 2047, Loss 2.953259\n",
      "  params :  tensor([  5.2724, -16.7649])\n",
      "  grad :  tensor([-0.0162,  0.0919])\n",
      "Epoch 2048, Loss 2.953171\n",
      "  params :  tensor([  5.2725, -16.7659])\n",
      "  grad :  tensor([-0.0162,  0.0917])\n",
      "Epoch 2049, Loss 2.953085\n",
      "  params :  tensor([  5.2727, -16.7668])\n",
      "  grad :  tensor([-0.0162,  0.0915])\n",
      "Epoch 2050, Loss 2.953000\n",
      "  params :  tensor([  5.2728, -16.7677])\n",
      "  grad :  tensor([-0.0162,  0.0914])\n",
      "Epoch 2051, Loss 2.952913\n",
      "  params :  tensor([  5.2730, -16.7686])\n",
      "  grad :  tensor([-0.0161,  0.0912])\n",
      "Epoch 2052, Loss 2.952828\n",
      "  params :  tensor([  5.2732, -16.7695])\n",
      "  grad :  tensor([-0.0161,  0.0911])\n",
      "Epoch 2053, Loss 2.952742\n",
      "  params :  tensor([  5.2733, -16.7704])\n",
      "  grad :  tensor([-0.0161,  0.0909])\n",
      "Epoch 2054, Loss 2.952657\n",
      "  params :  tensor([  5.2735, -16.7713])\n",
      "  grad :  tensor([-0.0160,  0.0908])\n",
      "Epoch 2055, Loss 2.952571\n",
      "  params :  tensor([  5.2736, -16.7722])\n",
      "  grad :  tensor([-0.0160,  0.0906])\n",
      "Epoch 2056, Loss 2.952487\n",
      "  params :  tensor([  5.2738, -16.7731])\n",
      "  grad :  tensor([-0.0160,  0.0905])\n",
      "Epoch 2057, Loss 2.952403\n",
      "  params :  tensor([  5.2740, -16.7740])\n",
      "  grad :  tensor([-0.0160,  0.0903])\n",
      "Epoch 2058, Loss 2.952318\n",
      "  params :  tensor([  5.2741, -16.7749])\n",
      "  grad :  tensor([-0.0159,  0.0902])\n",
      "Epoch 2059, Loss 2.952235\n",
      "  params :  tensor([  5.2743, -16.7758])\n",
      "  grad :  tensor([-0.0159,  0.0900])\n",
      "Epoch 2060, Loss 2.952152\n",
      "  params :  tensor([  5.2744, -16.7767])\n",
      "  grad :  tensor([-0.0159,  0.0899])\n",
      "Epoch 2061, Loss 2.952068\n",
      "  params :  tensor([  5.2746, -16.7776])\n",
      "  grad :  tensor([-0.0158,  0.0897])\n",
      "Epoch 2062, Loss 2.951985\n",
      "  params :  tensor([  5.2748, -16.7785])\n",
      "  grad :  tensor([-0.0158,  0.0895])\n",
      "Epoch 2063, Loss 2.951902\n",
      "  params :  tensor([  5.2749, -16.7794])\n",
      "  grad :  tensor([-0.0158,  0.0894])\n",
      "Epoch 2064, Loss 2.951820\n",
      "  params :  tensor([  5.2751, -16.7803])\n",
      "  grad :  tensor([-0.0158,  0.0892])\n",
      "Epoch 2065, Loss 2.951738\n",
      "  params :  tensor([  5.2752, -16.7812])\n",
      "  grad :  tensor([-0.0157,  0.0891])\n",
      "Epoch 2066, Loss 2.951656\n",
      "  params :  tensor([  5.2754, -16.7821])\n",
      "  grad :  tensor([-0.0157,  0.0889])\n",
      "Epoch 2067, Loss 2.951576\n",
      "  params :  tensor([  5.2755, -16.7830])\n",
      "  grad :  tensor([-0.0157,  0.0888])\n",
      "Epoch 2068, Loss 2.951494\n",
      "  params :  tensor([  5.2757, -16.7839])\n",
      "  grad :  tensor([-0.0157,  0.0886])\n",
      "Epoch 2069, Loss 2.951413\n",
      "  params :  tensor([  5.2759, -16.7848])\n",
      "  grad :  tensor([-0.0157,  0.0885])\n",
      "Epoch 2070, Loss 2.951333\n",
      "  params :  tensor([  5.2760, -16.7856])\n",
      "  grad :  tensor([-0.0156,  0.0883])\n",
      "Epoch 2071, Loss 2.951252\n",
      "  params :  tensor([  5.2762, -16.7865])\n",
      "  grad :  tensor([-0.0156,  0.0882])\n",
      "Epoch 2072, Loss 2.951171\n",
      "  params :  tensor([  5.2763, -16.7874])\n",
      "  grad :  tensor([-0.0155,  0.0880])\n",
      "Epoch 2073, Loss 2.951093\n",
      "  params :  tensor([  5.2765, -16.7883])\n",
      "  grad :  tensor([-0.0155,  0.0879])\n",
      "Epoch 2074, Loss 2.951012\n",
      "  params :  tensor([  5.2766, -16.7892])\n",
      "  grad :  tensor([-0.0155,  0.0877])\n",
      "Epoch 2075, Loss 2.950932\n",
      "  params :  tensor([  5.2768, -16.7900])\n",
      "  grad :  tensor([-0.0155,  0.0876])\n",
      "Epoch 2076, Loss 2.950853\n",
      "  params :  tensor([  5.2769, -16.7909])\n",
      "  grad :  tensor([-0.0154,  0.0874])\n",
      "Epoch 2077, Loss 2.950774\n",
      "  params :  tensor([  5.2771, -16.7918])\n",
      "  grad :  tensor([-0.0154,  0.0873])\n",
      "Epoch 2078, Loss 2.950697\n",
      "  params :  tensor([  5.2772, -16.7927])\n",
      "  grad :  tensor([-0.0154,  0.0871])\n",
      "Epoch 2079, Loss 2.950618\n",
      "  params :  tensor([  5.2774, -16.7935])\n",
      "  grad :  tensor([-0.0154,  0.0870])\n",
      "Epoch 2080, Loss 2.950540\n",
      "  params :  tensor([  5.2776, -16.7944])\n",
      "  grad :  tensor([-0.0154,  0.0868])\n",
      "Epoch 2081, Loss 2.950463\n",
      "  params :  tensor([  5.2777, -16.7953])\n",
      "  grad :  tensor([-0.0153,  0.0867])\n",
      "Epoch 2082, Loss 2.950385\n",
      "  params :  tensor([  5.2779, -16.7961])\n",
      "  grad :  tensor([-0.0153,  0.0866])\n",
      "Epoch 2083, Loss 2.950308\n",
      "  params :  tensor([  5.2780, -16.7970])\n",
      "  grad :  tensor([-0.0153,  0.0864])\n",
      "Epoch 2084, Loss 2.950231\n",
      "  params :  tensor([  5.2782, -16.7979])\n",
      "  grad :  tensor([-0.0152,  0.0863])\n",
      "Epoch 2085, Loss 2.950154\n",
      "  params :  tensor([  5.2783, -16.7987])\n",
      "  grad :  tensor([-0.0152,  0.0861])\n",
      "Epoch 2086, Loss 2.950078\n",
      "  params :  tensor([  5.2785, -16.7996])\n",
      "  grad :  tensor([-0.0152,  0.0860])\n",
      "Epoch 2087, Loss 2.950003\n",
      "  params :  tensor([  5.2786, -16.8004])\n",
      "  grad :  tensor([-0.0152,  0.0858])\n",
      "Epoch 2088, Loss 2.949925\n",
      "  params :  tensor([  5.2788, -16.8013])\n",
      "  grad :  tensor([-0.0152,  0.0857])\n",
      "Epoch 2089, Loss 2.949850\n",
      "  params :  tensor([  5.2789, -16.8021])\n",
      "  grad :  tensor([-0.0151,  0.0855])\n",
      "Epoch 2090, Loss 2.949776\n",
      "  params :  tensor([  5.2791, -16.8030])\n",
      "  grad :  tensor([-0.0151,  0.0854])\n",
      "Epoch 2091, Loss 2.949699\n",
      "  params :  tensor([  5.2792, -16.8039])\n",
      "  grad :  tensor([-0.0151,  0.0852])\n",
      "Epoch 2092, Loss 2.949626\n",
      "  params :  tensor([  5.2794, -16.8047])\n",
      "  grad :  tensor([-0.0150,  0.0851])\n",
      "Epoch 2093, Loss 2.949550\n",
      "  params :  tensor([  5.2795, -16.8056])\n",
      "  grad :  tensor([-0.0150,  0.0850])\n",
      "Epoch 2094, Loss 2.949476\n",
      "  params :  tensor([  5.2797, -16.8064])\n",
      "  grad :  tensor([-0.0150,  0.0848])\n",
      "Epoch 2095, Loss 2.949401\n",
      "  params :  tensor([  5.2798, -16.8072])\n",
      "  grad :  tensor([-0.0149,  0.0847])\n",
      "Epoch 2096, Loss 2.949328\n",
      "  params :  tensor([  5.2800, -16.8081])\n",
      "  grad :  tensor([-0.0150,  0.0845])\n",
      "Epoch 2097, Loss 2.949254\n",
      "  params :  tensor([  5.2801, -16.8089])\n",
      "  grad :  tensor([-0.0149,  0.0844])\n",
      "Epoch 2098, Loss 2.949182\n",
      "  params :  tensor([  5.2803, -16.8098])\n",
      "  grad :  tensor([-0.0149,  0.0842])\n",
      "Epoch 2099, Loss 2.949108\n",
      "  params :  tensor([  5.2804, -16.8106])\n",
      "  grad :  tensor([-0.0149,  0.0841])\n",
      "Epoch 2100, Loss 2.949035\n",
      "  params :  tensor([  5.2806, -16.8115])\n",
      "  grad :  tensor([-0.0148,  0.0839])\n",
      "Epoch 2101, Loss 2.948962\n",
      "  params :  tensor([  5.2807, -16.8123])\n",
      "  grad :  tensor([-0.0148,  0.0838])\n",
      "Epoch 2102, Loss 2.948890\n",
      "  params :  tensor([  5.2809, -16.8131])\n",
      "  grad :  tensor([-0.0148,  0.0837])\n",
      "Epoch 2103, Loss 2.948818\n",
      "  params :  tensor([  5.2810, -16.8140])\n",
      "  grad :  tensor([-0.0148,  0.0835])\n",
      "Epoch 2104, Loss 2.948745\n",
      "  params :  tensor([  5.2812, -16.8148])\n",
      "  grad :  tensor([-0.0148,  0.0834])\n",
      "Epoch 2105, Loss 2.948675\n",
      "  params :  tensor([  5.2813, -16.8156])\n",
      "  grad :  tensor([-0.0147,  0.0832])\n",
      "Epoch 2106, Loss 2.948602\n",
      "  params :  tensor([  5.2815, -16.8165])\n",
      "  grad :  tensor([-0.0147,  0.0831])\n",
      "Epoch 2107, Loss 2.948532\n",
      "  params :  tensor([  5.2816, -16.8173])\n",
      "  grad :  tensor([-0.0146,  0.0830])\n",
      "Epoch 2108, Loss 2.948462\n",
      "  params :  tensor([  5.2817, -16.8181])\n",
      "  grad :  tensor([-0.0146,  0.0828])\n",
      "Epoch 2109, Loss 2.948391\n",
      "  params :  tensor([  5.2819, -16.8189])\n",
      "  grad :  tensor([-0.0146,  0.0827])\n",
      "Epoch 2110, Loss 2.948321\n",
      "  params :  tensor([  5.2820, -16.8198])\n",
      "  grad :  tensor([-0.0146,  0.0825])\n",
      "Epoch 2111, Loss 2.948250\n",
      "  params :  tensor([  5.2822, -16.8206])\n",
      "  grad :  tensor([-0.0145,  0.0824])\n",
      "Epoch 2112, Loss 2.948181\n",
      "  params :  tensor([  5.2823, -16.8214])\n",
      "  grad :  tensor([-0.0145,  0.0823])\n",
      "Epoch 2113, Loss 2.948109\n",
      "  params :  tensor([  5.2825, -16.8222])\n",
      "  grad :  tensor([-0.0145,  0.0821])\n",
      "Epoch 2114, Loss 2.948041\n",
      "  params :  tensor([  5.2826, -16.8231])\n",
      "  grad :  tensor([-0.0145,  0.0820])\n",
      "Epoch 2115, Loss 2.947971\n",
      "  params :  tensor([  5.2828, -16.8239])\n",
      "  grad :  tensor([-0.0144,  0.0818])\n",
      "Epoch 2116, Loss 2.947902\n",
      "  params :  tensor([  5.2829, -16.8247])\n",
      "  grad :  tensor([-0.0144,  0.0817])\n",
      "Epoch 2117, Loss 2.947833\n",
      "  params :  tensor([  5.2831, -16.8255])\n",
      "  grad :  tensor([-0.0144,  0.0816])\n",
      "Epoch 2118, Loss 2.947765\n",
      "  params :  tensor([  5.2832, -16.8263])\n",
      "  grad :  tensor([-0.0144,  0.0814])\n",
      "Epoch 2119, Loss 2.947696\n",
      "  params :  tensor([  5.2833, -16.8271])\n",
      "  grad :  tensor([-0.0144,  0.0813])\n",
      "Epoch 2120, Loss 2.947628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.2835, -16.8280])\n",
      "  grad :  tensor([-0.0143,  0.0811])\n",
      "Epoch 2121, Loss 2.947560\n",
      "  params :  tensor([  5.2836, -16.8288])\n",
      "  grad :  tensor([-0.0143,  0.0810])\n",
      "Epoch 2122, Loss 2.947494\n",
      "  params :  tensor([  5.2838, -16.8296])\n",
      "  grad :  tensor([-0.0143,  0.0809])\n",
      "Epoch 2123, Loss 2.947426\n",
      "  params :  tensor([  5.2839, -16.8304])\n",
      "  grad :  tensor([-0.0143,  0.0807])\n",
      "Epoch 2124, Loss 2.947357\n",
      "  params :  tensor([  5.2841, -16.8312])\n",
      "  grad :  tensor([-0.0142,  0.0806])\n",
      "Epoch 2125, Loss 2.947293\n",
      "  params :  tensor([  5.2842, -16.8320])\n",
      "  grad :  tensor([-0.0142,  0.0805])\n",
      "Epoch 2126, Loss 2.947225\n",
      "  params :  tensor([  5.2843, -16.8328])\n",
      "  grad :  tensor([-0.0142,  0.0803])\n",
      "Epoch 2127, Loss 2.947158\n",
      "  params :  tensor([  5.2845, -16.8336])\n",
      "  grad :  tensor([-0.0142,  0.0802])\n",
      "Epoch 2128, Loss 2.947092\n",
      "  params :  tensor([  5.2846, -16.8344])\n",
      "  grad :  tensor([-0.0141,  0.0800])\n",
      "Epoch 2129, Loss 2.947026\n",
      "  params :  tensor([  5.2848, -16.8352])\n",
      "  grad :  tensor([-0.0141,  0.0799])\n",
      "Epoch 2130, Loss 2.946960\n",
      "  params :  tensor([  5.2849, -16.8360])\n",
      "  grad :  tensor([-0.0141,  0.0798])\n",
      "Epoch 2131, Loss 2.946895\n",
      "  params :  tensor([  5.2850, -16.8368])\n",
      "  grad :  tensor([-0.0141,  0.0796])\n",
      "Epoch 2132, Loss 2.946830\n",
      "  params :  tensor([  5.2852, -16.8376])\n",
      "  grad :  tensor([-0.0141,  0.0795])\n",
      "Epoch 2133, Loss 2.946764\n",
      "  params :  tensor([  5.2853, -16.8384])\n",
      "  grad :  tensor([-0.0140,  0.0794])\n",
      "Epoch 2134, Loss 2.946700\n",
      "  params :  tensor([  5.2855, -16.8392])\n",
      "  grad :  tensor([-0.0140,  0.0792])\n",
      "Epoch 2135, Loss 2.946635\n",
      "  params :  tensor([  5.2856, -16.8400])\n",
      "  grad :  tensor([-0.0140,  0.0791])\n",
      "Epoch 2136, Loss 2.946571\n",
      "  params :  tensor([  5.2857, -16.8407])\n",
      "  grad :  tensor([-0.0139,  0.0790])\n",
      "Epoch 2137, Loss 2.946507\n",
      "  params :  tensor([  5.2859, -16.8415])\n",
      "  grad :  tensor([-0.0139,  0.0788])\n",
      "Epoch 2138, Loss 2.946442\n",
      "  params :  tensor([  5.2860, -16.8423])\n",
      "  grad :  tensor([-0.0139,  0.0787])\n",
      "Epoch 2139, Loss 2.946378\n",
      "  params :  tensor([  5.2862, -16.8431])\n",
      "  grad :  tensor([-0.0139,  0.0786])\n",
      "Epoch 2140, Loss 2.946314\n",
      "  params :  tensor([  5.2863, -16.8439])\n",
      "  grad :  tensor([-0.0138,  0.0784])\n",
      "Epoch 2141, Loss 2.946251\n",
      "  params :  tensor([  5.2864, -16.8447])\n",
      "  grad :  tensor([-0.0138,  0.0783])\n",
      "Epoch 2142, Loss 2.946189\n",
      "  params :  tensor([  5.2866, -16.8455])\n",
      "  grad :  tensor([-0.0138,  0.0782])\n",
      "Epoch 2143, Loss 2.946126\n",
      "  params :  tensor([  5.2867, -16.8462])\n",
      "  grad :  tensor([-0.0138,  0.0780])\n",
      "Epoch 2144, Loss 2.946063\n",
      "  params :  tensor([  5.2869, -16.8470])\n",
      "  grad :  tensor([-0.0138,  0.0779])\n",
      "Epoch 2145, Loss 2.946001\n",
      "  params :  tensor([  5.2870, -16.8478])\n",
      "  grad :  tensor([-0.0137,  0.0778])\n",
      "Epoch 2146, Loss 2.945937\n",
      "  params :  tensor([  5.2871, -16.8486])\n",
      "  grad :  tensor([-0.0137,  0.0776])\n",
      "Epoch 2147, Loss 2.945876\n",
      "  params :  tensor([  5.2873, -16.8493])\n",
      "  grad :  tensor([-0.0137,  0.0775])\n",
      "Epoch 2148, Loss 2.945815\n",
      "  params :  tensor([  5.2874, -16.8501])\n",
      "  grad :  tensor([-0.0137,  0.0774])\n",
      "Epoch 2149, Loss 2.945753\n",
      "  params :  tensor([  5.2875, -16.8509])\n",
      "  grad :  tensor([-0.0136,  0.0772])\n",
      "Epoch 2150, Loss 2.945690\n",
      "  params :  tensor([  5.2877, -16.8517])\n",
      "  grad :  tensor([-0.0136,  0.0771])\n",
      "Epoch 2151, Loss 2.945630\n",
      "  params :  tensor([  5.2878, -16.8524])\n",
      "  grad :  tensor([-0.0136,  0.0770])\n",
      "Epoch 2152, Loss 2.945567\n",
      "  params :  tensor([  5.2879, -16.8532])\n",
      "  grad :  tensor([-0.0136,  0.0768])\n",
      "Epoch 2153, Loss 2.945508\n",
      "  params :  tensor([  5.2881, -16.8540])\n",
      "  grad :  tensor([-0.0135,  0.0767])\n",
      "Epoch 2154, Loss 2.945447\n",
      "  params :  tensor([  5.2882, -16.8547])\n",
      "  grad :  tensor([-0.0135,  0.0766])\n",
      "Epoch 2155, Loss 2.945385\n",
      "  params :  tensor([  5.2884, -16.8555])\n",
      "  grad :  tensor([-0.0135,  0.0765])\n",
      "Epoch 2156, Loss 2.945325\n",
      "  params :  tensor([  5.2885, -16.8563])\n",
      "  grad :  tensor([-0.0135,  0.0763])\n",
      "Epoch 2157, Loss 2.945267\n",
      "  params :  tensor([  5.2886, -16.8570])\n",
      "  grad :  tensor([-0.0135,  0.0762])\n",
      "Epoch 2158, Loss 2.945206\n",
      "  params :  tensor([  5.2888, -16.8578])\n",
      "  grad :  tensor([-0.0134,  0.0761])\n",
      "Epoch 2159, Loss 2.945146\n",
      "  params :  tensor([  5.2889, -16.8585])\n",
      "  grad :  tensor([-0.0134,  0.0759])\n",
      "Epoch 2160, Loss 2.945088\n",
      "  params :  tensor([  5.2890, -16.8593])\n",
      "  grad :  tensor([-0.0134,  0.0758])\n",
      "Epoch 2161, Loss 2.945028\n",
      "  params :  tensor([  5.2892, -16.8601])\n",
      "  grad :  tensor([-0.0134,  0.0757])\n",
      "Epoch 2162, Loss 2.944969\n",
      "  params :  tensor([  5.2893, -16.8608])\n",
      "  grad :  tensor([-0.0133,  0.0755])\n",
      "Epoch 2163, Loss 2.944911\n",
      "  params :  tensor([  5.2894, -16.8616])\n",
      "  grad :  tensor([-0.0133,  0.0754])\n",
      "Epoch 2164, Loss 2.944852\n",
      "  params :  tensor([  5.2896, -16.8623])\n",
      "  grad :  tensor([-0.0133,  0.0753])\n",
      "Epoch 2165, Loss 2.944792\n",
      "  params :  tensor([  5.2897, -16.8631])\n",
      "  grad :  tensor([-0.0133,  0.0752])\n",
      "Epoch 2166, Loss 2.944736\n",
      "  params :  tensor([  5.2898, -16.8638])\n",
      "  grad :  tensor([-0.0133,  0.0750])\n",
      "Epoch 2167, Loss 2.944678\n",
      "  params :  tensor([  5.2900, -16.8646])\n",
      "  grad :  tensor([-0.0132,  0.0749])\n",
      "Epoch 2168, Loss 2.944619\n",
      "  params :  tensor([  5.2901, -16.8653])\n",
      "  grad :  tensor([-0.0132,  0.0748])\n",
      "Epoch 2169, Loss 2.944562\n",
      "  params :  tensor([  5.2902, -16.8661])\n",
      "  grad :  tensor([-0.0132,  0.0747])\n",
      "Epoch 2170, Loss 2.944504\n",
      "  params :  tensor([  5.2903, -16.8668])\n",
      "  grad :  tensor([-0.0132,  0.0745])\n",
      "Epoch 2171, Loss 2.944447\n",
      "  params :  tensor([  5.2905, -16.8676])\n",
      "  grad :  tensor([-0.0132,  0.0744])\n",
      "Epoch 2172, Loss 2.944391\n",
      "  params :  tensor([  5.2906, -16.8683])\n",
      "  grad :  tensor([-0.0131,  0.0743])\n",
      "Epoch 2173, Loss 2.944332\n",
      "  params :  tensor([  5.2907, -16.8690])\n",
      "  grad :  tensor([-0.0131,  0.0742])\n",
      "Epoch 2174, Loss 2.944276\n",
      "  params :  tensor([  5.2909, -16.8698])\n",
      "  grad :  tensor([-0.0131,  0.0740])\n",
      "Epoch 2175, Loss 2.944220\n",
      "  params :  tensor([  5.2910, -16.8705])\n",
      "  grad :  tensor([-0.0131,  0.0739])\n",
      "Epoch 2176, Loss 2.944164\n",
      "  params :  tensor([  5.2911, -16.8713])\n",
      "  grad :  tensor([-0.0130,  0.0738])\n",
      "Epoch 2177, Loss 2.944108\n",
      "  params :  tensor([  5.2913, -16.8720])\n",
      "  grad :  tensor([-0.0130,  0.0736])\n",
      "Epoch 2178, Loss 2.944053\n",
      "  params :  tensor([  5.2914, -16.8727])\n",
      "  grad :  tensor([-0.0130,  0.0735])\n",
      "Epoch 2179, Loss 2.943996\n",
      "  params :  tensor([  5.2915, -16.8735])\n",
      "  grad :  tensor([-0.0130,  0.0734])\n",
      "Epoch 2180, Loss 2.943941\n",
      "  params :  tensor([  5.2917, -16.8742])\n",
      "  grad :  tensor([-0.0129,  0.0733])\n",
      "Epoch 2181, Loss 2.943887\n",
      "  params :  tensor([  5.2918, -16.8749])\n",
      "  grad :  tensor([-0.0129,  0.0731])\n",
      "Epoch 2182, Loss 2.943831\n",
      "  params :  tensor([  5.2919, -16.8757])\n",
      "  grad :  tensor([-0.0129,  0.0730])\n",
      "Epoch 2183, Loss 2.943776\n",
      "  params :  tensor([  5.2920, -16.8764])\n",
      "  grad :  tensor([-0.0129,  0.0729])\n",
      "Epoch 2184, Loss 2.943721\n",
      "  params :  tensor([  5.2922, -16.8771])\n",
      "  grad :  tensor([-0.0129,  0.0728])\n",
      "Epoch 2185, Loss 2.943666\n",
      "  params :  tensor([  5.2923, -16.8778])\n",
      "  grad :  tensor([-0.0128,  0.0727])\n",
      "Epoch 2186, Loss 2.943613\n",
      "  params :  tensor([  5.2924, -16.8786])\n",
      "  grad :  tensor([-0.0128,  0.0725])\n",
      "Epoch 2187, Loss 2.943558\n",
      "  params :  tensor([  5.2926, -16.8793])\n",
      "  grad :  tensor([-0.0128,  0.0724])\n",
      "Epoch 2188, Loss 2.943503\n",
      "  params :  tensor([  5.2927, -16.8800])\n",
      "  grad :  tensor([-0.0128,  0.0723])\n",
      "Epoch 2189, Loss 2.943451\n",
      "  params :  tensor([  5.2928, -16.8807])\n",
      "  grad :  tensor([-0.0127,  0.0722])\n",
      "Epoch 2190, Loss 2.943395\n",
      "  params :  tensor([  5.2929, -16.8815])\n",
      "  grad :  tensor([-0.0127,  0.0720])\n",
      "Epoch 2191, Loss 2.943343\n",
      "  params :  tensor([  5.2931, -16.8822])\n",
      "  grad :  tensor([-0.0127,  0.0719])\n",
      "Epoch 2192, Loss 2.943290\n",
      "  params :  tensor([  5.2932, -16.8829])\n",
      "  grad :  tensor([-0.0127,  0.0718])\n",
      "Epoch 2193, Loss 2.943235\n",
      "  params :  tensor([  5.2933, -16.8836])\n",
      "  grad :  tensor([-0.0127,  0.0717])\n",
      "Epoch 2194, Loss 2.943183\n",
      "  params :  tensor([  5.2934, -16.8843])\n",
      "  grad :  tensor([-0.0126,  0.0715])\n",
      "Epoch 2195, Loss 2.943130\n",
      "  params :  tensor([  5.2936, -16.8850])\n",
      "  grad :  tensor([-0.0126,  0.0714])\n",
      "Epoch 2196, Loss 2.943079\n",
      "  params :  tensor([  5.2937, -16.8857])\n",
      "  grad :  tensor([-0.0126,  0.0713])\n",
      "Epoch 2197, Loss 2.943027\n",
      "  params :  tensor([  5.2938, -16.8865])\n",
      "  grad :  tensor([-0.0126,  0.0712])\n",
      "Epoch 2198, Loss 2.942973\n",
      "  params :  tensor([  5.2939, -16.8872])\n",
      "  grad :  tensor([-0.0126,  0.0711])\n",
      "Epoch 2199, Loss 2.942922\n",
      "  params :  tensor([  5.2941, -16.8879])\n",
      "  grad :  tensor([-0.0125,  0.0709])\n",
      "Epoch 2200, Loss 2.942870\n",
      "  params :  tensor([  5.2942, -16.8886])\n",
      "  grad :  tensor([-0.0125,  0.0708])\n",
      "Epoch 2201, Loss 2.942818\n",
      "  params :  tensor([  5.2943, -16.8893])\n",
      "  grad :  tensor([-0.0125,  0.0707])\n",
      "Epoch 2202, Loss 2.942766\n",
      "  params :  tensor([  5.2944, -16.8900])\n",
      "  grad :  tensor([-0.0125,  0.0706])\n",
      "Epoch 2203, Loss 2.942714\n",
      "  params :  tensor([  5.2946, -16.8907])\n",
      "  grad :  tensor([-0.0124,  0.0705])\n",
      "Epoch 2204, Loss 2.942665\n",
      "  params :  tensor([  5.2947, -16.8914])\n",
      "  grad :  tensor([-0.0124,  0.0703])\n",
      "Epoch 2205, Loss 2.942612\n",
      "  params :  tensor([  5.2948, -16.8921])\n",
      "  grad :  tensor([-0.0124,  0.0702])\n",
      "Epoch 2206, Loss 2.942564\n",
      "  params :  tensor([  5.2949, -16.8928])\n",
      "  grad :  tensor([-0.0124,  0.0701])\n",
      "Epoch 2207, Loss 2.942510\n",
      "  params :  tensor([  5.2951, -16.8935])\n",
      "  grad :  tensor([-0.0124,  0.0700])\n",
      "Epoch 2208, Loss 2.942461\n",
      "  params :  tensor([  5.2952, -16.8942])\n",
      "  grad :  tensor([-0.0123,  0.0699])\n",
      "Epoch 2209, Loss 2.942411\n",
      "  params :  tensor([  5.2953, -16.8949])\n",
      "  grad :  tensor([-0.0123,  0.0697])\n",
      "Epoch 2210, Loss 2.942361\n",
      "  params :  tensor([  5.2954, -16.8956])\n",
      "  grad :  tensor([-0.0123,  0.0696])\n",
      "Epoch 2211, Loss 2.942310\n",
      "  params :  tensor([  5.2956, -16.8963])\n",
      "  grad :  tensor([-0.0123,  0.0695])\n",
      "Epoch 2212, Loss 2.942261\n",
      "  params :  tensor([  5.2957, -16.8970])\n",
      "  grad :  tensor([-0.0122,  0.0694])\n",
      "Epoch 2213, Loss 2.942211\n",
      "  params :  tensor([  5.2958, -16.8977])\n",
      "  grad :  tensor([-0.0122,  0.0693])\n",
      "Epoch 2214, Loss 2.942162\n",
      "  params :  tensor([  5.2959, -16.8984])\n",
      "  grad :  tensor([-0.0122,  0.0692])\n",
      "Epoch 2215, Loss 2.942112\n",
      "  params :  tensor([  5.2960, -16.8991])\n",
      "  grad :  tensor([-0.0122,  0.0690])\n",
      "Epoch 2216, Loss 2.942062\n",
      "  params :  tensor([  5.2962, -16.8998])\n",
      "  grad :  tensor([-0.0122,  0.0689])\n",
      "Epoch 2217, Loss 2.942014\n",
      "  params :  tensor([  5.2963, -16.9004])\n",
      "  grad :  tensor([-0.0122,  0.0688])\n",
      "Epoch 2218, Loss 2.941965\n",
      "  params :  tensor([  5.2964, -16.9011])\n",
      "  grad :  tensor([-0.0121,  0.0687])\n",
      "Epoch 2219, Loss 2.941918\n",
      "  params :  tensor([  5.2965, -16.9018])\n",
      "  grad :  tensor([-0.0121,  0.0686])\n",
      "Epoch 2220, Loss 2.941868\n",
      "  params :  tensor([  5.2967, -16.9025])\n",
      "  grad :  tensor([-0.0121,  0.0685])\n",
      "Epoch 2221, Loss 2.941821\n",
      "  params :  tensor([  5.2968, -16.9032])\n",
      "  grad :  tensor([-0.0121,  0.0683])\n",
      "Epoch 2222, Loss 2.941773\n",
      "  params :  tensor([  5.2969, -16.9039])\n",
      "  grad :  tensor([-0.0120,  0.0682])\n",
      "Epoch 2223, Loss 2.941724\n",
      "  params :  tensor([  5.2970, -16.9046])\n",
      "  grad :  tensor([-0.0120,  0.0681])\n",
      "Epoch 2224, Loss 2.941677\n",
      "  params :  tensor([  5.2971, -16.9052])\n",
      "  grad :  tensor([-0.0120,  0.0680])\n",
      "Epoch 2225, Loss 2.941629\n",
      "  params :  tensor([  5.2973, -16.9059])\n",
      "  grad :  tensor([-0.0120,  0.0679])\n",
      "Epoch 2226, Loss 2.941582\n",
      "  params :  tensor([  5.2974, -16.9066])\n",
      "  grad :  tensor([-0.0120,  0.0678])\n",
      "Epoch 2227, Loss 2.941534\n",
      "  params :  tensor([  5.2975, -16.9073])\n",
      "  grad :  tensor([-0.0119,  0.0676])\n",
      "Epoch 2228, Loss 2.941488\n",
      "  params :  tensor([  5.2976, -16.9079])\n",
      "  grad :  tensor([-0.0119,  0.0675])\n",
      "Epoch 2229, Loss 2.941440\n",
      "  params :  tensor([  5.2977, -16.9086])\n",
      "  grad :  tensor([-0.0119,  0.0674])\n",
      "Epoch 2230, Loss 2.941393\n",
      "  params :  tensor([  5.2979, -16.9093])\n",
      "  grad :  tensor([-0.0119,  0.0673])\n",
      "Epoch 2231, Loss 2.941346\n",
      "  params :  tensor([  5.2980, -16.9100])\n",
      "  grad :  tensor([-0.0119,  0.0672])\n",
      "Epoch 2232, Loss 2.941299\n",
      "  params :  tensor([  5.2981, -16.9106])\n",
      "  grad :  tensor([-0.0118,  0.0671])\n",
      "Epoch 2233, Loss 2.941253\n",
      "  params :  tensor([  5.2982, -16.9113])\n",
      "  grad :  tensor([-0.0118,  0.0670])\n",
      "Epoch 2234, Loss 2.941206\n",
      "  params :  tensor([  5.2983, -16.9120])\n",
      "  grad :  tensor([-0.0118,  0.0668])\n",
      "Epoch 2235, Loss 2.941163\n",
      "  params :  tensor([  5.2984, -16.9126])\n",
      "  grad :  tensor([-0.0118,  0.0667])\n",
      "Epoch 2236, Loss 2.941116\n",
      "  params :  tensor([  5.2986, -16.9133])\n",
      "  grad :  tensor([-0.0118,  0.0666])\n",
      "Epoch 2237, Loss 2.941070\n",
      "  params :  tensor([  5.2987, -16.9140])\n",
      "  grad :  tensor([-0.0117,  0.0665])\n",
      "Epoch 2238, Loss 2.941025\n",
      "  params :  tensor([  5.2988, -16.9146])\n",
      "  grad :  tensor([-0.0117,  0.0664])\n",
      "Epoch 2239, Loss 2.940979\n",
      "  params :  tensor([  5.2989, -16.9153])\n",
      "  grad :  tensor([-0.0117,  0.0663])\n",
      "Epoch 2240, Loss 2.940933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.2990, -16.9160])\n",
      "  grad :  tensor([-0.0117,  0.0662])\n",
      "Epoch 2241, Loss 2.940890\n",
      "  params :  tensor([  5.2991, -16.9166])\n",
      "  grad :  tensor([-0.0117,  0.0661])\n",
      "Epoch 2242, Loss 2.940844\n",
      "  params :  tensor([  5.2993, -16.9173])\n",
      "  grad :  tensor([-0.0117,  0.0659])\n",
      "Epoch 2243, Loss 2.940798\n",
      "  params :  tensor([  5.2994, -16.9179])\n",
      "  grad :  tensor([-0.0116,  0.0658])\n",
      "Epoch 2244, Loss 2.940753\n",
      "  params :  tensor([  5.2995, -16.9186])\n",
      "  grad :  tensor([-0.0116,  0.0657])\n",
      "Epoch 2245, Loss 2.940711\n",
      "  params :  tensor([  5.2996, -16.9192])\n",
      "  grad :  tensor([-0.0116,  0.0656])\n",
      "Epoch 2246, Loss 2.940666\n",
      "  params :  tensor([  5.2997, -16.9199])\n",
      "  grad :  tensor([-0.0116,  0.0655])\n",
      "Epoch 2247, Loss 2.940621\n",
      "  params :  tensor([  5.2998, -16.9206])\n",
      "  grad :  tensor([-0.0115,  0.0654])\n",
      "Epoch 2248, Loss 2.940576\n",
      "  params :  tensor([  5.3000, -16.9212])\n",
      "  grad :  tensor([-0.0115,  0.0653])\n",
      "Epoch 2249, Loss 2.940533\n",
      "  params :  tensor([  5.3001, -16.9219])\n",
      "  grad :  tensor([-0.0115,  0.0652])\n",
      "Epoch 2250, Loss 2.940489\n",
      "  params :  tensor([  5.3002, -16.9225])\n",
      "  grad :  tensor([-0.0115,  0.0650])\n",
      "Epoch 2251, Loss 2.940446\n",
      "  params :  tensor([  5.3003, -16.9232])\n",
      "  grad :  tensor([-0.0115,  0.0649])\n",
      "Epoch 2252, Loss 2.940403\n",
      "  params :  tensor([  5.3004, -16.9238])\n",
      "  grad :  tensor([-0.0114,  0.0648])\n",
      "Epoch 2253, Loss 2.940358\n",
      "  params :  tensor([  5.3005, -16.9245])\n",
      "  grad :  tensor([-0.0114,  0.0647])\n",
      "Epoch 2254, Loss 2.940316\n",
      "  params :  tensor([  5.3006, -16.9251])\n",
      "  grad :  tensor([-0.0114,  0.0646])\n",
      "Epoch 2255, Loss 2.940274\n",
      "  params :  tensor([  5.3008, -16.9257])\n",
      "  grad :  tensor([-0.0114,  0.0645])\n",
      "Epoch 2256, Loss 2.940229\n",
      "  params :  tensor([  5.3009, -16.9264])\n",
      "  grad :  tensor([-0.0114,  0.0644])\n",
      "Epoch 2257, Loss 2.940188\n",
      "  params :  tensor([  5.3010, -16.9270])\n",
      "  grad :  tensor([-0.0114,  0.0643])\n",
      "Epoch 2258, Loss 2.940144\n",
      "  params :  tensor([  5.3011, -16.9277])\n",
      "  grad :  tensor([-0.0114,  0.0642])\n",
      "Epoch 2259, Loss 2.940102\n",
      "  params :  tensor([  5.3012, -16.9283])\n",
      "  grad :  tensor([-0.0113,  0.0641])\n",
      "Epoch 2260, Loss 2.940060\n",
      "  params :  tensor([  5.3013, -16.9290])\n",
      "  grad :  tensor([-0.0113,  0.0640])\n",
      "Epoch 2261, Loss 2.940018\n",
      "  params :  tensor([  5.3014, -16.9296])\n",
      "  grad :  tensor([-0.0113,  0.0638])\n",
      "Epoch 2262, Loss 2.939977\n",
      "  params :  tensor([  5.3016, -16.9302])\n",
      "  grad :  tensor([-0.0113,  0.0637])\n",
      "Epoch 2263, Loss 2.939934\n",
      "  params :  tensor([  5.3017, -16.9309])\n",
      "  grad :  tensor([-0.0112,  0.0636])\n",
      "Epoch 2264, Loss 2.939891\n",
      "  params :  tensor([  5.3018, -16.9315])\n",
      "  grad :  tensor([-0.0112,  0.0635])\n",
      "Epoch 2265, Loss 2.939851\n",
      "  params :  tensor([  5.3019, -16.9321])\n",
      "  grad :  tensor([-0.0112,  0.0634])\n",
      "Epoch 2266, Loss 2.939809\n",
      "  params :  tensor([  5.3020, -16.9328])\n",
      "  grad :  tensor([-0.0112,  0.0633])\n",
      "Epoch 2267, Loss 2.939770\n",
      "  params :  tensor([  5.3021, -16.9334])\n",
      "  grad :  tensor([-0.0112,  0.0632])\n",
      "Epoch 2268, Loss 2.939727\n",
      "  params :  tensor([  5.3022, -16.9340])\n",
      "  grad :  tensor([-0.0111,  0.0631])\n",
      "Epoch 2269, Loss 2.939686\n",
      "  params :  tensor([  5.3023, -16.9347])\n",
      "  grad :  tensor([-0.0111,  0.0630])\n",
      "Epoch 2270, Loss 2.939646\n",
      "  params :  tensor([  5.3024, -16.9353])\n",
      "  grad :  tensor([-0.0111,  0.0629])\n",
      "Epoch 2271, Loss 2.939605\n",
      "  params :  tensor([  5.3026, -16.9359])\n",
      "  grad :  tensor([-0.0111,  0.0628])\n",
      "Epoch 2272, Loss 2.939566\n",
      "  params :  tensor([  5.3027, -16.9365])\n",
      "  grad :  tensor([-0.0111,  0.0627])\n",
      "Epoch 2273, Loss 2.939522\n",
      "  params :  tensor([  5.3028, -16.9372])\n",
      "  grad :  tensor([-0.0111,  0.0626])\n",
      "Epoch 2274, Loss 2.939483\n",
      "  params :  tensor([  5.3029, -16.9378])\n",
      "  grad :  tensor([-0.0110,  0.0624])\n",
      "Epoch 2275, Loss 2.939443\n",
      "  params :  tensor([  5.3030, -16.9384])\n",
      "  grad :  tensor([-0.0110,  0.0623])\n",
      "Epoch 2276, Loss 2.939403\n",
      "  params :  tensor([  5.3031, -16.9390])\n",
      "  grad :  tensor([-0.0110,  0.0622])\n",
      "Epoch 2277, Loss 2.939361\n",
      "  params :  tensor([  5.3032, -16.9397])\n",
      "  grad :  tensor([-0.0110,  0.0621])\n",
      "Epoch 2278, Loss 2.939323\n",
      "  params :  tensor([  5.3033, -16.9403])\n",
      "  grad :  tensor([-0.0110,  0.0620])\n",
      "Epoch 2279, Loss 2.939282\n",
      "  params :  tensor([  5.3034, -16.9409])\n",
      "  grad :  tensor([-0.0109,  0.0619])\n",
      "Epoch 2280, Loss 2.939243\n",
      "  params :  tensor([  5.3035, -16.9415])\n",
      "  grad :  tensor([-0.0109,  0.0618])\n",
      "Epoch 2281, Loss 2.939205\n",
      "  params :  tensor([  5.3037, -16.9421])\n",
      "  grad :  tensor([-0.0109,  0.0617])\n",
      "Epoch 2282, Loss 2.939165\n",
      "  params :  tensor([  5.3038, -16.9428])\n",
      "  grad :  tensor([-0.0109,  0.0616])\n",
      "Epoch 2283, Loss 2.939127\n",
      "  params :  tensor([  5.3039, -16.9434])\n",
      "  grad :  tensor([-0.0109,  0.0615])\n",
      "Epoch 2284, Loss 2.939087\n",
      "  params :  tensor([  5.3040, -16.9440])\n",
      "  grad :  tensor([-0.0108,  0.0614])\n",
      "Epoch 2285, Loss 2.939049\n",
      "  params :  tensor([  5.3041, -16.9446])\n",
      "  grad :  tensor([-0.0108,  0.0613])\n",
      "Epoch 2286, Loss 2.939011\n",
      "  params :  tensor([  5.3042, -16.9452])\n",
      "  grad :  tensor([-0.0108,  0.0612])\n",
      "Epoch 2287, Loss 2.938971\n",
      "  params :  tensor([  5.3043, -16.9458])\n",
      "  grad :  tensor([-0.0108,  0.0611])\n",
      "Epoch 2288, Loss 2.938933\n",
      "  params :  tensor([  5.3044, -16.9464])\n",
      "  grad :  tensor([-0.0108,  0.0610])\n",
      "Epoch 2289, Loss 2.938893\n",
      "  params :  tensor([  5.3045, -16.9470])\n",
      "  grad :  tensor([-0.0108,  0.0609])\n",
      "Epoch 2290, Loss 2.938857\n",
      "  params :  tensor([  5.3046, -16.9476])\n",
      "  grad :  tensor([-0.0107,  0.0608])\n",
      "Epoch 2291, Loss 2.938820\n",
      "  params :  tensor([  5.3047, -16.9482])\n",
      "  grad :  tensor([-0.0107,  0.0607])\n",
      "Epoch 2292, Loss 2.938779\n",
      "  params :  tensor([  5.3048, -16.9489])\n",
      "  grad :  tensor([-0.0107,  0.0606])\n",
      "Epoch 2293, Loss 2.938743\n",
      "  params :  tensor([  5.3049, -16.9495])\n",
      "  grad :  tensor([-0.0107,  0.0605])\n",
      "Epoch 2294, Loss 2.938705\n",
      "  params :  tensor([  5.3051, -16.9501])\n",
      "  grad :  tensor([-0.0107,  0.0604])\n",
      "Epoch 2295, Loss 2.938667\n",
      "  params :  tensor([  5.3052, -16.9507])\n",
      "  grad :  tensor([-0.0106,  0.0603])\n",
      "Epoch 2296, Loss 2.938629\n",
      "  params :  tensor([  5.3053, -16.9513])\n",
      "  grad :  tensor([-0.0106,  0.0602])\n",
      "Epoch 2297, Loss 2.938593\n",
      "  params :  tensor([  5.3054, -16.9519])\n",
      "  grad :  tensor([-0.0106,  0.0601])\n",
      "Epoch 2298, Loss 2.938555\n",
      "  params :  tensor([  5.3055, -16.9525])\n",
      "  grad :  tensor([-0.0106,  0.0600])\n",
      "Epoch 2299, Loss 2.938519\n",
      "  params :  tensor([  5.3056, -16.9531])\n",
      "  grad :  tensor([-0.0106,  0.0598])\n",
      "Epoch 2300, Loss 2.938481\n",
      "  params :  tensor([  5.3057, -16.9537])\n",
      "  grad :  tensor([-0.0106,  0.0597])\n",
      "Epoch 2301, Loss 2.938444\n",
      "  params :  tensor([  5.3058, -16.9543])\n",
      "  grad :  tensor([-0.0105,  0.0596])\n",
      "Epoch 2302, Loss 2.938408\n",
      "  params :  tensor([  5.3059, -16.9549])\n",
      "  grad :  tensor([-0.0105,  0.0595])\n",
      "Epoch 2303, Loss 2.938371\n",
      "  params :  tensor([  5.3060, -16.9554])\n",
      "  grad :  tensor([-0.0105,  0.0594])\n",
      "Epoch 2304, Loss 2.938335\n",
      "  params :  tensor([  5.3061, -16.9560])\n",
      "  grad :  tensor([-0.0105,  0.0593])\n",
      "Epoch 2305, Loss 2.938299\n",
      "  params :  tensor([  5.3062, -16.9566])\n",
      "  grad :  tensor([-0.0105,  0.0592])\n",
      "Epoch 2306, Loss 2.938263\n",
      "  params :  tensor([  5.3063, -16.9572])\n",
      "  grad :  tensor([-0.0105,  0.0591])\n",
      "Epoch 2307, Loss 2.938227\n",
      "  params :  tensor([  5.3064, -16.9578])\n",
      "  grad :  tensor([-0.0104,  0.0590])\n",
      "Epoch 2308, Loss 2.938190\n",
      "  params :  tensor([  5.3065, -16.9584])\n",
      "  grad :  tensor([-0.0104,  0.0589])\n",
      "Epoch 2309, Loss 2.938155\n",
      "  params :  tensor([  5.3066, -16.9590])\n",
      "  grad :  tensor([-0.0104,  0.0588])\n",
      "Epoch 2310, Loss 2.938118\n",
      "  params :  tensor([  5.3067, -16.9596])\n",
      "  grad :  tensor([-0.0104,  0.0587])\n",
      "Epoch 2311, Loss 2.938084\n",
      "  params :  tensor([  5.3068, -16.9602])\n",
      "  grad :  tensor([-0.0104,  0.0586])\n",
      "Epoch 2312, Loss 2.938049\n",
      "  params :  tensor([  5.3069, -16.9608])\n",
      "  grad :  tensor([-0.0103,  0.0585])\n",
      "Epoch 2313, Loss 2.938014\n",
      "  params :  tensor([  5.3070, -16.9613])\n",
      "  grad :  tensor([-0.0103,  0.0584])\n",
      "Epoch 2314, Loss 2.937977\n",
      "  params :  tensor([  5.3072, -16.9619])\n",
      "  grad :  tensor([-0.0103,  0.0583])\n",
      "Epoch 2315, Loss 2.937943\n",
      "  params :  tensor([  5.3073, -16.9625])\n",
      "  grad :  tensor([-0.0103,  0.0582])\n",
      "Epoch 2316, Loss 2.937908\n",
      "  params :  tensor([  5.3074, -16.9631])\n",
      "  grad :  tensor([-0.0103,  0.0581])\n",
      "Epoch 2317, Loss 2.937872\n",
      "  params :  tensor([  5.3075, -16.9637])\n",
      "  grad :  tensor([-0.0103,  0.0580])\n",
      "Epoch 2318, Loss 2.937839\n",
      "  params :  tensor([  5.3076, -16.9642])\n",
      "  grad :  tensor([-0.0102,  0.0580])\n",
      "Epoch 2319, Loss 2.937804\n",
      "  params :  tensor([  5.3077, -16.9648])\n",
      "  grad :  tensor([-0.0102,  0.0578])\n",
      "Epoch 2320, Loss 2.937769\n",
      "  params :  tensor([  5.3078, -16.9654])\n",
      "  grad :  tensor([-0.0102,  0.0578])\n",
      "Epoch 2321, Loss 2.937734\n",
      "  params :  tensor([  5.3079, -16.9660])\n",
      "  grad :  tensor([-0.0102,  0.0577])\n",
      "Epoch 2322, Loss 2.937700\n",
      "  params :  tensor([  5.3080, -16.9666])\n",
      "  grad :  tensor([-0.0102,  0.0576])\n",
      "Epoch 2323, Loss 2.937665\n",
      "  params :  tensor([  5.3081, -16.9671])\n",
      "  grad :  tensor([-0.0102,  0.0575])\n",
      "Epoch 2324, Loss 2.937632\n",
      "  params :  tensor([  5.3082, -16.9677])\n",
      "  grad :  tensor([-0.0101,  0.0574])\n",
      "Epoch 2325, Loss 2.937598\n",
      "  params :  tensor([  5.3083, -16.9683])\n",
      "  grad :  tensor([-0.0101,  0.0573])\n",
      "Epoch 2326, Loss 2.937565\n",
      "  params :  tensor([  5.3084, -16.9688])\n",
      "  grad :  tensor([-0.0101,  0.0572])\n",
      "Epoch 2327, Loss 2.937531\n",
      "  params :  tensor([  5.3085, -16.9694])\n",
      "  grad :  tensor([-0.0101,  0.0571])\n",
      "Epoch 2328, Loss 2.937499\n",
      "  params :  tensor([  5.3086, -16.9700])\n",
      "  grad :  tensor([-0.0101,  0.0570])\n",
      "Epoch 2329, Loss 2.937465\n",
      "  params :  tensor([  5.3087, -16.9706])\n",
      "  grad :  tensor([-0.0101,  0.0569])\n",
      "Epoch 2330, Loss 2.937430\n",
      "  params :  tensor([  5.3088, -16.9711])\n",
      "  grad :  tensor([-0.0100,  0.0568])\n",
      "Epoch 2331, Loss 2.937398\n",
      "  params :  tensor([  5.3089, -16.9717])\n",
      "  grad :  tensor([-0.0100,  0.0567])\n",
      "Epoch 2332, Loss 2.937364\n",
      "  params :  tensor([  5.3090, -16.9723])\n",
      "  grad :  tensor([-0.0100,  0.0566])\n",
      "Epoch 2333, Loss 2.937332\n",
      "  params :  tensor([  5.3091, -16.9728])\n",
      "  grad :  tensor([-0.0100,  0.0565])\n",
      "Epoch 2334, Loss 2.937299\n",
      "  params :  tensor([  5.3092, -16.9734])\n",
      "  grad :  tensor([-0.0100,  0.0564])\n",
      "Epoch 2335, Loss 2.937265\n",
      "  params :  tensor([  5.3093, -16.9739])\n",
      "  grad :  tensor([-0.0100,  0.0563])\n",
      "Epoch 2336, Loss 2.937232\n",
      "  params :  tensor([  5.3094, -16.9745])\n",
      "  grad :  tensor([-0.0099,  0.0562])\n",
      "Epoch 2337, Loss 2.937201\n",
      "  params :  tensor([  5.3095, -16.9751])\n",
      "  grad :  tensor([-0.0099,  0.0561])\n",
      "Epoch 2338, Loss 2.937167\n",
      "  params :  tensor([  5.3096, -16.9756])\n",
      "  grad :  tensor([-0.0099,  0.0560])\n",
      "Epoch 2339, Loss 2.937134\n",
      "  params :  tensor([  5.3097, -16.9762])\n",
      "  grad :  tensor([-0.0099,  0.0559])\n",
      "Epoch 2340, Loss 2.937104\n",
      "  params :  tensor([  5.3098, -16.9767])\n",
      "  grad :  tensor([-0.0099,  0.0558])\n",
      "Epoch 2341, Loss 2.937071\n",
      "  params :  tensor([  5.3099, -16.9773])\n",
      "  grad :  tensor([-0.0098,  0.0557])\n",
      "Epoch 2342, Loss 2.937039\n",
      "  params :  tensor([  5.3100, -16.9779])\n",
      "  grad :  tensor([-0.0098,  0.0556])\n",
      "Epoch 2343, Loss 2.937008\n",
      "  params :  tensor([  5.3101, -16.9784])\n",
      "  grad :  tensor([-0.0098,  0.0555])\n",
      "Epoch 2344, Loss 2.936976\n",
      "  params :  tensor([  5.3102, -16.9790])\n",
      "  grad :  tensor([-0.0098,  0.0554])\n",
      "Epoch 2345, Loss 2.936945\n",
      "  params :  tensor([  5.3103, -16.9795])\n",
      "  grad :  tensor([-0.0098,  0.0553])\n",
      "Epoch 2346, Loss 2.936912\n",
      "  params :  tensor([  5.3104, -16.9801])\n",
      "  grad :  tensor([-0.0098,  0.0553])\n",
      "Epoch 2347, Loss 2.936883\n",
      "  params :  tensor([  5.3105, -16.9806])\n",
      "  grad :  tensor([-0.0097,  0.0552])\n",
      "Epoch 2348, Loss 2.936851\n",
      "  params :  tensor([  5.3106, -16.9812])\n",
      "  grad :  tensor([-0.0097,  0.0551])\n",
      "Epoch 2349, Loss 2.936819\n",
      "  params :  tensor([  5.3107, -16.9817])\n",
      "  grad :  tensor([-0.0097,  0.0550])\n",
      "Epoch 2350, Loss 2.936788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3107, -16.9823])\n",
      "  grad :  tensor([-0.0097,  0.0549])\n",
      "Epoch 2351, Loss 2.936757\n",
      "  params :  tensor([  5.3108, -16.9828])\n",
      "  grad :  tensor([-0.0097,  0.0548])\n",
      "Epoch 2352, Loss 2.936725\n",
      "  params :  tensor([  5.3109, -16.9834])\n",
      "  grad :  tensor([-0.0097,  0.0547])\n",
      "Epoch 2353, Loss 2.936694\n",
      "  params :  tensor([  5.3110, -16.9839])\n",
      "  grad :  tensor([-0.0096,  0.0546])\n",
      "Epoch 2354, Loss 2.936665\n",
      "  params :  tensor([  5.3111, -16.9845])\n",
      "  grad :  tensor([-0.0096,  0.0545])\n",
      "Epoch 2355, Loss 2.936633\n",
      "  params :  tensor([  5.3112, -16.9850])\n",
      "  grad :  tensor([-0.0096,  0.0544])\n",
      "Epoch 2356, Loss 2.936602\n",
      "  params :  tensor([  5.3113, -16.9856])\n",
      "  grad :  tensor([-0.0096,  0.0543])\n",
      "Epoch 2357, Loss 2.936572\n",
      "  params :  tensor([  5.3114, -16.9861])\n",
      "  grad :  tensor([-0.0096,  0.0542])\n",
      "Epoch 2358, Loss 2.936542\n",
      "  params :  tensor([  5.3115, -16.9866])\n",
      "  grad :  tensor([-0.0095,  0.0541])\n",
      "Epoch 2359, Loss 2.936511\n",
      "  params :  tensor([  5.3116, -16.9872])\n",
      "  grad :  tensor([-0.0096,  0.0540])\n",
      "Epoch 2360, Loss 2.936481\n",
      "  params :  tensor([  5.3117, -16.9877])\n",
      "  grad :  tensor([-0.0095,  0.0540])\n",
      "Epoch 2361, Loss 2.936451\n",
      "  params :  tensor([  5.3118, -16.9883])\n",
      "  grad :  tensor([-0.0095,  0.0539])\n",
      "Epoch 2362, Loss 2.936421\n",
      "  params :  tensor([  5.3119, -16.9888])\n",
      "  grad :  tensor([-0.0095,  0.0538])\n",
      "Epoch 2363, Loss 2.936392\n",
      "  params :  tensor([  5.3120, -16.9893])\n",
      "  grad :  tensor([-0.0095,  0.0537])\n",
      "Epoch 2364, Loss 2.936362\n",
      "  params :  tensor([  5.3121, -16.9899])\n",
      "  grad :  tensor([-0.0094,  0.0536])\n",
      "Epoch 2365, Loss 2.936332\n",
      "  params :  tensor([  5.3122, -16.9904])\n",
      "  grad :  tensor([-0.0094,  0.0535])\n",
      "Epoch 2366, Loss 2.936304\n",
      "  params :  tensor([  5.3123, -16.9909])\n",
      "  grad :  tensor([-0.0094,  0.0534])\n",
      "Epoch 2367, Loss 2.936274\n",
      "  params :  tensor([  5.3124, -16.9915])\n",
      "  grad :  tensor([-0.0094,  0.0533])\n",
      "Epoch 2368, Loss 2.936244\n",
      "  params :  tensor([  5.3125, -16.9920])\n",
      "  grad :  tensor([-0.0094,  0.0532])\n",
      "Epoch 2369, Loss 2.936216\n",
      "  params :  tensor([  5.3126, -16.9925])\n",
      "  grad :  tensor([-0.0094,  0.0531])\n",
      "Epoch 2370, Loss 2.936188\n",
      "  params :  tensor([  5.3127, -16.9931])\n",
      "  grad :  tensor([-0.0094,  0.0530])\n",
      "Epoch 2371, Loss 2.936156\n",
      "  params :  tensor([  5.3127, -16.9936])\n",
      "  grad :  tensor([-0.0094,  0.0530])\n",
      "Epoch 2372, Loss 2.936128\n",
      "  params :  tensor([  5.3128, -16.9941])\n",
      "  grad :  tensor([-0.0093,  0.0529])\n",
      "Epoch 2373, Loss 2.936100\n",
      "  params :  tensor([  5.3129, -16.9946])\n",
      "  grad :  tensor([-0.0093,  0.0528])\n",
      "Epoch 2374, Loss 2.936072\n",
      "  params :  tensor([  5.3130, -16.9952])\n",
      "  grad :  tensor([-0.0093,  0.0527])\n",
      "Epoch 2375, Loss 2.936042\n",
      "  params :  tensor([  5.3131, -16.9957])\n",
      "  grad :  tensor([-0.0093,  0.0526])\n",
      "Epoch 2376, Loss 2.936014\n",
      "  params :  tensor([  5.3132, -16.9962])\n",
      "  grad :  tensor([-0.0093,  0.0525])\n",
      "Epoch 2377, Loss 2.935986\n",
      "  params :  tensor([  5.3133, -16.9967])\n",
      "  grad :  tensor([-0.0093,  0.0524])\n",
      "Epoch 2378, Loss 2.935957\n",
      "  params :  tensor([  5.3134, -16.9973])\n",
      "  grad :  tensor([-0.0093,  0.0523])\n",
      "Epoch 2379, Loss 2.935928\n",
      "  params :  tensor([  5.3135, -16.9978])\n",
      "  grad :  tensor([-0.0092,  0.0522])\n",
      "Epoch 2380, Loss 2.935901\n",
      "  params :  tensor([  5.3136, -16.9983])\n",
      "  grad :  tensor([-0.0092,  0.0522])\n",
      "Epoch 2381, Loss 2.935873\n",
      "  params :  tensor([  5.3137, -16.9988])\n",
      "  grad :  tensor([-0.0092,  0.0521])\n",
      "Epoch 2382, Loss 2.935845\n",
      "  params :  tensor([  5.3138, -16.9994])\n",
      "  grad :  tensor([-0.0092,  0.0520])\n",
      "Epoch 2383, Loss 2.935817\n",
      "  params :  tensor([  5.3139, -16.9999])\n",
      "  grad :  tensor([-0.0092,  0.0519])\n",
      "Epoch 2384, Loss 2.935789\n",
      "  params :  tensor([  5.3139, -17.0004])\n",
      "  grad :  tensor([-0.0092,  0.0518])\n",
      "Epoch 2385, Loss 2.935762\n",
      "  params :  tensor([  5.3140, -17.0009])\n",
      "  grad :  tensor([-0.0092,  0.0517])\n",
      "Epoch 2386, Loss 2.935734\n",
      "  params :  tensor([  5.3141, -17.0014])\n",
      "  grad :  tensor([-0.0091,  0.0516])\n",
      "Epoch 2387, Loss 2.935707\n",
      "  params :  tensor([  5.3142, -17.0019])\n",
      "  grad :  tensor([-0.0091,  0.0515])\n",
      "Epoch 2388, Loss 2.935679\n",
      "  params :  tensor([  5.3143, -17.0025])\n",
      "  grad :  tensor([-0.0091,  0.0514])\n",
      "Epoch 2389, Loss 2.935650\n",
      "  params :  tensor([  5.3144, -17.0030])\n",
      "  grad :  tensor([-0.0091,  0.0514])\n",
      "Epoch 2390, Loss 2.935626\n",
      "  params :  tensor([  5.3145, -17.0035])\n",
      "  grad :  tensor([-0.0090,  0.0513])\n",
      "Epoch 2391, Loss 2.935596\n",
      "  params :  tensor([  5.3146, -17.0040])\n",
      "  grad :  tensor([-0.0090,  0.0512])\n",
      "Epoch 2392, Loss 2.935571\n",
      "  params :  tensor([  5.3147, -17.0045])\n",
      "  grad :  tensor([-0.0090,  0.0511])\n",
      "Epoch 2393, Loss 2.935544\n",
      "  params :  tensor([  5.3148, -17.0050])\n",
      "  grad :  tensor([-0.0090,  0.0510])\n",
      "Epoch 2394, Loss 2.935516\n",
      "  params :  tensor([  5.3149, -17.0055])\n",
      "  grad :  tensor([-0.0090,  0.0509])\n",
      "Epoch 2395, Loss 2.935489\n",
      "  params :  tensor([  5.3149, -17.0060])\n",
      "  grad :  tensor([-0.0090,  0.0508])\n",
      "Epoch 2396, Loss 2.935465\n",
      "  params :  tensor([  5.3150, -17.0065])\n",
      "  grad :  tensor([-0.0090,  0.0507])\n",
      "Epoch 2397, Loss 2.935436\n",
      "  params :  tensor([  5.3151, -17.0070])\n",
      "  grad :  tensor([-0.0090,  0.0507])\n",
      "Epoch 2398, Loss 2.935411\n",
      "  params :  tensor([  5.3152, -17.0076])\n",
      "  grad :  tensor([-0.0089,  0.0506])\n",
      "Epoch 2399, Loss 2.935385\n",
      "  params :  tensor([  5.3153, -17.0081])\n",
      "  grad :  tensor([-0.0089,  0.0505])\n",
      "Epoch 2400, Loss 2.935356\n",
      "  params :  tensor([  5.3154, -17.0086])\n",
      "  grad :  tensor([-0.0089,  0.0504])\n",
      "Epoch 2401, Loss 2.935332\n",
      "  params :  tensor([  5.3155, -17.0091])\n",
      "  grad :  tensor([-0.0089,  0.0503])\n",
      "Epoch 2402, Loss 2.935304\n",
      "  params :  tensor([  5.3156, -17.0096])\n",
      "  grad :  tensor([-0.0089,  0.0502])\n",
      "Epoch 2403, Loss 2.935281\n",
      "  params :  tensor([  5.3157, -17.0101])\n",
      "  grad :  tensor([-0.0088,  0.0502])\n",
      "Epoch 2404, Loss 2.935252\n",
      "  params :  tensor([  5.3157, -17.0106])\n",
      "  grad :  tensor([-0.0088,  0.0501])\n",
      "Epoch 2405, Loss 2.935228\n",
      "  params :  tensor([  5.3158, -17.0111])\n",
      "  grad :  tensor([-0.0088,  0.0500])\n",
      "Epoch 2406, Loss 2.935203\n",
      "  params :  tensor([  5.3159, -17.0116])\n",
      "  grad :  tensor([-0.0088,  0.0499])\n",
      "Epoch 2407, Loss 2.935177\n",
      "  params :  tensor([  5.3160, -17.0121])\n",
      "  grad :  tensor([-0.0088,  0.0498])\n",
      "Epoch 2408, Loss 2.935152\n",
      "  params :  tensor([  5.3161, -17.0126])\n",
      "  grad :  tensor([-0.0088,  0.0497])\n",
      "Epoch 2409, Loss 2.935126\n",
      "  params :  tensor([  5.3162, -17.0131])\n",
      "  grad :  tensor([-0.0088,  0.0496])\n",
      "Epoch 2410, Loss 2.935100\n",
      "  params :  tensor([  5.3163, -17.0136])\n",
      "  grad :  tensor([-0.0088,  0.0496])\n",
      "Epoch 2411, Loss 2.935075\n",
      "  params :  tensor([  5.3164, -17.0140])\n",
      "  grad :  tensor([-0.0087,  0.0495])\n",
      "Epoch 2412, Loss 2.935049\n",
      "  params :  tensor([  5.3164, -17.0145])\n",
      "  grad :  tensor([-0.0087,  0.0494])\n",
      "Epoch 2413, Loss 2.935024\n",
      "  params :  tensor([  5.3165, -17.0150])\n",
      "  grad :  tensor([-0.0087,  0.0493])\n",
      "Epoch 2414, Loss 2.935001\n",
      "  params :  tensor([  5.3166, -17.0155])\n",
      "  grad :  tensor([-0.0087,  0.0492])\n",
      "Epoch 2415, Loss 2.934973\n",
      "  params :  tensor([  5.3167, -17.0160])\n",
      "  grad :  tensor([-0.0087,  0.0491])\n",
      "Epoch 2416, Loss 2.934949\n",
      "  params :  tensor([  5.3168, -17.0165])\n",
      "  grad :  tensor([-0.0087,  0.0491])\n",
      "Epoch 2417, Loss 2.934925\n",
      "  params :  tensor([  5.3169, -17.0170])\n",
      "  grad :  tensor([-0.0086,  0.0490])\n",
      "Epoch 2418, Loss 2.934899\n",
      "  params :  tensor([  5.3170, -17.0175])\n",
      "  grad :  tensor([-0.0086,  0.0489])\n",
      "Epoch 2419, Loss 2.934876\n",
      "  params :  tensor([  5.3171, -17.0180])\n",
      "  grad :  tensor([-0.0086,  0.0488])\n",
      "Epoch 2420, Loss 2.934853\n",
      "  params :  tensor([  5.3171, -17.0185])\n",
      "  grad :  tensor([-0.0086,  0.0487])\n",
      "Epoch 2421, Loss 2.934826\n",
      "  params :  tensor([  5.3172, -17.0189])\n",
      "  grad :  tensor([-0.0086,  0.0486])\n",
      "Epoch 2422, Loss 2.934802\n",
      "  params :  tensor([  5.3173, -17.0194])\n",
      "  grad :  tensor([-0.0086,  0.0486])\n",
      "Epoch 2423, Loss 2.934777\n",
      "  params :  tensor([  5.3174, -17.0199])\n",
      "  grad :  tensor([-0.0086,  0.0485])\n",
      "Epoch 2424, Loss 2.934753\n",
      "  params :  tensor([  5.3175, -17.0204])\n",
      "  grad :  tensor([-0.0086,  0.0484])\n",
      "Epoch 2425, Loss 2.934730\n",
      "  params :  tensor([  5.3176, -17.0209])\n",
      "  grad :  tensor([-0.0086,  0.0483])\n",
      "Epoch 2426, Loss 2.934705\n",
      "  params :  tensor([  5.3177, -17.0214])\n",
      "  grad :  tensor([-0.0085,  0.0482])\n",
      "Epoch 2427, Loss 2.934681\n",
      "  params :  tensor([  5.3177, -17.0219])\n",
      "  grad :  tensor([-0.0085,  0.0481])\n",
      "Epoch 2428, Loss 2.934658\n",
      "  params :  tensor([  5.3178, -17.0223])\n",
      "  grad :  tensor([-0.0085,  0.0481])\n",
      "Epoch 2429, Loss 2.934635\n",
      "  params :  tensor([  5.3179, -17.0228])\n",
      "  grad :  tensor([-0.0085,  0.0480])\n",
      "Epoch 2430, Loss 2.934609\n",
      "  params :  tensor([  5.3180, -17.0233])\n",
      "  grad :  tensor([-0.0085,  0.0479])\n",
      "Epoch 2431, Loss 2.934585\n",
      "  params :  tensor([  5.3181, -17.0238])\n",
      "  grad :  tensor([-0.0084,  0.0478])\n",
      "Epoch 2432, Loss 2.934563\n",
      "  params :  tensor([  5.3182, -17.0242])\n",
      "  grad :  tensor([-0.0084,  0.0477])\n",
      "Epoch 2433, Loss 2.934541\n",
      "  params :  tensor([  5.3182, -17.0247])\n",
      "  grad :  tensor([-0.0084,  0.0477])\n",
      "Epoch 2434, Loss 2.934516\n",
      "  params :  tensor([  5.3183, -17.0252])\n",
      "  grad :  tensor([-0.0084,  0.0476])\n",
      "Epoch 2435, Loss 2.934493\n",
      "  params :  tensor([  5.3184, -17.0257])\n",
      "  grad :  tensor([-0.0084,  0.0475])\n",
      "Epoch 2436, Loss 2.934469\n",
      "  params :  tensor([  5.3185, -17.0261])\n",
      "  grad :  tensor([-0.0084,  0.0474])\n",
      "Epoch 2437, Loss 2.934446\n",
      "  params :  tensor([  5.3186, -17.0266])\n",
      "  grad :  tensor([-0.0084,  0.0473])\n",
      "Epoch 2438, Loss 2.934423\n",
      "  params :  tensor([  5.3187, -17.0271])\n",
      "  grad :  tensor([-0.0083,  0.0473])\n",
      "Epoch 2439, Loss 2.934400\n",
      "  params :  tensor([  5.3187, -17.0276])\n",
      "  grad :  tensor([-0.0083,  0.0472])\n",
      "Epoch 2440, Loss 2.934377\n",
      "  params :  tensor([  5.3188, -17.0280])\n",
      "  grad :  tensor([-0.0083,  0.0471])\n",
      "Epoch 2441, Loss 2.934355\n",
      "  params :  tensor([  5.3189, -17.0285])\n",
      "  grad :  tensor([-0.0083,  0.0470])\n",
      "Epoch 2442, Loss 2.934331\n",
      "  params :  tensor([  5.3190, -17.0290])\n",
      "  grad :  tensor([-0.0083,  0.0469])\n",
      "Epoch 2443, Loss 2.934309\n",
      "  params :  tensor([  5.3191, -17.0294])\n",
      "  grad :  tensor([-0.0083,  0.0469])\n",
      "Epoch 2444, Loss 2.934287\n",
      "  params :  tensor([  5.3192, -17.0299])\n",
      "  grad :  tensor([-0.0083,  0.0468])\n",
      "Epoch 2445, Loss 2.934264\n",
      "  params :  tensor([  5.3192, -17.0304])\n",
      "  grad :  tensor([-0.0083,  0.0467])\n",
      "Epoch 2446, Loss 2.934242\n",
      "  params :  tensor([  5.3193, -17.0308])\n",
      "  grad :  tensor([-0.0083,  0.0466])\n",
      "Epoch 2447, Loss 2.934219\n",
      "  params :  tensor([  5.3194, -17.0313])\n",
      "  grad :  tensor([-0.0082,  0.0465])\n",
      "Epoch 2448, Loss 2.934198\n",
      "  params :  tensor([  5.3195, -17.0318])\n",
      "  grad :  tensor([-0.0082,  0.0465])\n",
      "Epoch 2449, Loss 2.934175\n",
      "  params :  tensor([  5.3196, -17.0322])\n",
      "  grad :  tensor([-0.0082,  0.0464])\n",
      "Epoch 2450, Loss 2.934151\n",
      "  params :  tensor([  5.3197, -17.0327])\n",
      "  grad :  tensor([-0.0082,  0.0463])\n",
      "Epoch 2451, Loss 2.934129\n",
      "  params :  tensor([  5.3197, -17.0332])\n",
      "  grad :  tensor([-0.0082,  0.0462])\n",
      "Epoch 2452, Loss 2.934108\n",
      "  params :  tensor([  5.3198, -17.0336])\n",
      "  grad :  tensor([-0.0082,  0.0461])\n",
      "Epoch 2453, Loss 2.934084\n",
      "  params :  tensor([  5.3199, -17.0341])\n",
      "  grad :  tensor([-0.0081,  0.0461])\n",
      "Epoch 2454, Loss 2.934065\n",
      "  params :  tensor([  5.3200, -17.0345])\n",
      "  grad :  tensor([-0.0081,  0.0460])\n",
      "Epoch 2455, Loss 2.934043\n",
      "  params :  tensor([  5.3201, -17.0350])\n",
      "  grad :  tensor([-0.0081,  0.0459])\n",
      "Epoch 2456, Loss 2.934020\n",
      "  params :  tensor([  5.3201, -17.0355])\n",
      "  grad :  tensor([-0.0081,  0.0458])\n",
      "Epoch 2457, Loss 2.934000\n",
      "  params :  tensor([  5.3202, -17.0359])\n",
      "  grad :  tensor([-0.0081,  0.0457])\n",
      "Epoch 2458, Loss 2.933978\n",
      "  params :  tensor([  5.3203, -17.0364])\n",
      "  grad :  tensor([-0.0081,  0.0457])\n",
      "Epoch 2459, Loss 2.933956\n",
      "  params :  tensor([  5.3204, -17.0368])\n",
      "  grad :  tensor([-0.0080,  0.0456])\n",
      "Epoch 2460, Loss 2.933935\n",
      "  params :  tensor([  5.3205, -17.0373])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad :  tensor([-0.0080,  0.0455])\n",
      "Epoch 2461, Loss 2.933914\n",
      "  params :  tensor([  5.3205, -17.0377])\n",
      "  grad :  tensor([-0.0080,  0.0454])\n",
      "Epoch 2462, Loss 2.933893\n",
      "  params :  tensor([  5.3206, -17.0382])\n",
      "  grad :  tensor([-0.0080,  0.0454])\n",
      "Epoch 2463, Loss 2.933871\n",
      "  params :  tensor([  5.3207, -17.0386])\n",
      "  grad :  tensor([-0.0080,  0.0453])\n",
      "Epoch 2464, Loss 2.933849\n",
      "  params :  tensor([  5.3208, -17.0391])\n",
      "  grad :  tensor([-0.0080,  0.0452])\n",
      "Epoch 2465, Loss 2.933828\n",
      "  params :  tensor([  5.3209, -17.0396])\n",
      "  grad :  tensor([-0.0080,  0.0451])\n",
      "Epoch 2466, Loss 2.933807\n",
      "  params :  tensor([  5.3209, -17.0400])\n",
      "  grad :  tensor([-0.0080,  0.0451])\n",
      "Epoch 2467, Loss 2.933787\n",
      "  params :  tensor([  5.3210, -17.0405])\n",
      "  grad :  tensor([-0.0079,  0.0450])\n",
      "Epoch 2468, Loss 2.933767\n",
      "  params :  tensor([  5.3211, -17.0409])\n",
      "  grad :  tensor([-0.0079,  0.0449])\n",
      "Epoch 2469, Loss 2.933746\n",
      "  params :  tensor([  5.3212, -17.0413])\n",
      "  grad :  tensor([-0.0079,  0.0448])\n",
      "Epoch 2470, Loss 2.933723\n",
      "  params :  tensor([  5.3213, -17.0418])\n",
      "  grad :  tensor([-0.0079,  0.0448])\n",
      "Epoch 2471, Loss 2.933704\n",
      "  params :  tensor([  5.3213, -17.0422])\n",
      "  grad :  tensor([-0.0079,  0.0447])\n",
      "Epoch 2472, Loss 2.933682\n",
      "  params :  tensor([  5.3214, -17.0427])\n",
      "  grad :  tensor([-0.0079,  0.0446])\n",
      "Epoch 2473, Loss 2.933662\n",
      "  params :  tensor([  5.3215, -17.0431])\n",
      "  grad :  tensor([-0.0079,  0.0445])\n",
      "Epoch 2474, Loss 2.933643\n",
      "  params :  tensor([  5.3216, -17.0436])\n",
      "  grad :  tensor([-0.0079,  0.0444])\n",
      "Epoch 2475, Loss 2.933622\n",
      "  params :  tensor([  5.3217, -17.0440])\n",
      "  grad :  tensor([-0.0078,  0.0444])\n",
      "Epoch 2476, Loss 2.933602\n",
      "  params :  tensor([  5.3217, -17.0445])\n",
      "  grad :  tensor([-0.0078,  0.0443])\n",
      "Epoch 2477, Loss 2.933583\n",
      "  params :  tensor([  5.3218, -17.0449])\n",
      "  grad :  tensor([-0.0078,  0.0442])\n",
      "Epoch 2478, Loss 2.933561\n",
      "  params :  tensor([  5.3219, -17.0453])\n",
      "  grad :  tensor([-0.0078,  0.0441])\n",
      "Epoch 2479, Loss 2.933541\n",
      "  params :  tensor([  5.3220, -17.0458])\n",
      "  grad :  tensor([-0.0078,  0.0441])\n",
      "Epoch 2480, Loss 2.933521\n",
      "  params :  tensor([  5.3220, -17.0462])\n",
      "  grad :  tensor([-0.0078,  0.0440])\n",
      "Epoch 2481, Loss 2.933501\n",
      "  params :  tensor([  5.3221, -17.0467])\n",
      "  grad :  tensor([-0.0078,  0.0439])\n",
      "Epoch 2482, Loss 2.933480\n",
      "  params :  tensor([  5.3222, -17.0471])\n",
      "  grad :  tensor([-0.0077,  0.0438])\n",
      "Epoch 2483, Loss 2.933463\n",
      "  params :  tensor([  5.3223, -17.0475])\n",
      "  grad :  tensor([-0.0077,  0.0438])\n",
      "Epoch 2484, Loss 2.933442\n",
      "  params :  tensor([  5.3224, -17.0480])\n",
      "  grad :  tensor([-0.0077,  0.0437])\n",
      "Epoch 2485, Loss 2.933422\n",
      "  params :  tensor([  5.3224, -17.0484])\n",
      "  grad :  tensor([-0.0077,  0.0436])\n",
      "Epoch 2486, Loss 2.933403\n",
      "  params :  tensor([  5.3225, -17.0489])\n",
      "  grad :  tensor([-0.0077,  0.0436])\n",
      "Epoch 2487, Loss 2.933382\n",
      "  params :  tensor([  5.3226, -17.0493])\n",
      "  grad :  tensor([-0.0077,  0.0435])\n",
      "Epoch 2488, Loss 2.933365\n",
      "  params :  tensor([  5.3227, -17.0497])\n",
      "  grad :  tensor([-0.0077,  0.0434])\n",
      "Epoch 2489, Loss 2.933345\n",
      "  params :  tensor([  5.3227, -17.0502])\n",
      "  grad :  tensor([-0.0077,  0.0433])\n",
      "Epoch 2490, Loss 2.933325\n",
      "  params :  tensor([  5.3228, -17.0506])\n",
      "  grad :  tensor([-0.0076,  0.0433])\n",
      "Epoch 2491, Loss 2.933306\n",
      "  params :  tensor([  5.3229, -17.0510])\n",
      "  grad :  tensor([-0.0076,  0.0432])\n",
      "Epoch 2492, Loss 2.933287\n",
      "  params :  tensor([  5.3230, -17.0515])\n",
      "  grad :  tensor([-0.0076,  0.0431])\n",
      "Epoch 2493, Loss 2.933266\n",
      "  params :  tensor([  5.3230, -17.0519])\n",
      "  grad :  tensor([-0.0076,  0.0430])\n",
      "Epoch 2494, Loss 2.933249\n",
      "  params :  tensor([  5.3231, -17.0523])\n",
      "  grad :  tensor([-0.0076,  0.0430])\n",
      "Epoch 2495, Loss 2.933229\n",
      "  params :  tensor([  5.3232, -17.0527])\n",
      "  grad :  tensor([-0.0076,  0.0429])\n",
      "Epoch 2496, Loss 2.933209\n",
      "  params :  tensor([  5.3233, -17.0532])\n",
      "  grad :  tensor([-0.0076,  0.0428])\n",
      "Epoch 2497, Loss 2.933190\n",
      "  params :  tensor([  5.3233, -17.0536])\n",
      "  grad :  tensor([-0.0075,  0.0427])\n",
      "Epoch 2498, Loss 2.933172\n",
      "  params :  tensor([  5.3234, -17.0540])\n",
      "  grad :  tensor([-0.0075,  0.0427])\n",
      "Epoch 2499, Loss 2.933154\n",
      "  params :  tensor([  5.3235, -17.0544])\n",
      "  grad :  tensor([-0.0075,  0.0426])\n",
      "Epoch 2500, Loss 2.933134\n",
      "  params :  tensor([  5.3236, -17.0549])\n",
      "  grad :  tensor([-0.0075,  0.0425])\n",
      "Epoch 2501, Loss 2.933116\n",
      "  params :  tensor([  5.3236, -17.0553])\n",
      "  grad :  tensor([-0.0075,  0.0425])\n",
      "Epoch 2502, Loss 2.933097\n",
      "  params :  tensor([  5.3237, -17.0557])\n",
      "  grad :  tensor([-0.0075,  0.0424])\n",
      "Epoch 2503, Loss 2.933079\n",
      "  params :  tensor([  5.3238, -17.0561])\n",
      "  grad :  tensor([-0.0075,  0.0423])\n",
      "Epoch 2504, Loss 2.933060\n",
      "  params :  tensor([  5.3239, -17.0566])\n",
      "  grad :  tensor([-0.0075,  0.0422])\n",
      "Epoch 2505, Loss 2.933043\n",
      "  params :  tensor([  5.3239, -17.0570])\n",
      "  grad :  tensor([-0.0074,  0.0422])\n",
      "Epoch 2506, Loss 2.933025\n",
      "  params :  tensor([  5.3240, -17.0574])\n",
      "  grad :  tensor([-0.0074,  0.0421])\n",
      "Epoch 2507, Loss 2.933007\n",
      "  params :  tensor([  5.3241, -17.0578])\n",
      "  grad :  tensor([-0.0074,  0.0420])\n",
      "Epoch 2508, Loss 2.932988\n",
      "  params :  tensor([  5.3242, -17.0582])\n",
      "  grad :  tensor([-0.0074,  0.0420])\n",
      "Epoch 2509, Loss 2.932970\n",
      "  params :  tensor([  5.3242, -17.0587])\n",
      "  grad :  tensor([-0.0074,  0.0419])\n",
      "Epoch 2510, Loss 2.932953\n",
      "  params :  tensor([  5.3243, -17.0591])\n",
      "  grad :  tensor([-0.0074,  0.0418])\n",
      "Epoch 2511, Loss 2.932932\n",
      "  params :  tensor([  5.3244, -17.0595])\n",
      "  grad :  tensor([-0.0074,  0.0417])\n",
      "Epoch 2512, Loss 2.932915\n",
      "  params :  tensor([  5.3245, -17.0599])\n",
      "  grad :  tensor([-0.0073,  0.0417])\n",
      "Epoch 2513, Loss 2.932898\n",
      "  params :  tensor([  5.3245, -17.0603])\n",
      "  grad :  tensor([-0.0073,  0.0416])\n",
      "Epoch 2514, Loss 2.932880\n",
      "  params :  tensor([  5.3246, -17.0608])\n",
      "  grad :  tensor([-0.0073,  0.0415])\n",
      "Epoch 2515, Loss 2.932862\n",
      "  params :  tensor([  5.3247, -17.0612])\n",
      "  grad :  tensor([-0.0073,  0.0415])\n",
      "Epoch 2516, Loss 2.932846\n",
      "  params :  tensor([  5.3248, -17.0616])\n",
      "  grad :  tensor([-0.0073,  0.0414])\n",
      "Epoch 2517, Loss 2.932826\n",
      "  params :  tensor([  5.3248, -17.0620])\n",
      "  grad :  tensor([-0.0073,  0.0413])\n",
      "Epoch 2518, Loss 2.932810\n",
      "  params :  tensor([  5.3249, -17.0624])\n",
      "  grad :  tensor([-0.0073,  0.0412])\n",
      "Epoch 2519, Loss 2.932790\n",
      "  params :  tensor([  5.3250, -17.0628])\n",
      "  grad :  tensor([-0.0073,  0.0412])\n",
      "Epoch 2520, Loss 2.932774\n",
      "  params :  tensor([  5.3250, -17.0632])\n",
      "  grad :  tensor([-0.0073,  0.0411])\n",
      "Epoch 2521, Loss 2.932758\n",
      "  params :  tensor([  5.3251, -17.0636])\n",
      "  grad :  tensor([-0.0073,  0.0410])\n",
      "Epoch 2522, Loss 2.932739\n",
      "  params :  tensor([  5.3252, -17.0640])\n",
      "  grad :  tensor([-0.0073,  0.0410])\n",
      "Epoch 2523, Loss 2.932723\n",
      "  params :  tensor([  5.3253, -17.0645])\n",
      "  grad :  tensor([-0.0072,  0.0409])\n",
      "Epoch 2524, Loss 2.932706\n",
      "  params :  tensor([  5.3253, -17.0649])\n",
      "  grad :  tensor([-0.0072,  0.0408])\n",
      "Epoch 2525, Loss 2.932689\n",
      "  params :  tensor([  5.3254, -17.0653])\n",
      "  grad :  tensor([-0.0072,  0.0408])\n",
      "Epoch 2526, Loss 2.932671\n",
      "  params :  tensor([  5.3255, -17.0657])\n",
      "  grad :  tensor([-0.0072,  0.0407])\n",
      "Epoch 2527, Loss 2.932654\n",
      "  params :  tensor([  5.3256, -17.0661])\n",
      "  grad :  tensor([-0.0072,  0.0406])\n",
      "Epoch 2528, Loss 2.932637\n",
      "  params :  tensor([  5.3256, -17.0665])\n",
      "  grad :  tensor([-0.0072,  0.0405])\n",
      "Epoch 2529, Loss 2.932619\n",
      "  params :  tensor([  5.3257, -17.0669])\n",
      "  grad :  tensor([-0.0072,  0.0405])\n",
      "Epoch 2530, Loss 2.932603\n",
      "  params :  tensor([  5.3258, -17.0673])\n",
      "  grad :  tensor([-0.0071,  0.0404])\n",
      "Epoch 2531, Loss 2.932585\n",
      "  params :  tensor([  5.3258, -17.0677])\n",
      "  grad :  tensor([-0.0071,  0.0403])\n",
      "Epoch 2532, Loss 2.932569\n",
      "  params :  tensor([  5.3259, -17.0681])\n",
      "  grad :  tensor([-0.0071,  0.0403])\n",
      "Epoch 2533, Loss 2.932553\n",
      "  params :  tensor([  5.3260, -17.0685])\n",
      "  grad :  tensor([-0.0071,  0.0402])\n",
      "Epoch 2534, Loss 2.932535\n",
      "  params :  tensor([  5.3261, -17.0689])\n",
      "  grad :  tensor([-0.0071,  0.0401])\n",
      "Epoch 2535, Loss 2.932520\n",
      "  params :  tensor([  5.3261, -17.0693])\n",
      "  grad :  tensor([-0.0071,  0.0401])\n",
      "Epoch 2536, Loss 2.932502\n",
      "  params :  tensor([  5.3262, -17.0697])\n",
      "  grad :  tensor([-0.0071,  0.0400])\n",
      "Epoch 2537, Loss 2.932487\n",
      "  params :  tensor([  5.3263, -17.0701])\n",
      "  grad :  tensor([-0.0071,  0.0399])\n",
      "Epoch 2538, Loss 2.932469\n",
      "  params :  tensor([  5.3263, -17.0705])\n",
      "  grad :  tensor([-0.0070,  0.0399])\n",
      "Epoch 2539, Loss 2.932455\n",
      "  params :  tensor([  5.3264, -17.0709])\n",
      "  grad :  tensor([-0.0070,  0.0398])\n",
      "Epoch 2540, Loss 2.932438\n",
      "  params :  tensor([  5.3265, -17.0713])\n",
      "  grad :  tensor([-0.0070,  0.0397])\n",
      "Epoch 2541, Loss 2.932421\n",
      "  params :  tensor([  5.3265, -17.0717])\n",
      "  grad :  tensor([-0.0070,  0.0397])\n",
      "Epoch 2542, Loss 2.932404\n",
      "  params :  tensor([  5.3266, -17.0721])\n",
      "  grad :  tensor([-0.0070,  0.0396])\n",
      "Epoch 2543, Loss 2.932387\n",
      "  params :  tensor([  5.3267, -17.0725])\n",
      "  grad :  tensor([-0.0070,  0.0395])\n",
      "Epoch 2544, Loss 2.932371\n",
      "  params :  tensor([  5.3268, -17.0729])\n",
      "  grad :  tensor([-0.0070,  0.0395])\n",
      "Epoch 2545, Loss 2.932358\n",
      "  params :  tensor([  5.3268, -17.0733])\n",
      "  grad :  tensor([-0.0070,  0.0394])\n",
      "Epoch 2546, Loss 2.932340\n",
      "  params :  tensor([  5.3269, -17.0737])\n",
      "  grad :  tensor([-0.0069,  0.0393])\n",
      "Epoch 2547, Loss 2.932324\n",
      "  params :  tensor([  5.3270, -17.0741])\n",
      "  grad :  tensor([-0.0069,  0.0393])\n",
      "Epoch 2548, Loss 2.932310\n",
      "  params :  tensor([  5.3270, -17.0745])\n",
      "  grad :  tensor([-0.0069,  0.0392])\n",
      "Epoch 2549, Loss 2.932293\n",
      "  params :  tensor([  5.3271, -17.0749])\n",
      "  grad :  tensor([-0.0069,  0.0391])\n",
      "Epoch 2550, Loss 2.932277\n",
      "  params :  tensor([  5.3272, -17.0752])\n",
      "  grad :  tensor([-0.0069,  0.0391])\n",
      "Epoch 2551, Loss 2.932261\n",
      "  params :  tensor([  5.3272, -17.0756])\n",
      "  grad :  tensor([-0.0069,  0.0390])\n",
      "Epoch 2552, Loss 2.932246\n",
      "  params :  tensor([  5.3273, -17.0760])\n",
      "  grad :  tensor([-0.0069,  0.0389])\n",
      "Epoch 2553, Loss 2.932229\n",
      "  params :  tensor([  5.3274, -17.0764])\n",
      "  grad :  tensor([-0.0069,  0.0389])\n",
      "Epoch 2554, Loss 2.932215\n",
      "  params :  tensor([  5.3274, -17.0768])\n",
      "  grad :  tensor([-0.0069,  0.0388])\n",
      "Epoch 2555, Loss 2.932198\n",
      "  params :  tensor([  5.3275, -17.0772])\n",
      "  grad :  tensor([-0.0068,  0.0387])\n",
      "Epoch 2556, Loss 2.932183\n",
      "  params :  tensor([  5.3276, -17.0776])\n",
      "  grad :  tensor([-0.0068,  0.0387])\n",
      "Epoch 2557, Loss 2.932167\n",
      "  params :  tensor([  5.3276, -17.0780])\n",
      "  grad :  tensor([-0.0068,  0.0386])\n",
      "Epoch 2558, Loss 2.932153\n",
      "  params :  tensor([  5.3277, -17.0783])\n",
      "  grad :  tensor([-0.0068,  0.0385])\n",
      "Epoch 2559, Loss 2.932137\n",
      "  params :  tensor([  5.3278, -17.0787])\n",
      "  grad :  tensor([-0.0068,  0.0385])\n",
      "Epoch 2560, Loss 2.932122\n",
      "  params :  tensor([  5.3279, -17.0791])\n",
      "  grad :  tensor([-0.0068,  0.0384])\n",
      "Epoch 2561, Loss 2.932107\n",
      "  params :  tensor([  5.3279, -17.0795])\n",
      "  grad :  tensor([-0.0068,  0.0383])\n",
      "Epoch 2562, Loss 2.932092\n",
      "  params :  tensor([  5.3280, -17.0799])\n",
      "  grad :  tensor([-0.0068,  0.0383])\n",
      "Epoch 2563, Loss 2.932076\n",
      "  params :  tensor([  5.3281, -17.0803])\n",
      "  grad :  tensor([-0.0067,  0.0382])\n",
      "Epoch 2564, Loss 2.932061\n",
      "  params :  tensor([  5.3281, -17.0806])\n",
      "  grad :  tensor([-0.0067,  0.0381])\n",
      "Epoch 2565, Loss 2.932047\n",
      "  params :  tensor([  5.3282, -17.0810])\n",
      "  grad :  tensor([-0.0067,  0.0381])\n",
      "Epoch 2566, Loss 2.932031\n",
      "  params :  tensor([  5.3283, -17.0814])\n",
      "  grad :  tensor([-0.0067,  0.0380])\n",
      "Epoch 2567, Loss 2.932017\n",
      "  params :  tensor([  5.3283, -17.0818])\n",
      "  grad :  tensor([-0.0067,  0.0379])\n",
      "Epoch 2568, Loss 2.932002\n",
      "  params :  tensor([  5.3284, -17.0822])\n",
      "  grad :  tensor([-0.0067,  0.0379])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2569, Loss 2.931986\n",
      "  params :  tensor([  5.3285, -17.0825])\n",
      "  grad :  tensor([-0.0067,  0.0378])\n",
      "Epoch 2570, Loss 2.931972\n",
      "  params :  tensor([  5.3285, -17.0829])\n",
      "  grad :  tensor([-0.0067,  0.0378])\n",
      "Epoch 2571, Loss 2.931957\n",
      "  params :  tensor([  5.3286, -17.0833])\n",
      "  grad :  tensor([-0.0067,  0.0377])\n",
      "Epoch 2572, Loss 2.931941\n",
      "  params :  tensor([  5.3287, -17.0837])\n",
      "  grad :  tensor([-0.0067,  0.0376])\n",
      "Epoch 2573, Loss 2.931929\n",
      "  params :  tensor([  5.3287, -17.0840])\n",
      "  grad :  tensor([-0.0066,  0.0376])\n",
      "Epoch 2574, Loss 2.931914\n",
      "  params :  tensor([  5.3288, -17.0844])\n",
      "  grad :  tensor([-0.0066,  0.0375])\n",
      "Epoch 2575, Loss 2.931900\n",
      "  params :  tensor([  5.3289, -17.0848])\n",
      "  grad :  tensor([-0.0066,  0.0374])\n",
      "Epoch 2576, Loss 2.931885\n",
      "  params :  tensor([  5.3289, -17.0852])\n",
      "  grad :  tensor([-0.0066,  0.0374])\n",
      "Epoch 2577, Loss 2.931870\n",
      "  params :  tensor([  5.3290, -17.0855])\n",
      "  grad :  tensor([-0.0066,  0.0373])\n",
      "Epoch 2578, Loss 2.931855\n",
      "  params :  tensor([  5.3291, -17.0859])\n",
      "  grad :  tensor([-0.0066,  0.0372])\n",
      "Epoch 2579, Loss 2.931842\n",
      "  params :  tensor([  5.3291, -17.0863])\n",
      "  grad :  tensor([-0.0066,  0.0372])\n",
      "Epoch 2580, Loss 2.931828\n",
      "  params :  tensor([  5.3292, -17.0867])\n",
      "  grad :  tensor([-0.0066,  0.0371])\n",
      "Epoch 2581, Loss 2.931813\n",
      "  params :  tensor([  5.3293, -17.0870])\n",
      "  grad :  tensor([-0.0065,  0.0371])\n",
      "Epoch 2582, Loss 2.931799\n",
      "  params :  tensor([  5.3293, -17.0874])\n",
      "  grad :  tensor([-0.0065,  0.0370])\n",
      "Epoch 2583, Loss 2.931786\n",
      "  params :  tensor([  5.3294, -17.0878])\n",
      "  grad :  tensor([-0.0065,  0.0369])\n",
      "Epoch 2584, Loss 2.931771\n",
      "  params :  tensor([  5.3294, -17.0881])\n",
      "  grad :  tensor([-0.0065,  0.0369])\n",
      "Epoch 2585, Loss 2.931759\n",
      "  params :  tensor([  5.3295, -17.0885])\n",
      "  grad :  tensor([-0.0065,  0.0368])\n",
      "Epoch 2586, Loss 2.931742\n",
      "  params :  tensor([  5.3296, -17.0889])\n",
      "  grad :  tensor([-0.0065,  0.0367])\n",
      "Epoch 2587, Loss 2.931729\n",
      "  params :  tensor([  5.3296, -17.0892])\n",
      "  grad :  tensor([-0.0065,  0.0367])\n",
      "Epoch 2588, Loss 2.931717\n",
      "  params :  tensor([  5.3297, -17.0896])\n",
      "  grad :  tensor([-0.0065,  0.0366])\n",
      "Epoch 2589, Loss 2.931701\n",
      "  params :  tensor([  5.3298, -17.0900])\n",
      "  grad :  tensor([-0.0065,  0.0366])\n",
      "Epoch 2590, Loss 2.931687\n",
      "  params :  tensor([  5.3298, -17.0903])\n",
      "  grad :  tensor([-0.0065,  0.0365])\n",
      "Epoch 2591, Loss 2.931674\n",
      "  params :  tensor([  5.3299, -17.0907])\n",
      "  grad :  tensor([-0.0064,  0.0364])\n",
      "Epoch 2592, Loss 2.931660\n",
      "  params :  tensor([  5.3300, -17.0911])\n",
      "  grad :  tensor([-0.0064,  0.0364])\n",
      "Epoch 2593, Loss 2.931648\n",
      "  params :  tensor([  5.3300, -17.0914])\n",
      "  grad :  tensor([-0.0064,  0.0363])\n",
      "Epoch 2594, Loss 2.931632\n",
      "  params :  tensor([  5.3301, -17.0918])\n",
      "  grad :  tensor([-0.0064,  0.0362])\n",
      "Epoch 2595, Loss 2.931619\n",
      "  params :  tensor([  5.3302, -17.0921])\n",
      "  grad :  tensor([-0.0064,  0.0362])\n",
      "Epoch 2596, Loss 2.931606\n",
      "  params :  tensor([  5.3302, -17.0925])\n",
      "  grad :  tensor([-0.0064,  0.0361])\n",
      "Epoch 2597, Loss 2.931593\n",
      "  params :  tensor([  5.3303, -17.0929])\n",
      "  grad :  tensor([-0.0064,  0.0361])\n",
      "Epoch 2598, Loss 2.931580\n",
      "  params :  tensor([  5.3303, -17.0932])\n",
      "  grad :  tensor([-0.0064,  0.0360])\n",
      "Epoch 2599, Loss 2.931566\n",
      "  params :  tensor([  5.3304, -17.0936])\n",
      "  grad :  tensor([-0.0064,  0.0359])\n",
      "Epoch 2600, Loss 2.931554\n",
      "  params :  tensor([  5.3305, -17.0939])\n",
      "  grad :  tensor([-0.0064,  0.0359])\n",
      "Epoch 2601, Loss 2.931538\n",
      "  params :  tensor([  5.3305, -17.0943])\n",
      "  grad :  tensor([-0.0063,  0.0358])\n",
      "Epoch 2602, Loss 2.931526\n",
      "  params :  tensor([  5.3306, -17.0947])\n",
      "  grad :  tensor([-0.0063,  0.0358])\n",
      "Epoch 2603, Loss 2.931512\n",
      "  params :  tensor([  5.3307, -17.0950])\n",
      "  grad :  tensor([-0.0063,  0.0357])\n",
      "Epoch 2604, Loss 2.931499\n",
      "  params :  tensor([  5.3307, -17.0954])\n",
      "  grad :  tensor([-0.0063,  0.0356])\n",
      "Epoch 2605, Loss 2.931488\n",
      "  params :  tensor([  5.3308, -17.0957])\n",
      "  grad :  tensor([-0.0063,  0.0356])\n",
      "Epoch 2606, Loss 2.931474\n",
      "  params :  tensor([  5.3309, -17.0961])\n",
      "  grad :  tensor([-0.0063,  0.0355])\n",
      "Epoch 2607, Loss 2.931462\n",
      "  params :  tensor([  5.3309, -17.0964])\n",
      "  grad :  tensor([-0.0062,  0.0355])\n",
      "Epoch 2608, Loss 2.931448\n",
      "  params :  tensor([  5.3310, -17.0968])\n",
      "  grad :  tensor([-0.0062,  0.0354])\n",
      "Epoch 2609, Loss 2.931436\n",
      "  params :  tensor([  5.3310, -17.0971])\n",
      "  grad :  tensor([-0.0062,  0.0353])\n",
      "Epoch 2610, Loss 2.931423\n",
      "  params :  tensor([  5.3311, -17.0975])\n",
      "  grad :  tensor([-0.0062,  0.0353])\n",
      "Epoch 2611, Loss 2.931411\n",
      "  params :  tensor([  5.3312, -17.0979])\n",
      "  grad :  tensor([-0.0062,  0.0352])\n",
      "Epoch 2612, Loss 2.931397\n",
      "  params :  tensor([  5.3312, -17.0982])\n",
      "  grad :  tensor([-0.0062,  0.0352])\n",
      "Epoch 2613, Loss 2.931384\n",
      "  params :  tensor([  5.3313, -17.0986])\n",
      "  grad :  tensor([-0.0062,  0.0351])\n",
      "Epoch 2614, Loss 2.931371\n",
      "  params :  tensor([  5.3313, -17.0989])\n",
      "  grad :  tensor([-0.0062,  0.0350])\n",
      "Epoch 2615, Loss 2.931358\n",
      "  params :  tensor([  5.3314, -17.0993])\n",
      "  grad :  tensor([-0.0062,  0.0350])\n",
      "Epoch 2616, Loss 2.931346\n",
      "  params :  tensor([  5.3315, -17.0996])\n",
      "  grad :  tensor([-0.0062,  0.0349])\n",
      "Epoch 2617, Loss 2.931335\n",
      "  params :  tensor([  5.3315, -17.1000])\n",
      "  grad :  tensor([-0.0062,  0.0349])\n",
      "Epoch 2618, Loss 2.931322\n",
      "  params :  tensor([  5.3316, -17.1003])\n",
      "  grad :  tensor([-0.0062,  0.0348])\n",
      "Epoch 2619, Loss 2.931308\n",
      "  params :  tensor([  5.3317, -17.1006])\n",
      "  grad :  tensor([-0.0061,  0.0347])\n",
      "Epoch 2620, Loss 2.931296\n",
      "  params :  tensor([  5.3317, -17.1010])\n",
      "  grad :  tensor([-0.0061,  0.0347])\n",
      "Epoch 2621, Loss 2.931282\n",
      "  params :  tensor([  5.3318, -17.1013])\n",
      "  grad :  tensor([-0.0061,  0.0346])\n",
      "Epoch 2622, Loss 2.931272\n",
      "  params :  tensor([  5.3318, -17.1017])\n",
      "  grad :  tensor([-0.0061,  0.0346])\n",
      "Epoch 2623, Loss 2.931258\n",
      "  params :  tensor([  5.3319, -17.1020])\n",
      "  grad :  tensor([-0.0061,  0.0345])\n",
      "Epoch 2624, Loss 2.931245\n",
      "  params :  tensor([  5.3320, -17.1024])\n",
      "  grad :  tensor([-0.0061,  0.0344])\n",
      "Epoch 2625, Loss 2.931234\n",
      "  params :  tensor([  5.3320, -17.1027])\n",
      "  grad :  tensor([-0.0061,  0.0344])\n",
      "Epoch 2626, Loss 2.931222\n",
      "  params :  tensor([  5.3321, -17.1031])\n",
      "  grad :  tensor([-0.0061,  0.0343])\n",
      "Epoch 2627, Loss 2.931211\n",
      "  params :  tensor([  5.3321, -17.1034])\n",
      "  grad :  tensor([-0.0060,  0.0343])\n",
      "Epoch 2628, Loss 2.931196\n",
      "  params :  tensor([  5.3322, -17.1038])\n",
      "  grad :  tensor([-0.0060,  0.0342])\n",
      "Epoch 2629, Loss 2.931185\n",
      "  params :  tensor([  5.3323, -17.1041])\n",
      "  grad :  tensor([-0.0060,  0.0342])\n",
      "Epoch 2630, Loss 2.931173\n",
      "  params :  tensor([  5.3323, -17.1044])\n",
      "  grad :  tensor([-0.0060,  0.0341])\n",
      "Epoch 2631, Loss 2.931162\n",
      "  params :  tensor([  5.3324, -17.1048])\n",
      "  grad :  tensor([-0.0060,  0.0340])\n",
      "Epoch 2632, Loss 2.931149\n",
      "  params :  tensor([  5.3324, -17.1051])\n",
      "  grad :  tensor([-0.0060,  0.0340])\n",
      "Epoch 2633, Loss 2.931138\n",
      "  params :  tensor([  5.3325, -17.1055])\n",
      "  grad :  tensor([-0.0060,  0.0339])\n",
      "Epoch 2634, Loss 2.931126\n",
      "  params :  tensor([  5.3326, -17.1058])\n",
      "  grad :  tensor([-0.0060,  0.0339])\n",
      "Epoch 2635, Loss 2.931114\n",
      "  params :  tensor([  5.3326, -17.1061])\n",
      "  grad :  tensor([-0.0060,  0.0338])\n",
      "Epoch 2636, Loss 2.931101\n",
      "  params :  tensor([  5.3327, -17.1065])\n",
      "  grad :  tensor([-0.0060,  0.0337])\n",
      "Epoch 2637, Loss 2.931090\n",
      "  params :  tensor([  5.3327, -17.1068])\n",
      "  grad :  tensor([-0.0059,  0.0337])\n",
      "Epoch 2638, Loss 2.931079\n",
      "  params :  tensor([  5.3328, -17.1071])\n",
      "  grad :  tensor([-0.0059,  0.0336])\n",
      "Epoch 2639, Loss 2.931067\n",
      "  params :  tensor([  5.3329, -17.1075])\n",
      "  grad :  tensor([-0.0059,  0.0336])\n",
      "Epoch 2640, Loss 2.931054\n",
      "  params :  tensor([  5.3329, -17.1078])\n",
      "  grad :  tensor([-0.0059,  0.0335])\n",
      "Epoch 2641, Loss 2.931044\n",
      "  params :  tensor([  5.3330, -17.1081])\n",
      "  grad :  tensor([-0.0059,  0.0335])\n",
      "Epoch 2642, Loss 2.931034\n",
      "  params :  tensor([  5.3330, -17.1085])\n",
      "  grad :  tensor([-0.0059,  0.0334])\n",
      "Epoch 2643, Loss 2.931021\n",
      "  params :  tensor([  5.3331, -17.1088])\n",
      "  grad :  tensor([-0.0059,  0.0333])\n",
      "Epoch 2644, Loss 2.931010\n",
      "  params :  tensor([  5.3332, -17.1091])\n",
      "  grad :  tensor([-0.0059,  0.0333])\n",
      "Epoch 2645, Loss 2.930999\n",
      "  params :  tensor([  5.3332, -17.1095])\n",
      "  grad :  tensor([-0.0059,  0.0332])\n",
      "Epoch 2646, Loss 2.930987\n",
      "  params :  tensor([  5.3333, -17.1098])\n",
      "  grad :  tensor([-0.0059,  0.0332])\n",
      "Epoch 2647, Loss 2.930976\n",
      "  params :  tensor([  5.3333, -17.1101])\n",
      "  grad :  tensor([-0.0059,  0.0331])\n",
      "Epoch 2648, Loss 2.930964\n",
      "  params :  tensor([  5.3334, -17.1105])\n",
      "  grad :  tensor([-0.0059,  0.0331])\n",
      "Epoch 2649, Loss 2.930953\n",
      "  params :  tensor([  5.3335, -17.1108])\n",
      "  grad :  tensor([-0.0058,  0.0330])\n",
      "Epoch 2650, Loss 2.930941\n",
      "  params :  tensor([  5.3335, -17.1111])\n",
      "  grad :  tensor([-0.0058,  0.0330])\n",
      "Epoch 2651, Loss 2.930932\n",
      "  params :  tensor([  5.3336, -17.1115])\n",
      "  grad :  tensor([-0.0058,  0.0329])\n",
      "Epoch 2652, Loss 2.930921\n",
      "  params :  tensor([  5.3336, -17.1118])\n",
      "  grad :  tensor([-0.0058,  0.0328])\n",
      "Epoch 2653, Loss 2.930908\n",
      "  params :  tensor([  5.3337, -17.1121])\n",
      "  grad :  tensor([-0.0058,  0.0328])\n",
      "Epoch 2654, Loss 2.930899\n",
      "  params :  tensor([  5.3337, -17.1124])\n",
      "  grad :  tensor([-0.0058,  0.0327])\n",
      "Epoch 2655, Loss 2.930885\n",
      "  params :  tensor([  5.3338, -17.1128])\n",
      "  grad :  tensor([-0.0058,  0.0327])\n",
      "Epoch 2656, Loss 2.930876\n",
      "  params :  tensor([  5.3339, -17.1131])\n",
      "  grad :  tensor([-0.0058,  0.0326])\n",
      "Epoch 2657, Loss 2.930863\n",
      "  params :  tensor([  5.3339, -17.1134])\n",
      "  grad :  tensor([-0.0057,  0.0326])\n",
      "Epoch 2658, Loss 2.930854\n",
      "  params :  tensor([  5.3340, -17.1137])\n",
      "  grad :  tensor([-0.0057,  0.0325])\n",
      "Epoch 2659, Loss 2.930841\n",
      "  params :  tensor([  5.3340, -17.1141])\n",
      "  grad :  tensor([-0.0057,  0.0325])\n",
      "Epoch 2660, Loss 2.930833\n",
      "  params :  tensor([  5.3341, -17.1144])\n",
      "  grad :  tensor([-0.0057,  0.0324])\n",
      "Epoch 2661, Loss 2.930821\n",
      "  params :  tensor([  5.3341, -17.1147])\n",
      "  grad :  tensor([-0.0057,  0.0323])\n",
      "Epoch 2662, Loss 2.930811\n",
      "  params :  tensor([  5.3342, -17.1150])\n",
      "  grad :  tensor([-0.0057,  0.0323])\n",
      "Epoch 2663, Loss 2.930801\n",
      "  params :  tensor([  5.3343, -17.1154])\n",
      "  grad :  tensor([-0.0057,  0.0322])\n",
      "Epoch 2664, Loss 2.930788\n",
      "  params :  tensor([  5.3343, -17.1157])\n",
      "  grad :  tensor([-0.0057,  0.0322])\n",
      "Epoch 2665, Loss 2.930778\n",
      "  params :  tensor([  5.3344, -17.1160])\n",
      "  grad :  tensor([-0.0057,  0.0321])\n",
      "Epoch 2666, Loss 2.930767\n",
      "  params :  tensor([  5.3344, -17.1163])\n",
      "  grad :  tensor([-0.0057,  0.0321])\n",
      "Epoch 2667, Loss 2.930757\n",
      "  params :  tensor([  5.3345, -17.1166])\n",
      "  grad :  tensor([-0.0057,  0.0320])\n",
      "Epoch 2668, Loss 2.930746\n",
      "  params :  tensor([  5.3345, -17.1170])\n",
      "  grad :  tensor([-0.0056,  0.0320])\n",
      "Epoch 2669, Loss 2.930736\n",
      "  params :  tensor([  5.3346, -17.1173])\n",
      "  grad :  tensor([-0.0056,  0.0319])\n",
      "Epoch 2670, Loss 2.930724\n",
      "  params :  tensor([  5.3347, -17.1176])\n",
      "  grad :  tensor([-0.0056,  0.0319])\n",
      "Epoch 2671, Loss 2.930715\n",
      "  params :  tensor([  5.3347, -17.1179])\n",
      "  grad :  tensor([-0.0056,  0.0318])\n",
      "Epoch 2672, Loss 2.930704\n",
      "  params :  tensor([  5.3348, -17.1182])\n",
      "  grad :  tensor([-0.0056,  0.0317])\n",
      "Epoch 2673, Loss 2.930694\n",
      "  params :  tensor([  5.3348, -17.1186])\n",
      "  grad :  tensor([-0.0056,  0.0317])\n",
      "Epoch 2674, Loss 2.930685\n",
      "  params :  tensor([  5.3349, -17.1189])\n",
      "  grad :  tensor([-0.0056,  0.0316])\n",
      "Epoch 2675, Loss 2.930674\n",
      "  params :  tensor([  5.3349, -17.1192])\n",
      "  grad :  tensor([-0.0056,  0.0316])\n",
      "Epoch 2676, Loss 2.930663\n",
      "  params :  tensor([  5.3350, -17.1195])\n",
      "  grad :  tensor([-0.0056,  0.0315])\n",
      "Epoch 2677, Loss 2.930654\n",
      "  params :  tensor([  5.3350, -17.1198])\n",
      "  grad :  tensor([-0.0056,  0.0315])\n",
      "Epoch 2678, Loss 2.930644\n",
      "  params :  tensor([  5.3351, -17.1201])\n",
      "  grad :  tensor([-0.0055,  0.0314])\n",
      "Epoch 2679, Loss 2.930631\n",
      "  params :  tensor([  5.3352, -17.1204])\n",
      "  grad :  tensor([-0.0055,  0.0314])\n",
      "Epoch 2680, Loss 2.930621\n",
      "  params :  tensor([  5.3352, -17.1208])\n",
      "  grad :  tensor([-0.0055,  0.0313])\n",
      "Epoch 2681, Loss 2.930613\n",
      "  params :  tensor([  5.3353, -17.1211])\n",
      "  grad :  tensor([-0.0055,  0.0313])\n",
      "Epoch 2682, Loss 2.930603\n",
      "  params :  tensor([  5.3353, -17.1214])\n",
      "  grad :  tensor([-0.0055,  0.0312])\n",
      "Epoch 2683, Loss 2.930593\n",
      "  params :  tensor([  5.3354, -17.1217])\n",
      "  grad :  tensor([-0.0055,  0.0312])\n",
      "Epoch 2684, Loss 2.930582\n",
      "  params :  tensor([  5.3354, -17.1220])\n",
      "  grad :  tensor([-0.0055,  0.0311])\n",
      "Epoch 2685, Loss 2.930571\n",
      "  params :  tensor([  5.3355, -17.1223])\n",
      "  grad :  tensor([-0.0055,  0.0310])\n",
      "Epoch 2686, Loss 2.930562\n",
      "  params :  tensor([  5.3355, -17.1226])\n",
      "  grad :  tensor([-0.0055,  0.0310])\n",
      "Epoch 2687, Loss 2.930552\n",
      "  params :  tensor([  5.3356, -17.1229])\n",
      "  grad :  tensor([-0.0055,  0.0309])\n",
      "Epoch 2688, Loss 2.930543\n",
      "  params :  tensor([  5.3356, -17.1232])\n",
      "  grad :  tensor([-0.0055,  0.0309])\n",
      "Epoch 2689, Loss 2.930534\n",
      "  params :  tensor([  5.3357, -17.1236])\n",
      "  grad :  tensor([-0.0055,  0.0308])\n",
      "Epoch 2690, Loss 2.930523\n",
      "  params :  tensor([  5.3358, -17.1239])\n",
      "  grad :  tensor([-0.0054,  0.0308])\n",
      "Epoch 2691, Loss 2.930514\n",
      "  params :  tensor([  5.3358, -17.1242])\n",
      "  grad :  tensor([-0.0054,  0.0307])\n",
      "Epoch 2692, Loss 2.930502\n",
      "  params :  tensor([  5.3359, -17.1245])\n",
      "  grad :  tensor([-0.0054,  0.0307])\n",
      "Epoch 2693, Loss 2.930493\n",
      "  params :  tensor([  5.3359, -17.1248])\n",
      "  grad :  tensor([-0.0054,  0.0306])\n",
      "Epoch 2694, Loss 2.930482\n",
      "  params :  tensor([  5.3360, -17.1251])\n",
      "  grad :  tensor([-0.0054,  0.0306])\n",
      "Epoch 2695, Loss 2.930474\n",
      "  params :  tensor([  5.3360, -17.1254])\n",
      "  grad :  tensor([-0.0054,  0.0305])\n",
      "Epoch 2696, Loss 2.930464\n",
      "  params :  tensor([  5.3361, -17.1257])\n",
      "  grad :  tensor([-0.0054,  0.0305])\n",
      "Epoch 2697, Loss 2.930454\n",
      "  params :  tensor([  5.3361, -17.1260])\n",
      "  grad :  tensor([-0.0054,  0.0304])\n",
      "Epoch 2698, Loss 2.930445\n",
      "  params :  tensor([  5.3362, -17.1263])\n",
      "  grad :  tensor([-0.0054,  0.0304])\n",
      "Epoch 2699, Loss 2.930436\n",
      "  params :  tensor([  5.3362, -17.1266])\n",
      "  grad :  tensor([-0.0054,  0.0303])\n",
      "Epoch 2700, Loss 2.930426\n",
      "  params :  tensor([  5.3363, -17.1269])\n",
      "  grad :  tensor([-0.0054,  0.0303])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2701, Loss 2.930416\n",
      "  params :  tensor([  5.3364, -17.1272])\n",
      "  grad :  tensor([-0.0054,  0.0302])\n",
      "Epoch 2702, Loss 2.930408\n",
      "  params :  tensor([  5.3364, -17.1275])\n",
      "  grad :  tensor([-0.0053,  0.0302])\n",
      "Epoch 2703, Loss 2.930398\n",
      "  params :  tensor([  5.3365, -17.1278])\n",
      "  grad :  tensor([-0.0053,  0.0301])\n",
      "Epoch 2704, Loss 2.930388\n",
      "  params :  tensor([  5.3365, -17.1281])\n",
      "  grad :  tensor([-0.0053,  0.0301])\n",
      "Epoch 2705, Loss 2.930380\n",
      "  params :  tensor([  5.3366, -17.1284])\n",
      "  grad :  tensor([-0.0053,  0.0300])\n",
      "Epoch 2706, Loss 2.930370\n",
      "  params :  tensor([  5.3366, -17.1287])\n",
      "  grad :  tensor([-0.0053,  0.0300])\n",
      "Epoch 2707, Loss 2.930360\n",
      "  params :  tensor([  5.3367, -17.1290])\n",
      "  grad :  tensor([-0.0053,  0.0299])\n",
      "Epoch 2708, Loss 2.930353\n",
      "  params :  tensor([  5.3367, -17.1293])\n",
      "  grad :  tensor([-0.0053,  0.0299])\n",
      "Epoch 2709, Loss 2.930342\n",
      "  params :  tensor([  5.3368, -17.1296])\n",
      "  grad :  tensor([-0.0053,  0.0298])\n",
      "Epoch 2710, Loss 2.930335\n",
      "  params :  tensor([  5.3368, -17.1299])\n",
      "  grad :  tensor([-0.0053,  0.0298])\n",
      "Epoch 2711, Loss 2.930325\n",
      "  params :  tensor([  5.3369, -17.1302])\n",
      "  grad :  tensor([-0.0053,  0.0297])\n",
      "Epoch 2712, Loss 2.930315\n",
      "  params :  tensor([  5.3369, -17.1305])\n",
      "  grad :  tensor([-0.0053,  0.0297])\n",
      "Epoch 2713, Loss 2.930306\n",
      "  params :  tensor([  5.3370, -17.1308])\n",
      "  grad :  tensor([-0.0052,  0.0296])\n",
      "Epoch 2714, Loss 2.930298\n",
      "  params :  tensor([  5.3370, -17.1311])\n",
      "  grad :  tensor([-0.0052,  0.0296])\n",
      "Epoch 2715, Loss 2.930288\n",
      "  params :  tensor([  5.3371, -17.1314])\n",
      "  grad :  tensor([-0.0052,  0.0295])\n",
      "Epoch 2716, Loss 2.930279\n",
      "  params :  tensor([  5.3371, -17.1317])\n",
      "  grad :  tensor([-0.0052,  0.0295])\n",
      "Epoch 2717, Loss 2.930270\n",
      "  params :  tensor([  5.3372, -17.1320])\n",
      "  grad :  tensor([-0.0052,  0.0294])\n",
      "Epoch 2718, Loss 2.930262\n",
      "  params :  tensor([  5.3372, -17.1323])\n",
      "  grad :  tensor([-0.0052,  0.0294])\n",
      "Epoch 2719, Loss 2.930254\n",
      "  params :  tensor([  5.3373, -17.1326])\n",
      "  grad :  tensor([-0.0052,  0.0293])\n",
      "Epoch 2720, Loss 2.930244\n",
      "  params :  tensor([  5.3373, -17.1329])\n",
      "  grad :  tensor([-0.0052,  0.0293])\n",
      "Epoch 2721, Loss 2.930235\n",
      "  params :  tensor([  5.3374, -17.1332])\n",
      "  grad :  tensor([-0.0052,  0.0292])\n",
      "Epoch 2722, Loss 2.930226\n",
      "  params :  tensor([  5.3375, -17.1334])\n",
      "  grad :  tensor([-0.0052,  0.0292])\n",
      "Epoch 2723, Loss 2.930218\n",
      "  params :  tensor([  5.3375, -17.1337])\n",
      "  grad :  tensor([-0.0051,  0.0291])\n",
      "Epoch 2724, Loss 2.930209\n",
      "  params :  tensor([  5.3376, -17.1340])\n",
      "  grad :  tensor([-0.0051,  0.0291])\n",
      "Epoch 2725, Loss 2.930201\n",
      "  params :  tensor([  5.3376, -17.1343])\n",
      "  grad :  tensor([-0.0051,  0.0290])\n",
      "Epoch 2726, Loss 2.930190\n",
      "  params :  tensor([  5.3377, -17.1346])\n",
      "  grad :  tensor([-0.0051,  0.0290])\n",
      "Epoch 2727, Loss 2.930183\n",
      "  params :  tensor([  5.3377, -17.1349])\n",
      "  grad :  tensor([-0.0051,  0.0289])\n",
      "Epoch 2728, Loss 2.930173\n",
      "  params :  tensor([  5.3378, -17.1352])\n",
      "  grad :  tensor([-0.0051,  0.0289])\n",
      "Epoch 2729, Loss 2.930166\n",
      "  params :  tensor([  5.3378, -17.1355])\n",
      "  grad :  tensor([-0.0051,  0.0288])\n",
      "Epoch 2730, Loss 2.930156\n",
      "  params :  tensor([  5.3379, -17.1358])\n",
      "  grad :  tensor([-0.0051,  0.0288])\n",
      "Epoch 2731, Loss 2.930149\n",
      "  params :  tensor([  5.3379, -17.1360])\n",
      "  grad :  tensor([-0.0051,  0.0287])\n",
      "Epoch 2732, Loss 2.930139\n",
      "  params :  tensor([  5.3380, -17.1363])\n",
      "  grad :  tensor([-0.0051,  0.0287])\n",
      "Epoch 2733, Loss 2.930131\n",
      "  params :  tensor([  5.3380, -17.1366])\n",
      "  grad :  tensor([-0.0050,  0.0286])\n",
      "Epoch 2734, Loss 2.930123\n",
      "  params :  tensor([  5.3381, -17.1369])\n",
      "  grad :  tensor([-0.0050,  0.0286])\n",
      "Epoch 2735, Loss 2.930113\n",
      "  params :  tensor([  5.3381, -17.1372])\n",
      "  grad :  tensor([-0.0050,  0.0285])\n",
      "Epoch 2736, Loss 2.930107\n",
      "  params :  tensor([  5.3382, -17.1375])\n",
      "  grad :  tensor([-0.0051,  0.0285])\n",
      "Epoch 2737, Loss 2.930099\n",
      "  params :  tensor([  5.3382, -17.1378])\n",
      "  grad :  tensor([-0.0050,  0.0284])\n",
      "Epoch 2738, Loss 2.930090\n",
      "  params :  tensor([  5.3383, -17.1380])\n",
      "  grad :  tensor([-0.0050,  0.0284])\n",
      "Epoch 2739, Loss 2.930081\n",
      "  params :  tensor([  5.3383, -17.1383])\n",
      "  grad :  tensor([-0.0050,  0.0283])\n",
      "Epoch 2740, Loss 2.930073\n",
      "  params :  tensor([  5.3384, -17.1386])\n",
      "  grad :  tensor([-0.0050,  0.0283])\n",
      "Epoch 2741, Loss 2.930064\n",
      "  params :  tensor([  5.3384, -17.1389])\n",
      "  grad :  tensor([-0.0050,  0.0282])\n",
      "Epoch 2742, Loss 2.930056\n",
      "  params :  tensor([  5.3385, -17.1392])\n",
      "  grad :  tensor([-0.0050,  0.0282])\n",
      "Epoch 2743, Loss 2.930048\n",
      "  params :  tensor([  5.3385, -17.1395])\n",
      "  grad :  tensor([-0.0050,  0.0281])\n",
      "Epoch 2744, Loss 2.930041\n",
      "  params :  tensor([  5.3386, -17.1397])\n",
      "  grad :  tensor([-0.0050,  0.0281])\n",
      "Epoch 2745, Loss 2.930032\n",
      "  params :  tensor([  5.3386, -17.1400])\n",
      "  grad :  tensor([-0.0050,  0.0280])\n",
      "Epoch 2746, Loss 2.930022\n",
      "  params :  tensor([  5.3387, -17.1403])\n",
      "  grad :  tensor([-0.0050,  0.0280])\n",
      "Epoch 2747, Loss 2.930016\n",
      "  params :  tensor([  5.3387, -17.1406])\n",
      "  grad :  tensor([-0.0049,  0.0279])\n",
      "Epoch 2748, Loss 2.930008\n",
      "  params :  tensor([  5.3388, -17.1409])\n",
      "  grad :  tensor([-0.0049,  0.0279])\n",
      "Epoch 2749, Loss 2.930000\n",
      "  params :  tensor([  5.3388, -17.1411])\n",
      "  grad :  tensor([-0.0049,  0.0279])\n",
      "Epoch 2750, Loss 2.929992\n",
      "  params :  tensor([  5.3389, -17.1414])\n",
      "  grad :  tensor([-0.0049,  0.0278])\n",
      "Epoch 2751, Loss 2.929983\n",
      "  params :  tensor([  5.3389, -17.1417])\n",
      "  grad :  tensor([-0.0049,  0.0278])\n",
      "Epoch 2752, Loss 2.929975\n",
      "  params :  tensor([  5.3390, -17.1420])\n",
      "  grad :  tensor([-0.0049,  0.0277])\n",
      "Epoch 2753, Loss 2.929968\n",
      "  params :  tensor([  5.3390, -17.1422])\n",
      "  grad :  tensor([-0.0049,  0.0277])\n",
      "Epoch 2754, Loss 2.929960\n",
      "  params :  tensor([  5.3391, -17.1425])\n",
      "  grad :  tensor([-0.0049,  0.0276])\n",
      "Epoch 2755, Loss 2.929953\n",
      "  params :  tensor([  5.3391, -17.1428])\n",
      "  grad :  tensor([-0.0049,  0.0276])\n",
      "Epoch 2756, Loss 2.929945\n",
      "  params :  tensor([  5.3392, -17.1431])\n",
      "  grad :  tensor([-0.0049,  0.0275])\n",
      "Epoch 2757, Loss 2.929936\n",
      "  params :  tensor([  5.3392, -17.1433])\n",
      "  grad :  tensor([-0.0049,  0.0275])\n",
      "Epoch 2758, Loss 2.929929\n",
      "  params :  tensor([  5.3392, -17.1436])\n",
      "  grad :  tensor([-0.0049,  0.0274])\n",
      "Epoch 2759, Loss 2.929921\n",
      "  params :  tensor([  5.3393, -17.1439])\n",
      "  grad :  tensor([-0.0048,  0.0274])\n",
      "Epoch 2760, Loss 2.929914\n",
      "  params :  tensor([  5.3393, -17.1442])\n",
      "  grad :  tensor([-0.0049,  0.0273])\n",
      "Epoch 2761, Loss 2.929905\n",
      "  params :  tensor([  5.3394, -17.1444])\n",
      "  grad :  tensor([-0.0048,  0.0273])\n",
      "Epoch 2762, Loss 2.929896\n",
      "  params :  tensor([  5.3394, -17.1447])\n",
      "  grad :  tensor([-0.0048,  0.0272])\n",
      "Epoch 2763, Loss 2.929891\n",
      "  params :  tensor([  5.3395, -17.1450])\n",
      "  grad :  tensor([-0.0048,  0.0272])\n",
      "Epoch 2764, Loss 2.929882\n",
      "  params :  tensor([  5.3395, -17.1453])\n",
      "  grad :  tensor([-0.0048,  0.0271])\n",
      "Epoch 2765, Loss 2.929875\n",
      "  params :  tensor([  5.3396, -17.1455])\n",
      "  grad :  tensor([-0.0048,  0.0271])\n",
      "Epoch 2766, Loss 2.929868\n",
      "  params :  tensor([  5.3396, -17.1458])\n",
      "  grad :  tensor([-0.0048,  0.0271])\n",
      "Epoch 2767, Loss 2.929859\n",
      "  params :  tensor([  5.3397, -17.1461])\n",
      "  grad :  tensor([-0.0048,  0.0270])\n",
      "Epoch 2768, Loss 2.929852\n",
      "  params :  tensor([  5.3397, -17.1463])\n",
      "  grad :  tensor([-0.0048,  0.0270])\n",
      "Epoch 2769, Loss 2.929845\n",
      "  params :  tensor([  5.3398, -17.1466])\n",
      "  grad :  tensor([-0.0048,  0.0269])\n",
      "Epoch 2770, Loss 2.929838\n",
      "  params :  tensor([  5.3398, -17.1469])\n",
      "  grad :  tensor([-0.0047,  0.0269])\n",
      "Epoch 2771, Loss 2.929830\n",
      "  params :  tensor([  5.3399, -17.1471])\n",
      "  grad :  tensor([-0.0047,  0.0268])\n",
      "Epoch 2772, Loss 2.929822\n",
      "  params :  tensor([  5.3399, -17.1474])\n",
      "  grad :  tensor([-0.0047,  0.0268])\n",
      "Epoch 2773, Loss 2.929816\n",
      "  params :  tensor([  5.3400, -17.1477])\n",
      "  grad :  tensor([-0.0047,  0.0267])\n",
      "Epoch 2774, Loss 2.929807\n",
      "  params :  tensor([  5.3400, -17.1479])\n",
      "  grad :  tensor([-0.0047,  0.0267])\n",
      "Epoch 2775, Loss 2.929800\n",
      "  params :  tensor([  5.3401, -17.1482])\n",
      "  grad :  tensor([-0.0047,  0.0266])\n",
      "Epoch 2776, Loss 2.929794\n",
      "  params :  tensor([  5.3401, -17.1485])\n",
      "  grad :  tensor([-0.0047,  0.0266])\n",
      "Epoch 2777, Loss 2.929786\n",
      "  params :  tensor([  5.3402, -17.1487])\n",
      "  grad :  tensor([-0.0047,  0.0266])\n",
      "Epoch 2778, Loss 2.929778\n",
      "  params :  tensor([  5.3402, -17.1490])\n",
      "  grad :  tensor([-0.0047,  0.0265])\n",
      "Epoch 2779, Loss 2.929771\n",
      "  params :  tensor([  5.3402, -17.1493])\n",
      "  grad :  tensor([-0.0047,  0.0265])\n",
      "Epoch 2780, Loss 2.929765\n",
      "  params :  tensor([  5.3403, -17.1495])\n",
      "  grad :  tensor([-0.0047,  0.0264])\n",
      "Epoch 2781, Loss 2.929757\n",
      "  params :  tensor([  5.3403, -17.1498])\n",
      "  grad :  tensor([-0.0047,  0.0264])\n",
      "Epoch 2782, Loss 2.929750\n",
      "  params :  tensor([  5.3404, -17.1501])\n",
      "  grad :  tensor([-0.0046,  0.0263])\n",
      "Epoch 2783, Loss 2.929743\n",
      "  params :  tensor([  5.3404, -17.1503])\n",
      "  grad :  tensor([-0.0046,  0.0263])\n",
      "Epoch 2784, Loss 2.929735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3405, -17.1506])\n",
      "  grad :  tensor([-0.0046,  0.0262])\n",
      "Epoch 2785, Loss 2.929729\n",
      "  params :  tensor([  5.3405, -17.1508])\n",
      "  grad :  tensor([-0.0047,  0.0262])\n",
      "Epoch 2786, Loss 2.929722\n",
      "  params :  tensor([  5.3406, -17.1511])\n",
      "  grad :  tensor([-0.0046,  0.0262])\n",
      "Epoch 2787, Loss 2.929714\n",
      "  params :  tensor([  5.3406, -17.1514])\n",
      "  grad :  tensor([-0.0046,  0.0261])\n",
      "Epoch 2788, Loss 2.929707\n",
      "  params :  tensor([  5.3407, -17.1516])\n",
      "  grad :  tensor([-0.0046,  0.0261])\n",
      "Epoch 2789, Loss 2.929701\n",
      "  params :  tensor([  5.3407, -17.1519])\n",
      "  grad :  tensor([-0.0046,  0.0260])\n",
      "Epoch 2790, Loss 2.929692\n",
      "  params :  tensor([  5.3408, -17.1522])\n",
      "  grad :  tensor([-0.0046,  0.0260])\n",
      "Epoch 2791, Loss 2.929685\n",
      "  params :  tensor([  5.3408, -17.1524])\n",
      "  grad :  tensor([-0.0046,  0.0259])\n",
      "Epoch 2792, Loss 2.929681\n",
      "  params :  tensor([  5.3408, -17.1527])\n",
      "  grad :  tensor([-0.0046,  0.0259])\n",
      "Epoch 2793, Loss 2.929672\n",
      "  params :  tensor([  5.3409, -17.1529])\n",
      "  grad :  tensor([-0.0046,  0.0258])\n",
      "Epoch 2794, Loss 2.929666\n",
      "  params :  tensor([  5.3409, -17.1532])\n",
      "  grad :  tensor([-0.0046,  0.0258])\n",
      "Epoch 2795, Loss 2.929659\n",
      "  params :  tensor([  5.3410, -17.1534])\n",
      "  grad :  tensor([-0.0045,  0.0258])\n",
      "Epoch 2796, Loss 2.929653\n",
      "  params :  tensor([  5.3410, -17.1537])\n",
      "  grad :  tensor([-0.0045,  0.0257])\n",
      "Epoch 2797, Loss 2.929646\n",
      "  params :  tensor([  5.3411, -17.1540])\n",
      "  grad :  tensor([-0.0045,  0.0257])\n",
      "Epoch 2798, Loss 2.929638\n",
      "  params :  tensor([  5.3411, -17.1542])\n",
      "  grad :  tensor([-0.0045,  0.0256])\n",
      "Epoch 2799, Loss 2.929632\n",
      "  params :  tensor([  5.3412, -17.1545])\n",
      "  grad :  tensor([-0.0045,  0.0256])\n",
      "Epoch 2800, Loss 2.929626\n",
      "  params :  tensor([  5.3412, -17.1547])\n",
      "  grad :  tensor([-0.0045,  0.0255])\n",
      "Epoch 2801, Loss 2.929620\n",
      "  params :  tensor([  5.3413, -17.1550])\n",
      "  grad :  tensor([-0.0045,  0.0255])\n",
      "Epoch 2802, Loss 2.929611\n",
      "  params :  tensor([  5.3413, -17.1552])\n",
      "  grad :  tensor([-0.0045,  0.0254])\n",
      "Epoch 2803, Loss 2.929605\n",
      "  params :  tensor([  5.3413, -17.1555])\n",
      "  grad :  tensor([-0.0045,  0.0254])\n",
      "Epoch 2804, Loss 2.929600\n",
      "  params :  tensor([  5.3414, -17.1557])\n",
      "  grad :  tensor([-0.0045,  0.0254])\n",
      "Epoch 2805, Loss 2.929592\n",
      "  params :  tensor([  5.3414, -17.1560])\n",
      "  grad :  tensor([-0.0045,  0.0253])\n",
      "Epoch 2806, Loss 2.929586\n",
      "  params :  tensor([  5.3415, -17.1562])\n",
      "  grad :  tensor([-0.0045,  0.0253])\n",
      "Epoch 2807, Loss 2.929579\n",
      "  params :  tensor([  5.3415, -17.1565])\n",
      "  grad :  tensor([-0.0045,  0.0252])\n",
      "Epoch 2808, Loss 2.929572\n",
      "  params :  tensor([  5.3416, -17.1568])\n",
      "  grad :  tensor([-0.0044,  0.0252])\n",
      "Epoch 2809, Loss 2.929566\n",
      "  params :  tensor([  5.3416, -17.1570])\n",
      "  grad :  tensor([-0.0044,  0.0251])\n",
      "Epoch 2810, Loss 2.929559\n",
      "  params :  tensor([  5.3417, -17.1573])\n",
      "  grad :  tensor([-0.0044,  0.0251])\n",
      "Epoch 2811, Loss 2.929551\n",
      "  params :  tensor([  5.3417, -17.1575])\n",
      "  grad :  tensor([-0.0044,  0.0251])\n",
      "Epoch 2812, Loss 2.929545\n",
      "  params :  tensor([  5.3417, -17.1578])\n",
      "  grad :  tensor([-0.0044,  0.0250])\n",
      "Epoch 2813, Loss 2.929540\n",
      "  params :  tensor([  5.3418, -17.1580])\n",
      "  grad :  tensor([-0.0044,  0.0250])\n",
      "Epoch 2814, Loss 2.929533\n",
      "  params :  tensor([  5.3418, -17.1583])\n",
      "  grad :  tensor([-0.0044,  0.0249])\n",
      "Epoch 2815, Loss 2.929528\n",
      "  params :  tensor([  5.3419, -17.1585])\n",
      "  grad :  tensor([-0.0044,  0.0249])\n",
      "Epoch 2816, Loss 2.929521\n",
      "  params :  tensor([  5.3419, -17.1588])\n",
      "  grad :  tensor([-0.0044,  0.0249])\n",
      "Epoch 2817, Loss 2.929513\n",
      "  params :  tensor([  5.3420, -17.1590])\n",
      "  grad :  tensor([-0.0044,  0.0248])\n",
      "Epoch 2818, Loss 2.929507\n",
      "  params :  tensor([  5.3420, -17.1592])\n",
      "  grad :  tensor([-0.0043,  0.0248])\n",
      "Epoch 2819, Loss 2.929501\n",
      "  params :  tensor([  5.3421, -17.1595])\n",
      "  grad :  tensor([-0.0044,  0.0247])\n",
      "Epoch 2820, Loss 2.929496\n",
      "  params :  tensor([  5.3421, -17.1597])\n",
      "  grad :  tensor([-0.0044,  0.0247])\n",
      "Epoch 2821, Loss 2.929489\n",
      "  params :  tensor([  5.3421, -17.1600])\n",
      "  grad :  tensor([-0.0044,  0.0246])\n",
      "Epoch 2822, Loss 2.929482\n",
      "  params :  tensor([  5.3422, -17.1602])\n",
      "  grad :  tensor([-0.0043,  0.0246])\n",
      "Epoch 2823, Loss 2.929476\n",
      "  params :  tensor([  5.3422, -17.1605])\n",
      "  grad :  tensor([-0.0043,  0.0246])\n",
      "Epoch 2824, Loss 2.929471\n",
      "  params :  tensor([  5.3423, -17.1607])\n",
      "  grad :  tensor([-0.0043,  0.0245])\n",
      "Epoch 2825, Loss 2.929463\n",
      "  params :  tensor([  5.3423, -17.1610])\n",
      "  grad :  tensor([-0.0043,  0.0245])\n",
      "Epoch 2826, Loss 2.929458\n",
      "  params :  tensor([  5.3424, -17.1612])\n",
      "  grad :  tensor([-0.0043,  0.0244])\n",
      "Epoch 2827, Loss 2.929452\n",
      "  params :  tensor([  5.3424, -17.1615])\n",
      "  grad :  tensor([-0.0043,  0.0244])\n",
      "Epoch 2828, Loss 2.929445\n",
      "  params :  tensor([  5.3424, -17.1617])\n",
      "  grad :  tensor([-0.0043,  0.0243])\n",
      "Epoch 2829, Loss 2.929439\n",
      "  params :  tensor([  5.3425, -17.1619])\n",
      "  grad :  tensor([-0.0043,  0.0243])\n",
      "Epoch 2830, Loss 2.929433\n",
      "  params :  tensor([  5.3425, -17.1622])\n",
      "  grad :  tensor([-0.0043,  0.0243])\n",
      "Epoch 2831, Loss 2.929427\n",
      "  params :  tensor([  5.3426, -17.1624])\n",
      "  grad :  tensor([-0.0043,  0.0242])\n",
      "Epoch 2832, Loss 2.929421\n",
      "  params :  tensor([  5.3426, -17.1627])\n",
      "  grad :  tensor([-0.0043,  0.0242])\n",
      "Epoch 2833, Loss 2.929415\n",
      "  params :  tensor([  5.3427, -17.1629])\n",
      "  grad :  tensor([-0.0043,  0.0241])\n",
      "Epoch 2834, Loss 2.929409\n",
      "  params :  tensor([  5.3427, -17.1632])\n",
      "  grad :  tensor([-0.0043,  0.0241])\n",
      "Epoch 2835, Loss 2.929404\n",
      "  params :  tensor([  5.3427, -17.1634])\n",
      "  grad :  tensor([-0.0043,  0.0241])\n",
      "Epoch 2836, Loss 2.929396\n",
      "  params :  tensor([  5.3428, -17.1636])\n",
      "  grad :  tensor([-0.0042,  0.0240])\n",
      "Epoch 2837, Loss 2.929391\n",
      "  params :  tensor([  5.3428, -17.1639])\n",
      "  grad :  tensor([-0.0042,  0.0240])\n",
      "Epoch 2838, Loss 2.929383\n",
      "  params :  tensor([  5.3429, -17.1641])\n",
      "  grad :  tensor([-0.0042,  0.0239])\n",
      "Epoch 2839, Loss 2.929380\n",
      "  params :  tensor([  5.3429, -17.1644])\n",
      "  grad :  tensor([-0.0042,  0.0239])\n",
      "Epoch 2840, Loss 2.929373\n",
      "  params :  tensor([  5.3430, -17.1646])\n",
      "  grad :  tensor([-0.0042,  0.0239])\n",
      "Epoch 2841, Loss 2.929368\n",
      "  params :  tensor([  5.3430, -17.1648])\n",
      "  grad :  tensor([-0.0042,  0.0238])\n",
      "Epoch 2842, Loss 2.929361\n",
      "  params :  tensor([  5.3430, -17.1651])\n",
      "  grad :  tensor([-0.0042,  0.0238])\n",
      "Epoch 2843, Loss 2.929356\n",
      "  params :  tensor([  5.3431, -17.1653])\n",
      "  grad :  tensor([-0.0042,  0.0237])\n",
      "Epoch 2844, Loss 2.929351\n",
      "  params :  tensor([  5.3431, -17.1655])\n",
      "  grad :  tensor([-0.0042,  0.0237])\n",
      "Epoch 2845, Loss 2.929344\n",
      "  params :  tensor([  5.3432, -17.1658])\n",
      "  grad :  tensor([-0.0042,  0.0237])\n",
      "Epoch 2846, Loss 2.929338\n",
      "  params :  tensor([  5.3432, -17.1660])\n",
      "  grad :  tensor([-0.0042,  0.0236])\n",
      "Epoch 2847, Loss 2.929332\n",
      "  params :  tensor([  5.3432, -17.1662])\n",
      "  grad :  tensor([-0.0042,  0.0236])\n",
      "Epoch 2848, Loss 2.929328\n",
      "  params :  tensor([  5.3433, -17.1665])\n",
      "  grad :  tensor([-0.0042,  0.0235])\n",
      "Epoch 2849, Loss 2.929321\n",
      "  params :  tensor([  5.3433, -17.1667])\n",
      "  grad :  tensor([-0.0041,  0.0235])\n",
      "Epoch 2850, Loss 2.929316\n",
      "  params :  tensor([  5.3434, -17.1670])\n",
      "  grad :  tensor([-0.0041,  0.0235])\n",
      "Epoch 2851, Loss 2.929309\n",
      "  params :  tensor([  5.3434, -17.1672])\n",
      "  grad :  tensor([-0.0041,  0.0234])\n",
      "Epoch 2852, Loss 2.929304\n",
      "  params :  tensor([  5.3435, -17.1674])\n",
      "  grad :  tensor([-0.0041,  0.0234])\n",
      "Epoch 2853, Loss 2.929300\n",
      "  params :  tensor([  5.3435, -17.1677])\n",
      "  grad :  tensor([-0.0041,  0.0233])\n",
      "Epoch 2854, Loss 2.929293\n",
      "  params :  tensor([  5.3435, -17.1679])\n",
      "  grad :  tensor([-0.0041,  0.0233])\n",
      "Epoch 2855, Loss 2.929288\n",
      "  params :  tensor([  5.3436, -17.1681])\n",
      "  grad :  tensor([-0.0041,  0.0233])\n",
      "Epoch 2856, Loss 2.929282\n",
      "  params :  tensor([  5.3436, -17.1684])\n",
      "  grad :  tensor([-0.0041,  0.0232])\n",
      "Epoch 2857, Loss 2.929277\n",
      "  params :  tensor([  5.3437, -17.1686])\n",
      "  grad :  tensor([-0.0041,  0.0232])\n",
      "Epoch 2858, Loss 2.929271\n",
      "  params :  tensor([  5.3437, -17.1688])\n",
      "  grad :  tensor([-0.0041,  0.0231])\n",
      "Epoch 2859, Loss 2.929266\n",
      "  params :  tensor([  5.3437, -17.1690])\n",
      "  grad :  tensor([-0.0041,  0.0231])\n",
      "Epoch 2860, Loss 2.929260\n",
      "  params :  tensor([  5.3438, -17.1693])\n",
      "  grad :  tensor([-0.0041,  0.0231])\n",
      "Epoch 2861, Loss 2.929255\n",
      "  params :  tensor([  5.3438, -17.1695])\n",
      "  grad :  tensor([-0.0041,  0.0230])\n",
      "Epoch 2862, Loss 2.929250\n",
      "  params :  tensor([  5.3439, -17.1697])\n",
      "  grad :  tensor([-0.0041,  0.0230])\n",
      "Epoch 2863, Loss 2.929244\n",
      "  params :  tensor([  5.3439, -17.1700])\n",
      "  grad :  tensor([-0.0040,  0.0229])\n",
      "Epoch 2864, Loss 2.929238\n",
      "  params :  tensor([  5.3439, -17.1702])\n",
      "  grad :  tensor([-0.0040,  0.0229])\n",
      "Epoch 2865, Loss 2.929234\n",
      "  params :  tensor([  5.3440, -17.1704])\n",
      "  grad :  tensor([-0.0040,  0.0229])\n",
      "Epoch 2866, Loss 2.929228\n",
      "  params :  tensor([  5.3440, -17.1707])\n",
      "  grad :  tensor([-0.0040,  0.0228])\n",
      "Epoch 2867, Loss 2.929222\n",
      "  params :  tensor([  5.3441, -17.1709])\n",
      "  grad :  tensor([-0.0040,  0.0228])\n",
      "Epoch 2868, Loss 2.929217\n",
      "  params :  tensor([  5.3441, -17.1711])\n",
      "  grad :  tensor([-0.0040,  0.0227])\n",
      "Epoch 2869, Loss 2.929211\n",
      "  params :  tensor([  5.3441, -17.1713])\n",
      "  grad :  tensor([-0.0040,  0.0227])\n",
      "Epoch 2870, Loss 2.929208\n",
      "  params :  tensor([  5.3442, -17.1716])\n",
      "  grad :  tensor([-0.0040,  0.0227])\n",
      "Epoch 2871, Loss 2.929201\n",
      "  params :  tensor([  5.3442, -17.1718])\n",
      "  grad :  tensor([-0.0040,  0.0226])\n",
      "Epoch 2872, Loss 2.929195\n",
      "  params :  tensor([  5.3443, -17.1720])\n",
      "  grad :  tensor([-0.0040,  0.0226])\n",
      "Epoch 2873, Loss 2.929191\n",
      "  params :  tensor([  5.3443, -17.1722])\n",
      "  grad :  tensor([-0.0040,  0.0226])\n",
      "Epoch 2874, Loss 2.929185\n",
      "  params :  tensor([  5.3443, -17.1725])\n",
      "  grad :  tensor([-0.0040,  0.0225])\n",
      "Epoch 2875, Loss 2.929180\n",
      "  params :  tensor([  5.3444, -17.1727])\n",
      "  grad :  tensor([-0.0040,  0.0225])\n",
      "Epoch 2876, Loss 2.929175\n",
      "  params :  tensor([  5.3444, -17.1729])\n",
      "  grad :  tensor([-0.0040,  0.0224])\n",
      "Epoch 2877, Loss 2.929170\n",
      "  params :  tensor([  5.3445, -17.1731])\n",
      "  grad :  tensor([-0.0040,  0.0224])\n",
      "Epoch 2878, Loss 2.929165\n",
      "  params :  tensor([  5.3445, -17.1734])\n",
      "  grad :  tensor([-0.0040,  0.0224])\n",
      "Epoch 2879, Loss 2.929160\n",
      "  params :  tensor([  5.3445, -17.1736])\n",
      "  grad :  tensor([-0.0039,  0.0223])\n",
      "Epoch 2880, Loss 2.929155\n",
      "  params :  tensor([  5.3446, -17.1738])\n",
      "  grad :  tensor([-0.0039,  0.0223])\n",
      "Epoch 2881, Loss 2.929149\n",
      "  params :  tensor([  5.3446, -17.1740])\n",
      "  grad :  tensor([-0.0039,  0.0223])\n",
      "Epoch 2882, Loss 2.929143\n",
      "  params :  tensor([  5.3447, -17.1742])\n",
      "  grad :  tensor([-0.0039,  0.0222])\n",
      "Epoch 2883, Loss 2.929139\n",
      "  params :  tensor([  5.3447, -17.1745])\n",
      "  grad :  tensor([-0.0039,  0.0222])\n",
      "Epoch 2884, Loss 2.929133\n",
      "  params :  tensor([  5.3447, -17.1747])\n",
      "  grad :  tensor([-0.0039,  0.0221])\n",
      "Epoch 2885, Loss 2.929128\n",
      "  params :  tensor([  5.3448, -17.1749])\n",
      "  grad :  tensor([-0.0039,  0.0221])\n",
      "Epoch 2886, Loss 2.929122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3448, -17.1751])\n",
      "  grad :  tensor([-0.0039,  0.0221])\n",
      "Epoch 2887, Loss 2.929119\n",
      "  params :  tensor([  5.3449, -17.1754])\n",
      "  grad :  tensor([-0.0039,  0.0220])\n",
      "Epoch 2888, Loss 2.929113\n",
      "  params :  tensor([  5.3449, -17.1756])\n",
      "  grad :  tensor([-0.0039,  0.0220])\n",
      "Epoch 2889, Loss 2.929108\n",
      "  params :  tensor([  5.3449, -17.1758])\n",
      "  grad :  tensor([-0.0039,  0.0220])\n",
      "Epoch 2890, Loss 2.929104\n",
      "  params :  tensor([  5.3450, -17.1760])\n",
      "  grad :  tensor([-0.0039,  0.0219])\n",
      "Epoch 2891, Loss 2.929099\n",
      "  params :  tensor([  5.3450, -17.1762])\n",
      "  grad :  tensor([-0.0039,  0.0219])\n",
      "Epoch 2892, Loss 2.929093\n",
      "  params :  tensor([  5.3450, -17.1764])\n",
      "  grad :  tensor([-0.0039,  0.0218])\n",
      "Epoch 2893, Loss 2.929088\n",
      "  params :  tensor([  5.3451, -17.1767])\n",
      "  grad :  tensor([-0.0039,  0.0218])\n",
      "Epoch 2894, Loss 2.929083\n",
      "  params :  tensor([  5.3451, -17.1769])\n",
      "  grad :  tensor([-0.0038,  0.0218])\n",
      "Epoch 2895, Loss 2.929079\n",
      "  params :  tensor([  5.3452, -17.1771])\n",
      "  grad :  tensor([-0.0038,  0.0217])\n",
      "Epoch 2896, Loss 2.929074\n",
      "  params :  tensor([  5.3452, -17.1773])\n",
      "  grad :  tensor([-0.0038,  0.0217])\n",
      "Epoch 2897, Loss 2.929069\n",
      "  params :  tensor([  5.3452, -17.1775])\n",
      "  grad :  tensor([-0.0038,  0.0217])\n",
      "Epoch 2898, Loss 2.929065\n",
      "  params :  tensor([  5.3453, -17.1777])\n",
      "  grad :  tensor([-0.0038,  0.0216])\n",
      "Epoch 2899, Loss 2.929058\n",
      "  params :  tensor([  5.3453, -17.1780])\n",
      "  grad :  tensor([-0.0038,  0.0216])\n",
      "Epoch 2900, Loss 2.929054\n",
      "  params :  tensor([  5.3454, -17.1782])\n",
      "  grad :  tensor([-0.0038,  0.0215])\n",
      "Epoch 2901, Loss 2.929050\n",
      "  params :  tensor([  5.3454, -17.1784])\n",
      "  grad :  tensor([-0.0038,  0.0215])\n",
      "Epoch 2902, Loss 2.929044\n",
      "  params :  tensor([  5.3454, -17.1786])\n",
      "  grad :  tensor([-0.0038,  0.0215])\n",
      "Epoch 2903, Loss 2.929041\n",
      "  params :  tensor([  5.3455, -17.1788])\n",
      "  grad :  tensor([-0.0038,  0.0214])\n",
      "Epoch 2904, Loss 2.929036\n",
      "  params :  tensor([  5.3455, -17.1790])\n",
      "  grad :  tensor([-0.0038,  0.0214])\n",
      "Epoch 2905, Loss 2.929031\n",
      "  params :  tensor([  5.3455, -17.1793])\n",
      "  grad :  tensor([-0.0038,  0.0214])\n",
      "Epoch 2906, Loss 2.929025\n",
      "  params :  tensor([  5.3456, -17.1795])\n",
      "  grad :  tensor([-0.0038,  0.0213])\n",
      "Epoch 2907, Loss 2.929021\n",
      "  params :  tensor([  5.3456, -17.1797])\n",
      "  grad :  tensor([-0.0038,  0.0213])\n",
      "Epoch 2908, Loss 2.929017\n",
      "  params :  tensor([  5.3457, -17.1799])\n",
      "  grad :  tensor([-0.0037,  0.0213])\n",
      "Epoch 2909, Loss 2.929012\n",
      "  params :  tensor([  5.3457, -17.1801])\n",
      "  grad :  tensor([-0.0037,  0.0212])\n",
      "Epoch 2910, Loss 2.929007\n",
      "  params :  tensor([  5.3457, -17.1803])\n",
      "  grad :  tensor([-0.0037,  0.0212])\n",
      "Epoch 2911, Loss 2.929003\n",
      "  params :  tensor([  5.3458, -17.1805])\n",
      "  grad :  tensor([-0.0037,  0.0211])\n",
      "Epoch 2912, Loss 2.928999\n",
      "  params :  tensor([  5.3458, -17.1807])\n",
      "  grad :  tensor([-0.0037,  0.0211])\n",
      "Epoch 2913, Loss 2.928993\n",
      "  params :  tensor([  5.3458, -17.1809])\n",
      "  grad :  tensor([-0.0037,  0.0211])\n",
      "Epoch 2914, Loss 2.928989\n",
      "  params :  tensor([  5.3459, -17.1812])\n",
      "  grad :  tensor([-0.0037,  0.0210])\n",
      "Epoch 2915, Loss 2.928985\n",
      "  params :  tensor([  5.3459, -17.1814])\n",
      "  grad :  tensor([-0.0037,  0.0210])\n",
      "Epoch 2916, Loss 2.928980\n",
      "  params :  tensor([  5.3460, -17.1816])\n",
      "  grad :  tensor([-0.0037,  0.0210])\n",
      "Epoch 2917, Loss 2.928976\n",
      "  params :  tensor([  5.3460, -17.1818])\n",
      "  grad :  tensor([-0.0037,  0.0209])\n",
      "Epoch 2918, Loss 2.928971\n",
      "  params :  tensor([  5.3460, -17.1820])\n",
      "  grad :  tensor([-0.0037,  0.0209])\n",
      "Epoch 2919, Loss 2.928967\n",
      "  params :  tensor([  5.3461, -17.1822])\n",
      "  grad :  tensor([-0.0037,  0.0209])\n",
      "Epoch 2920, Loss 2.928962\n",
      "  params :  tensor([  5.3461, -17.1824])\n",
      "  grad :  tensor([-0.0037,  0.0208])\n",
      "Epoch 2921, Loss 2.928958\n",
      "  params :  tensor([  5.3461, -17.1826])\n",
      "  grad :  tensor([-0.0037,  0.0208])\n",
      "Epoch 2922, Loss 2.928953\n",
      "  params :  tensor([  5.3462, -17.1828])\n",
      "  grad :  tensor([-0.0037,  0.0208])\n",
      "Epoch 2923, Loss 2.928947\n",
      "  params :  tensor([  5.3462, -17.1830])\n",
      "  grad :  tensor([-0.0036,  0.0207])\n",
      "Epoch 2924, Loss 2.928943\n",
      "  params :  tensor([  5.3462, -17.1832])\n",
      "  grad :  tensor([-0.0037,  0.0207])\n",
      "Epoch 2925, Loss 2.928940\n",
      "  params :  tensor([  5.3463, -17.1834])\n",
      "  grad :  tensor([-0.0036,  0.0206])\n",
      "Epoch 2926, Loss 2.928935\n",
      "  params :  tensor([  5.3463, -17.1837])\n",
      "  grad :  tensor([-0.0036,  0.0206])\n",
      "Epoch 2927, Loss 2.928932\n",
      "  params :  tensor([  5.3464, -17.1839])\n",
      "  grad :  tensor([-0.0036,  0.0206])\n",
      "Epoch 2928, Loss 2.928926\n",
      "  params :  tensor([  5.3464, -17.1841])\n",
      "  grad :  tensor([-0.0036,  0.0205])\n",
      "Epoch 2929, Loss 2.928923\n",
      "  params :  tensor([  5.3464, -17.1843])\n",
      "  grad :  tensor([-0.0036,  0.0205])\n",
      "Epoch 2930, Loss 2.928919\n",
      "  params :  tensor([  5.3465, -17.1845])\n",
      "  grad :  tensor([-0.0036,  0.0205])\n",
      "Epoch 2931, Loss 2.928913\n",
      "  params :  tensor([  5.3465, -17.1847])\n",
      "  grad :  tensor([-0.0036,  0.0204])\n",
      "Epoch 2932, Loss 2.928909\n",
      "  params :  tensor([  5.3465, -17.1849])\n",
      "  grad :  tensor([-0.0036,  0.0204])\n",
      "Epoch 2933, Loss 2.928904\n",
      "  params :  tensor([  5.3466, -17.1851])\n",
      "  grad :  tensor([-0.0036,  0.0204])\n",
      "Epoch 2934, Loss 2.928902\n",
      "  params :  tensor([  5.3466, -17.1853])\n",
      "  grad :  tensor([-0.0036,  0.0203])\n",
      "Epoch 2935, Loss 2.928897\n",
      "  params :  tensor([  5.3466, -17.1855])\n",
      "  grad :  tensor([-0.0036,  0.0203])\n",
      "Epoch 2936, Loss 2.928893\n",
      "  params :  tensor([  5.3467, -17.1857])\n",
      "  grad :  tensor([-0.0036,  0.0203])\n",
      "Epoch 2937, Loss 2.928887\n",
      "  params :  tensor([  5.3467, -17.1859])\n",
      "  grad :  tensor([-0.0036,  0.0202])\n",
      "Epoch 2938, Loss 2.928883\n",
      "  params :  tensor([  5.3468, -17.1861])\n",
      "  grad :  tensor([-0.0035,  0.0202])\n",
      "Epoch 2939, Loss 2.928880\n",
      "  params :  tensor([  5.3468, -17.1863])\n",
      "  grad :  tensor([-0.0036,  0.0202])\n",
      "Epoch 2940, Loss 2.928878\n",
      "  params :  tensor([  5.3468, -17.1865])\n",
      "  grad :  tensor([-0.0036,  0.0201])\n",
      "Epoch 2941, Loss 2.928871\n",
      "  params :  tensor([  5.3469, -17.1867])\n",
      "  grad :  tensor([-0.0035,  0.0201])\n",
      "Epoch 2942, Loss 2.928867\n",
      "  params :  tensor([  5.3469, -17.1869])\n",
      "  grad :  tensor([-0.0035,  0.0201])\n",
      "Epoch 2943, Loss 2.928864\n",
      "  params :  tensor([  5.3469, -17.1871])\n",
      "  grad :  tensor([-0.0035,  0.0200])\n",
      "Epoch 2944, Loss 2.928860\n",
      "  params :  tensor([  5.3470, -17.1873])\n",
      "  grad :  tensor([-0.0035,  0.0200])\n",
      "Epoch 2945, Loss 2.928855\n",
      "  params :  tensor([  5.3470, -17.1875])\n",
      "  grad :  tensor([-0.0035,  0.0200])\n",
      "Epoch 2946, Loss 2.928850\n",
      "  params :  tensor([  5.3470, -17.1877])\n",
      "  grad :  tensor([-0.0035,  0.0199])\n",
      "Epoch 2947, Loss 2.928845\n",
      "  params :  tensor([  5.3471, -17.1879])\n",
      "  grad :  tensor([-0.0035,  0.0199])\n",
      "Epoch 2948, Loss 2.928843\n",
      "  params :  tensor([  5.3471, -17.1881])\n",
      "  grad :  tensor([-0.0035,  0.0199])\n",
      "Epoch 2949, Loss 2.928838\n",
      "  params :  tensor([  5.3471, -17.1883])\n",
      "  grad :  tensor([-0.0035,  0.0198])\n",
      "Epoch 2950, Loss 2.928833\n",
      "  params :  tensor([  5.3472, -17.1885])\n",
      "  grad :  tensor([-0.0035,  0.0198])\n",
      "Epoch 2951, Loss 2.928830\n",
      "  params :  tensor([  5.3472, -17.1887])\n",
      "  grad :  tensor([-0.0035,  0.0198])\n",
      "Epoch 2952, Loss 2.928826\n",
      "  params :  tensor([  5.3472, -17.1889])\n",
      "  grad :  tensor([-0.0035,  0.0197])\n",
      "Epoch 2953, Loss 2.928823\n",
      "  params :  tensor([  5.3473, -17.1891])\n",
      "  grad :  tensor([-0.0035,  0.0197])\n",
      "Epoch 2954, Loss 2.928818\n",
      "  params :  tensor([  5.3473, -17.1893])\n",
      "  grad :  tensor([-0.0035,  0.0197])\n",
      "Epoch 2955, Loss 2.928816\n",
      "  params :  tensor([  5.3474, -17.1895])\n",
      "  grad :  tensor([-0.0035,  0.0196])\n",
      "Epoch 2956, Loss 2.928811\n",
      "  params :  tensor([  5.3474, -17.1897])\n",
      "  grad :  tensor([-0.0035,  0.0196])\n",
      "Epoch 2957, Loss 2.928805\n",
      "  params :  tensor([  5.3474, -17.1899])\n",
      "  grad :  tensor([-0.0034,  0.0196])\n",
      "Epoch 2958, Loss 2.928802\n",
      "  params :  tensor([  5.3475, -17.1901])\n",
      "  grad :  tensor([-0.0035,  0.0195])\n",
      "Epoch 2959, Loss 2.928799\n",
      "  params :  tensor([  5.3475, -17.1903])\n",
      "  grad :  tensor([-0.0034,  0.0195])\n",
      "Epoch 2960, Loss 2.928795\n",
      "  params :  tensor([  5.3475, -17.1905])\n",
      "  grad :  tensor([-0.0034,  0.0195])\n",
      "Epoch 2961, Loss 2.928789\n",
      "  params :  tensor([  5.3476, -17.1907])\n",
      "  grad :  tensor([-0.0034,  0.0194])\n",
      "Epoch 2962, Loss 2.928789\n",
      "  params :  tensor([  5.3476, -17.1908])\n",
      "  grad :  tensor([-0.0034,  0.0194])\n",
      "Epoch 2963, Loss 2.928783\n",
      "  params :  tensor([  5.3476, -17.1910])\n",
      "  grad :  tensor([-0.0034,  0.0194])\n",
      "Epoch 2964, Loss 2.928779\n",
      "  params :  tensor([  5.3477, -17.1912])\n",
      "  grad :  tensor([-0.0034,  0.0193])\n",
      "Epoch 2965, Loss 2.928775\n",
      "  params :  tensor([  5.3477, -17.1914])\n",
      "  grad :  tensor([-0.0034,  0.0193])\n",
      "Epoch 2966, Loss 2.928771\n",
      "  params :  tensor([  5.3477, -17.1916])\n",
      "  grad :  tensor([-0.0034,  0.0193])\n",
      "Epoch 2967, Loss 2.928767\n",
      "  params :  tensor([  5.3478, -17.1918])\n",
      "  grad :  tensor([-0.0034,  0.0192])\n",
      "Epoch 2968, Loss 2.928765\n",
      "  params :  tensor([  5.3478, -17.1920])\n",
      "  grad :  tensor([-0.0034,  0.0192])\n",
      "Epoch 2969, Loss 2.928761\n",
      "  params :  tensor([  5.3478, -17.1922])\n",
      "  grad :  tensor([-0.0034,  0.0192])\n",
      "Epoch 2970, Loss 2.928758\n",
      "  params :  tensor([  5.3479, -17.1924])\n",
      "  grad :  tensor([-0.0034,  0.0191])\n",
      "Epoch 2971, Loss 2.928752\n",
      "  params :  tensor([  5.3479, -17.1926])\n",
      "  grad :  tensor([-0.0034,  0.0191])\n",
      "Epoch 2972, Loss 2.928750\n",
      "  params :  tensor([  5.3479, -17.1928])\n",
      "  grad :  tensor([-0.0034,  0.0191])\n",
      "Epoch 2973, Loss 2.928745\n",
      "  params :  tensor([  5.3480, -17.1930])\n",
      "  grad :  tensor([-0.0034,  0.0190])\n",
      "Epoch 2974, Loss 2.928741\n",
      "  params :  tensor([  5.3480, -17.1931])\n",
      "  grad :  tensor([-0.0034,  0.0190])\n",
      "Epoch 2975, Loss 2.928737\n",
      "  params :  tensor([  5.3480, -17.1933])\n",
      "  grad :  tensor([-0.0034,  0.0190])\n",
      "Epoch 2976, Loss 2.928735\n",
      "  params :  tensor([  5.3481, -17.1935])\n",
      "  grad :  tensor([-0.0033,  0.0189])\n",
      "Epoch 2977, Loss 2.928730\n",
      "  params :  tensor([  5.3481, -17.1937])\n",
      "  grad :  tensor([-0.0033,  0.0189])\n",
      "Epoch 2978, Loss 2.928727\n",
      "  params :  tensor([  5.3481, -17.1939])\n",
      "  grad :  tensor([-0.0033,  0.0189])\n",
      "Epoch 2979, Loss 2.928723\n",
      "  params :  tensor([  5.3482, -17.1941])\n",
      "  grad :  tensor([-0.0033,  0.0188])\n",
      "Epoch 2980, Loss 2.928719\n",
      "  params :  tensor([  5.3482, -17.1943])\n",
      "  grad :  tensor([-0.0033,  0.0188])\n",
      "Epoch 2981, Loss 2.928716\n",
      "  params :  tensor([  5.3482, -17.1945])\n",
      "  grad :  tensor([-0.0033,  0.0188])\n",
      "Epoch 2982, Loss 2.928712\n",
      "  params :  tensor([  5.3483, -17.1947])\n",
      "  grad :  tensor([-0.0033,  0.0187])\n",
      "Epoch 2983, Loss 2.928708\n",
      "  params :  tensor([  5.3483, -17.1948])\n",
      "  grad :  tensor([-0.0033,  0.0187])\n",
      "Epoch 2984, Loss 2.928705\n",
      "  params :  tensor([  5.3483, -17.1950])\n",
      "  grad :  tensor([-0.0033,  0.0187])\n",
      "Epoch 2985, Loss 2.928700\n",
      "  params :  tensor([  5.3484, -17.1952])\n",
      "  grad :  tensor([-0.0033,  0.0186])\n",
      "Epoch 2986, Loss 2.928698\n",
      "  params :  tensor([  5.3484, -17.1954])\n",
      "  grad :  tensor([-0.0033,  0.0186])\n",
      "Epoch 2987, Loss 2.928695\n",
      "  params :  tensor([  5.3484, -17.1956])\n",
      "  grad :  tensor([-0.0033,  0.0186])\n",
      "Epoch 2988, Loss 2.928690\n",
      "  params :  tensor([  5.3485, -17.1958])\n",
      "  grad :  tensor([-0.0033,  0.0186])\n",
      "Epoch 2989, Loss 2.928687\n",
      "  params :  tensor([  5.3485, -17.1960])\n",
      "  grad :  tensor([-0.0033,  0.0185])\n",
      "Epoch 2990, Loss 2.928684\n",
      "  params :  tensor([  5.3485, -17.1961])\n",
      "  grad :  tensor([-0.0033,  0.0185])\n",
      "Epoch 2991, Loss 2.928679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3486, -17.1963])\n",
      "  grad :  tensor([-0.0032,  0.0185])\n",
      "Epoch 2992, Loss 2.928677\n",
      "  params :  tensor([  5.3486, -17.1965])\n",
      "  grad :  tensor([-0.0033,  0.0184])\n",
      "Epoch 2993, Loss 2.928673\n",
      "  params :  tensor([  5.3486, -17.1967])\n",
      "  grad :  tensor([-0.0033,  0.0184])\n",
      "Epoch 2994, Loss 2.928669\n",
      "  params :  tensor([  5.3487, -17.1969])\n",
      "  grad :  tensor([-0.0033,  0.0184])\n",
      "Epoch 2995, Loss 2.928666\n",
      "  params :  tensor([  5.3487, -17.1971])\n",
      "  grad :  tensor([-0.0032,  0.0183])\n",
      "Epoch 2996, Loss 2.928662\n",
      "  params :  tensor([  5.3487, -17.1972])\n",
      "  grad :  tensor([-0.0032,  0.0183])\n",
      "Epoch 2997, Loss 2.928660\n",
      "  params :  tensor([  5.3488, -17.1974])\n",
      "  grad :  tensor([-0.0032,  0.0183])\n",
      "Epoch 2998, Loss 2.928656\n",
      "  params :  tensor([  5.3488, -17.1976])\n",
      "  grad :  tensor([-0.0032,  0.0182])\n",
      "Epoch 2999, Loss 2.928651\n",
      "  params :  tensor([  5.3488, -17.1978])\n",
      "  grad :  tensor([-0.0032,  0.0182])\n",
      "Epoch 3000, Loss 2.928648\n",
      "  params :  tensor([  5.3489, -17.1980])\n",
      "  grad :  tensor([-0.0032,  0.0182])\n",
      "Epoch 3001, Loss 2.928646\n",
      "  params :  tensor([  5.3489, -17.1982])\n",
      "  grad :  tensor([-0.0032,  0.0181])\n",
      "Epoch 3002, Loss 2.928643\n",
      "  params :  tensor([  5.3489, -17.1983])\n",
      "  grad :  tensor([-0.0032,  0.0181])\n",
      "Epoch 3003, Loss 2.928638\n",
      "  params :  tensor([  5.3489, -17.1985])\n",
      "  grad :  tensor([-0.0032,  0.0181])\n",
      "Epoch 3004, Loss 2.928635\n",
      "  params :  tensor([  5.3490, -17.1987])\n",
      "  grad :  tensor([-0.0032,  0.0181])\n",
      "Epoch 3005, Loss 2.928632\n",
      "  params :  tensor([  5.3490, -17.1989])\n",
      "  grad :  tensor([-0.0032,  0.0180])\n",
      "Epoch 3006, Loss 2.928629\n",
      "  params :  tensor([  5.3490, -17.1991])\n",
      "  grad :  tensor([-0.0032,  0.0180])\n",
      "Epoch 3007, Loss 2.928625\n",
      "  params :  tensor([  5.3491, -17.1992])\n",
      "  grad :  tensor([-0.0032,  0.0180])\n",
      "Epoch 3008, Loss 2.928621\n",
      "  params :  tensor([  5.3491, -17.1994])\n",
      "  grad :  tensor([-0.0032,  0.0179])\n",
      "Epoch 3009, Loss 2.928617\n",
      "  params :  tensor([  5.3491, -17.1996])\n",
      "  grad :  tensor([-0.0032,  0.0179])\n",
      "Epoch 3010, Loss 2.928616\n",
      "  params :  tensor([  5.3492, -17.1998])\n",
      "  grad :  tensor([-0.0032,  0.0179])\n",
      "Epoch 3011, Loss 2.928612\n",
      "  params :  tensor([  5.3492, -17.2000])\n",
      "  grad :  tensor([-0.0032,  0.0178])\n",
      "Epoch 3012, Loss 2.928608\n",
      "  params :  tensor([  5.3492, -17.2001])\n",
      "  grad :  tensor([-0.0032,  0.0178])\n",
      "Epoch 3013, Loss 2.928604\n",
      "  params :  tensor([  5.3493, -17.2003])\n",
      "  grad :  tensor([-0.0031,  0.0178])\n",
      "Epoch 3014, Loss 2.928601\n",
      "  params :  tensor([  5.3493, -17.2005])\n",
      "  grad :  tensor([-0.0031,  0.0177])\n",
      "Epoch 3015, Loss 2.928599\n",
      "  params :  tensor([  5.3493, -17.2007])\n",
      "  grad :  tensor([-0.0031,  0.0177])\n",
      "Epoch 3016, Loss 2.928596\n",
      "  params :  tensor([  5.3494, -17.2008])\n",
      "  grad :  tensor([-0.0031,  0.0177])\n",
      "Epoch 3017, Loss 2.928592\n",
      "  params :  tensor([  5.3494, -17.2010])\n",
      "  grad :  tensor([-0.0031,  0.0177])\n",
      "Epoch 3018, Loss 2.928588\n",
      "  params :  tensor([  5.3494, -17.2012])\n",
      "  grad :  tensor([-0.0031,  0.0176])\n",
      "Epoch 3019, Loss 2.928586\n",
      "  params :  tensor([  5.3495, -17.2014])\n",
      "  grad :  tensor([-0.0031,  0.0176])\n",
      "Epoch 3020, Loss 2.928583\n",
      "  params :  tensor([  5.3495, -17.2015])\n",
      "  grad :  tensor([-0.0031,  0.0176])\n",
      "Epoch 3021, Loss 2.928580\n",
      "  params :  tensor([  5.3495, -17.2017])\n",
      "  grad :  tensor([-0.0031,  0.0175])\n",
      "Epoch 3022, Loss 2.928576\n",
      "  params :  tensor([  5.3495, -17.2019])\n",
      "  grad :  tensor([-0.0031,  0.0175])\n",
      "Epoch 3023, Loss 2.928574\n",
      "  params :  tensor([  5.3496, -17.2021])\n",
      "  grad :  tensor([-0.0031,  0.0175])\n",
      "Epoch 3024, Loss 2.928570\n",
      "  params :  tensor([  5.3496, -17.2022])\n",
      "  grad :  tensor([-0.0031,  0.0175])\n",
      "Epoch 3025, Loss 2.928567\n",
      "  params :  tensor([  5.3496, -17.2024])\n",
      "  grad :  tensor([-0.0031,  0.0174])\n",
      "Epoch 3026, Loss 2.928564\n",
      "  params :  tensor([  5.3497, -17.2026])\n",
      "  grad :  tensor([-0.0031,  0.0174])\n",
      "Epoch 3027, Loss 2.928561\n",
      "  params :  tensor([  5.3497, -17.2028])\n",
      "  grad :  tensor([-0.0030,  0.0174])\n",
      "Epoch 3028, Loss 2.928557\n",
      "  params :  tensor([  5.3497, -17.2029])\n",
      "  grad :  tensor([-0.0031,  0.0173])\n",
      "Epoch 3029, Loss 2.928555\n",
      "  params :  tensor([  5.3498, -17.2031])\n",
      "  grad :  tensor([-0.0031,  0.0173])\n",
      "Epoch 3030, Loss 2.928551\n",
      "  params :  tensor([  5.3498, -17.2033])\n",
      "  grad :  tensor([-0.0031,  0.0173])\n",
      "Epoch 3031, Loss 2.928548\n",
      "  params :  tensor([  5.3498, -17.2035])\n",
      "  grad :  tensor([-0.0031,  0.0172])\n",
      "Epoch 3032, Loss 2.928545\n",
      "  params :  tensor([  5.3498, -17.2036])\n",
      "  grad :  tensor([-0.0030,  0.0172])\n",
      "Epoch 3033, Loss 2.928543\n",
      "  params :  tensor([  5.3499, -17.2038])\n",
      "  grad :  tensor([-0.0030,  0.0172])\n",
      "Epoch 3034, Loss 2.928539\n",
      "  params :  tensor([  5.3499, -17.2040])\n",
      "  grad :  tensor([-0.0030,  0.0172])\n",
      "Epoch 3035, Loss 2.928536\n",
      "  params :  tensor([  5.3499, -17.2041])\n",
      "  grad :  tensor([-0.0030,  0.0171])\n",
      "Epoch 3036, Loss 2.928532\n",
      "  params :  tensor([  5.3500, -17.2043])\n",
      "  grad :  tensor([-0.0030,  0.0171])\n",
      "Epoch 3037, Loss 2.928531\n",
      "  params :  tensor([  5.3500, -17.2045])\n",
      "  grad :  tensor([-0.0030,  0.0171])\n",
      "Epoch 3038, Loss 2.928528\n",
      "  params :  tensor([  5.3500, -17.2047])\n",
      "  grad :  tensor([-0.0030,  0.0170])\n",
      "Epoch 3039, Loss 2.928524\n",
      "  params :  tensor([  5.3501, -17.2048])\n",
      "  grad :  tensor([-0.0030,  0.0170])\n",
      "Epoch 3040, Loss 2.928521\n",
      "  params :  tensor([  5.3501, -17.2050])\n",
      "  grad :  tensor([-0.0030,  0.0170])\n",
      "Epoch 3041, Loss 2.928519\n",
      "  params :  tensor([  5.3501, -17.2052])\n",
      "  grad :  tensor([-0.0030,  0.0170])\n",
      "Epoch 3042, Loss 2.928514\n",
      "  params :  tensor([  5.3502, -17.2053])\n",
      "  grad :  tensor([-0.0030,  0.0169])\n",
      "Epoch 3043, Loss 2.928512\n",
      "  params :  tensor([  5.3502, -17.2055])\n",
      "  grad :  tensor([-0.0030,  0.0169])\n",
      "Epoch 3044, Loss 2.928509\n",
      "  params :  tensor([  5.3502, -17.2057])\n",
      "  grad :  tensor([-0.0030,  0.0169])\n",
      "Epoch 3045, Loss 2.928505\n",
      "  params :  tensor([  5.3502, -17.2058])\n",
      "  grad :  tensor([-0.0030,  0.0168])\n",
      "Epoch 3046, Loss 2.928503\n",
      "  params :  tensor([  5.3503, -17.2060])\n",
      "  grad :  tensor([-0.0030,  0.0168])\n",
      "Epoch 3047, Loss 2.928500\n",
      "  params :  tensor([  5.3503, -17.2062])\n",
      "  grad :  tensor([-0.0030,  0.0168])\n",
      "Epoch 3048, Loss 2.928498\n",
      "  params :  tensor([  5.3503, -17.2063])\n",
      "  grad :  tensor([-0.0030,  0.0168])\n",
      "Epoch 3049, Loss 2.928495\n",
      "  params :  tensor([  5.3504, -17.2065])\n",
      "  grad :  tensor([-0.0030,  0.0167])\n",
      "Epoch 3050, Loss 2.928491\n",
      "  params :  tensor([  5.3504, -17.2067])\n",
      "  grad :  tensor([-0.0030,  0.0167])\n",
      "Epoch 3051, Loss 2.928489\n",
      "  params :  tensor([  5.3504, -17.2068])\n",
      "  grad :  tensor([-0.0030,  0.0167])\n",
      "Epoch 3052, Loss 2.928486\n",
      "  params :  tensor([  5.3504, -17.2070])\n",
      "  grad :  tensor([-0.0029,  0.0166])\n",
      "Epoch 3053, Loss 2.928484\n",
      "  params :  tensor([  5.3505, -17.2072])\n",
      "  grad :  tensor([-0.0029,  0.0166])\n",
      "Epoch 3054, Loss 2.928481\n",
      "  params :  tensor([  5.3505, -17.2073])\n",
      "  grad :  tensor([-0.0029,  0.0166])\n",
      "Epoch 3055, Loss 2.928477\n",
      "  params :  tensor([  5.3505, -17.2075])\n",
      "  grad :  tensor([-0.0029,  0.0165])\n",
      "Epoch 3056, Loss 2.928474\n",
      "  params :  tensor([  5.3506, -17.2077])\n",
      "  grad :  tensor([-0.0029,  0.0165])\n",
      "Epoch 3057, Loss 2.928472\n",
      "  params :  tensor([  5.3506, -17.2078])\n",
      "  grad :  tensor([-0.0029,  0.0165])\n",
      "Epoch 3058, Loss 2.928469\n",
      "  params :  tensor([  5.3506, -17.2080])\n",
      "  grad :  tensor([-0.0029,  0.0165])\n",
      "Epoch 3059, Loss 2.928468\n",
      "  params :  tensor([  5.3507, -17.2082])\n",
      "  grad :  tensor([-0.0029,  0.0164])\n",
      "Epoch 3060, Loss 2.928463\n",
      "  params :  tensor([  5.3507, -17.2083])\n",
      "  grad :  tensor([-0.0029,  0.0164])\n",
      "Epoch 3061, Loss 2.928460\n",
      "  params :  tensor([  5.3507, -17.2085])\n",
      "  grad :  tensor([-0.0029,  0.0164])\n",
      "Epoch 3062, Loss 2.928458\n",
      "  params :  tensor([  5.3507, -17.2087])\n",
      "  grad :  tensor([-0.0029,  0.0164])\n",
      "Epoch 3063, Loss 2.928456\n",
      "  params :  tensor([  5.3508, -17.2088])\n",
      "  grad :  tensor([-0.0029,  0.0163])\n",
      "Epoch 3064, Loss 2.928452\n",
      "  params :  tensor([  5.3508, -17.2090])\n",
      "  grad :  tensor([-0.0029,  0.0163])\n",
      "Epoch 3065, Loss 2.928449\n",
      "  params :  tensor([  5.3508, -17.2091])\n",
      "  grad :  tensor([-0.0029,  0.0163])\n",
      "Epoch 3066, Loss 2.928447\n",
      "  params :  tensor([  5.3509, -17.2093])\n",
      "  grad :  tensor([-0.0029,  0.0162])\n",
      "Epoch 3067, Loss 2.928443\n",
      "  params :  tensor([  5.3509, -17.2095])\n",
      "  grad :  tensor([-0.0029,  0.0162])\n",
      "Epoch 3068, Loss 2.928444\n",
      "  params :  tensor([  5.3509, -17.2096])\n",
      "  grad :  tensor([-0.0029,  0.0162])\n",
      "Epoch 3069, Loss 2.928440\n",
      "  params :  tensor([  5.3509, -17.2098])\n",
      "  grad :  tensor([-0.0029,  0.0162])\n",
      "Epoch 3070, Loss 2.928435\n",
      "  params :  tensor([  5.3510, -17.2100])\n",
      "  grad :  tensor([-0.0029,  0.0161])\n",
      "Epoch 3071, Loss 2.928435\n",
      "  params :  tensor([  5.3510, -17.2101])\n",
      "  grad :  tensor([-0.0029,  0.0161])\n",
      "Epoch 3072, Loss 2.928430\n",
      "  params :  tensor([  5.3510, -17.2103])\n",
      "  grad :  tensor([-0.0028,  0.0161])\n",
      "Epoch 3073, Loss 2.928428\n",
      "  params :  tensor([  5.3511, -17.2104])\n",
      "  grad :  tensor([-0.0028,  0.0161])\n",
      "Epoch 3074, Loss 2.928426\n",
      "  params :  tensor([  5.3511, -17.2106])\n",
      "  grad :  tensor([-0.0028,  0.0160])\n",
      "Epoch 3075, Loss 2.928423\n",
      "  params :  tensor([  5.3511, -17.2108])\n",
      "  grad :  tensor([-0.0028,  0.0160])\n",
      "Epoch 3076, Loss 2.928421\n",
      "  params :  tensor([  5.3511, -17.2109])\n",
      "  grad :  tensor([-0.0028,  0.0160])\n",
      "Epoch 3077, Loss 2.928417\n",
      "  params :  tensor([  5.3512, -17.2111])\n",
      "  grad :  tensor([-0.0028,  0.0159])\n",
      "Epoch 3078, Loss 2.928416\n",
      "  params :  tensor([  5.3512, -17.2112])\n",
      "  grad :  tensor([-0.0028,  0.0159])\n",
      "Epoch 3079, Loss 2.928411\n",
      "  params :  tensor([  5.3512, -17.2114])\n",
      "  grad :  tensor([-0.0028,  0.0159])\n",
      "Epoch 3080, Loss 2.928410\n",
      "  params :  tensor([  5.3512, -17.2116])\n",
      "  grad :  tensor([-0.0028,  0.0159])\n",
      "Epoch 3081, Loss 2.928407\n",
      "  params :  tensor([  5.3513, -17.2117])\n",
      "  grad :  tensor([-0.0028,  0.0158])\n",
      "Epoch 3082, Loss 2.928404\n",
      "  params :  tensor([  5.3513, -17.2119])\n",
      "  grad :  tensor([-0.0028,  0.0158])\n",
      "Epoch 3083, Loss 2.928402\n",
      "  params :  tensor([  5.3513, -17.2120])\n",
      "  grad :  tensor([-0.0028,  0.0158])\n",
      "Epoch 3084, Loss 2.928399\n",
      "  params :  tensor([  5.3514, -17.2122])\n",
      "  grad :  tensor([-0.0028,  0.0158])\n",
      "Epoch 3085, Loss 2.928396\n",
      "  params :  tensor([  5.3514, -17.2123])\n",
      "  grad :  tensor([-0.0028,  0.0157])\n",
      "Epoch 3086, Loss 2.928395\n",
      "  params :  tensor([  5.3514, -17.2125])\n",
      "  grad :  tensor([-0.0028,  0.0157])\n",
      "Epoch 3087, Loss 2.928392\n",
      "  params :  tensor([  5.3514, -17.2127])\n",
      "  grad :  tensor([-0.0027,  0.0157])\n",
      "Epoch 3088, Loss 2.928389\n",
      "  params :  tensor([  5.3515, -17.2128])\n",
      "  grad :  tensor([-0.0027,  0.0157])\n",
      "Epoch 3089, Loss 2.928386\n",
      "  params :  tensor([  5.3515, -17.2130])\n",
      "  grad :  tensor([-0.0027,  0.0156])\n",
      "Epoch 3090, Loss 2.928383\n",
      "  params :  tensor([  5.3515, -17.2131])\n",
      "  grad :  tensor([-0.0028,  0.0156])\n",
      "Epoch 3091, Loss 2.928382\n",
      "  params :  tensor([  5.3516, -17.2133])\n",
      "  grad :  tensor([-0.0028,  0.0156])\n",
      "Epoch 3092, Loss 2.928379\n",
      "  params :  tensor([  5.3516, -17.2134])\n",
      "  grad :  tensor([-0.0027,  0.0155])\n",
      "Epoch 3093, Loss 2.928378\n",
      "  params :  tensor([  5.3516, -17.2136])\n",
      "  grad :  tensor([-0.0027,  0.0155])\n",
      "Epoch 3094, Loss 2.928375\n",
      "  params :  tensor([  5.3516, -17.2137])\n",
      "  grad :  tensor([-0.0027,  0.0155])\n",
      "Epoch 3095, Loss 2.928372\n",
      "  params :  tensor([  5.3517, -17.2139])\n",
      "  grad :  tensor([-0.0027,  0.0155])\n",
      "Epoch 3096, Loss 2.928370\n",
      "  params :  tensor([  5.3517, -17.2141])\n",
      "  grad :  tensor([-0.0027,  0.0154])\n",
      "Epoch 3097, Loss 2.928368\n",
      "  params :  tensor([  5.3517, -17.2142])\n",
      "  grad :  tensor([-0.0027,  0.0154])\n",
      "Epoch 3098, Loss 2.928364\n",
      "  params :  tensor([  5.3517, -17.2144])\n",
      "  grad :  tensor([-0.0027,  0.0154])\n",
      "Epoch 3099, Loss 2.928362\n",
      "  params :  tensor([  5.3518, -17.2145])\n",
      "  grad :  tensor([-0.0027,  0.0154])\n",
      "Epoch 3100, Loss 2.928361\n",
      "  params :  tensor([  5.3518, -17.2147])\n",
      "  grad :  tensor([-0.0027,  0.0153])\n",
      "Epoch 3101, Loss 2.928356\n",
      "  params :  tensor([  5.3518, -17.2148])\n",
      "  grad :  tensor([-0.0027,  0.0153])\n",
      "Epoch 3102, Loss 2.928355\n",
      "  params :  tensor([  5.3519, -17.2150])\n",
      "  grad :  tensor([-0.0027,  0.0153])\n",
      "Epoch 3103, Loss 2.928353\n",
      "  params :  tensor([  5.3519, -17.2151])\n",
      "  grad :  tensor([-0.0027,  0.0153])\n",
      "Epoch 3104, Loss 2.928349\n",
      "  params :  tensor([  5.3519, -17.2153])\n",
      "  grad :  tensor([-0.0027,  0.0152])\n",
      "Epoch 3105, Loss 2.928348\n",
      "  params :  tensor([  5.3519, -17.2154])\n",
      "  grad :  tensor([-0.0027,  0.0152])\n",
      "Epoch 3106, Loss 2.928345\n",
      "  params :  tensor([  5.3520, -17.2156])\n",
      "  grad :  tensor([-0.0027,  0.0152])\n",
      "Epoch 3107, Loss 2.928343\n",
      "  params :  tensor([  5.3520, -17.2157])\n",
      "  grad :  tensor([-0.0027,  0.0152])\n",
      "Epoch 3108, Loss 2.928340\n",
      "  params :  tensor([  5.3520, -17.2159])\n",
      "  grad :  tensor([-0.0027,  0.0151])\n",
      "Epoch 3109, Loss 2.928339\n",
      "  params :  tensor([  5.3520, -17.2160])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad :  tensor([-0.0027,  0.0151])\n",
      "Epoch 3110, Loss 2.928337\n",
      "  params :  tensor([  5.3521, -17.2162])\n",
      "  grad :  tensor([-0.0027,  0.0151])\n",
      "Epoch 3111, Loss 2.928333\n",
      "  params :  tensor([  5.3521, -17.2163])\n",
      "  grad :  tensor([-0.0027,  0.0151])\n",
      "Epoch 3112, Loss 2.928332\n",
      "  params :  tensor([  5.3521, -17.2165])\n",
      "  grad :  tensor([-0.0027,  0.0150])\n",
      "Epoch 3113, Loss 2.928328\n",
      "  params :  tensor([  5.3521, -17.2166])\n",
      "  grad :  tensor([-0.0026,  0.0150])\n",
      "Epoch 3114, Loss 2.928329\n",
      "  params :  tensor([  5.3522, -17.2168])\n",
      "  grad :  tensor([-0.0027,  0.0150])\n",
      "Epoch 3115, Loss 2.928324\n",
      "  params :  tensor([  5.3522, -17.2169])\n",
      "  grad :  tensor([-0.0026,  0.0149])\n",
      "Epoch 3116, Loss 2.928323\n",
      "  params :  tensor([  5.3522, -17.2171])\n",
      "  grad :  tensor([-0.0026,  0.0149])\n",
      "Epoch 3117, Loss 2.928320\n",
      "  params :  tensor([  5.3523, -17.2172])\n",
      "  grad :  tensor([-0.0026,  0.0149])\n",
      "Epoch 3118, Loss 2.928319\n",
      "  params :  tensor([  5.3523, -17.2174])\n",
      "  grad :  tensor([-0.0026,  0.0149])\n",
      "Epoch 3119, Loss 2.928315\n",
      "  params :  tensor([  5.3523, -17.2175])\n",
      "  grad :  tensor([-0.0026,  0.0148])\n",
      "Epoch 3120, Loss 2.928313\n",
      "  params :  tensor([  5.3523, -17.2177])\n",
      "  grad :  tensor([-0.0026,  0.0148])\n",
      "Epoch 3121, Loss 2.928310\n",
      "  params :  tensor([  5.3524, -17.2178])\n",
      "  grad :  tensor([-0.0026,  0.0148])\n",
      "Epoch 3122, Loss 2.928308\n",
      "  params :  tensor([  5.3524, -17.2180])\n",
      "  grad :  tensor([-0.0026,  0.0148])\n",
      "Epoch 3123, Loss 2.928306\n",
      "  params :  tensor([  5.3524, -17.2181])\n",
      "  grad :  tensor([-0.0026,  0.0147])\n",
      "Epoch 3124, Loss 2.928304\n",
      "  params :  tensor([  5.3524, -17.2183])\n",
      "  grad :  tensor([-0.0026,  0.0147])\n",
      "Epoch 3125, Loss 2.928303\n",
      "  params :  tensor([  5.3525, -17.2184])\n",
      "  grad :  tensor([-0.0026,  0.0147])\n",
      "Epoch 3126, Loss 2.928299\n",
      "  params :  tensor([  5.3525, -17.2186])\n",
      "  grad :  tensor([-0.0026,  0.0147])\n",
      "Epoch 3127, Loss 2.928296\n",
      "  params :  tensor([  5.3525, -17.2187])\n",
      "  grad :  tensor([-0.0026,  0.0146])\n",
      "Epoch 3128, Loss 2.928295\n",
      "  params :  tensor([  5.3525, -17.2189])\n",
      "  grad :  tensor([-0.0026,  0.0146])\n",
      "Epoch 3129, Loss 2.928293\n",
      "  params :  tensor([  5.3526, -17.2190])\n",
      "  grad :  tensor([-0.0026,  0.0146])\n",
      "Epoch 3130, Loss 2.928291\n",
      "  params :  tensor([  5.3526, -17.2192])\n",
      "  grad :  tensor([-0.0026,  0.0146])\n",
      "Epoch 3131, Loss 2.928288\n",
      "  params :  tensor([  5.3526, -17.2193])\n",
      "  grad :  tensor([-0.0026,  0.0145])\n",
      "Epoch 3132, Loss 2.928287\n",
      "  params :  tensor([  5.3526, -17.2194])\n",
      "  grad :  tensor([-0.0026,  0.0145])\n",
      "Epoch 3133, Loss 2.928285\n",
      "  params :  tensor([  5.3527, -17.2196])\n",
      "  grad :  tensor([-0.0025,  0.0145])\n",
      "Epoch 3134, Loss 2.928282\n",
      "  params :  tensor([  5.3527, -17.2197])\n",
      "  grad :  tensor([-0.0026,  0.0145])\n",
      "Epoch 3135, Loss 2.928280\n",
      "  params :  tensor([  5.3527, -17.2199])\n",
      "  grad :  tensor([-0.0026,  0.0144])\n",
      "Epoch 3136, Loss 2.928276\n",
      "  params :  tensor([  5.3527, -17.2200])\n",
      "  grad :  tensor([-0.0025,  0.0144])\n",
      "Epoch 3137, Loss 2.928275\n",
      "  params :  tensor([  5.3528, -17.2202])\n",
      "  grad :  tensor([-0.0026,  0.0144])\n",
      "Epoch 3138, Loss 2.928273\n",
      "  params :  tensor([  5.3528, -17.2203])\n",
      "  grad :  tensor([-0.0025,  0.0144])\n",
      "Epoch 3139, Loss 2.928271\n",
      "  params :  tensor([  5.3528, -17.2205])\n",
      "  grad :  tensor([-0.0025,  0.0144])\n",
      "Epoch 3140, Loss 2.928268\n",
      "  params :  tensor([  5.3528, -17.2206])\n",
      "  grad :  tensor([-0.0025,  0.0143])\n",
      "Epoch 3141, Loss 2.928267\n",
      "  params :  tensor([  5.3529, -17.2207])\n",
      "  grad :  tensor([-0.0025,  0.0143])\n",
      "Epoch 3142, Loss 2.928264\n",
      "  params :  tensor([  5.3529, -17.2209])\n",
      "  grad :  tensor([-0.0025,  0.0143])\n",
      "Epoch 3143, Loss 2.928263\n",
      "  params :  tensor([  5.3529, -17.2210])\n",
      "  grad :  tensor([-0.0025,  0.0143])\n",
      "Epoch 3144, Loss 2.928260\n",
      "  params :  tensor([  5.3529, -17.2212])\n",
      "  grad :  tensor([-0.0025,  0.0142])\n",
      "Epoch 3145, Loss 2.928259\n",
      "  params :  tensor([  5.3530, -17.2213])\n",
      "  grad :  tensor([-0.0025,  0.0142])\n",
      "Epoch 3146, Loss 2.928256\n",
      "  params :  tensor([  5.3530, -17.2214])\n",
      "  grad :  tensor([-0.0025,  0.0142])\n",
      "Epoch 3147, Loss 2.928255\n",
      "  params :  tensor([  5.3530, -17.2216])\n",
      "  grad :  tensor([-0.0025,  0.0142])\n",
      "Epoch 3148, Loss 2.928252\n",
      "  params :  tensor([  5.3530, -17.2217])\n",
      "  grad :  tensor([-0.0025,  0.0141])\n",
      "Epoch 3149, Loss 2.928250\n",
      "  params :  tensor([  5.3531, -17.2219])\n",
      "  grad :  tensor([-0.0025,  0.0141])\n",
      "Epoch 3150, Loss 2.928249\n",
      "  params :  tensor([  5.3531, -17.2220])\n",
      "  grad :  tensor([-0.0025,  0.0141])\n",
      "Epoch 3151, Loss 2.928246\n",
      "  params :  tensor([  5.3531, -17.2222])\n",
      "  grad :  tensor([-0.0025,  0.0141])\n",
      "Epoch 3152, Loss 2.928245\n",
      "  params :  tensor([  5.3531, -17.2223])\n",
      "  grad :  tensor([-0.0025,  0.0140])\n",
      "Epoch 3153, Loss 2.928242\n",
      "  params :  tensor([  5.3532, -17.2224])\n",
      "  grad :  tensor([-0.0025,  0.0140])\n",
      "Epoch 3154, Loss 2.928239\n",
      "  params :  tensor([  5.3532, -17.2226])\n",
      "  grad :  tensor([-0.0025,  0.0140])\n",
      "Epoch 3155, Loss 2.928236\n",
      "  params :  tensor([  5.3532, -17.2227])\n",
      "  grad :  tensor([-0.0025,  0.0140])\n",
      "Epoch 3156, Loss 2.928236\n",
      "  params :  tensor([  5.3532, -17.2229])\n",
      "  grad :  tensor([-0.0024,  0.0139])\n",
      "Epoch 3157, Loss 2.928233\n",
      "  params :  tensor([  5.3533, -17.2230])\n",
      "  grad :  tensor([-0.0025,  0.0139])\n",
      "Epoch 3158, Loss 2.928231\n",
      "  params :  tensor([  5.3533, -17.2231])\n",
      "  grad :  tensor([-0.0024,  0.0139])\n",
      "Epoch 3159, Loss 2.928230\n",
      "  params :  tensor([  5.3533, -17.2233])\n",
      "  grad :  tensor([-0.0025,  0.0139])\n",
      "Epoch 3160, Loss 2.928227\n",
      "  params :  tensor([  5.3533, -17.2234])\n",
      "  grad :  tensor([-0.0024,  0.0138])\n",
      "Epoch 3161, Loss 2.928226\n",
      "  params :  tensor([  5.3534, -17.2235])\n",
      "  grad :  tensor([-0.0025,  0.0138])\n",
      "Epoch 3162, Loss 2.928225\n",
      "  params :  tensor([  5.3534, -17.2237])\n",
      "  grad :  tensor([-0.0024,  0.0138])\n",
      "Epoch 3163, Loss 2.928222\n",
      "  params :  tensor([  5.3534, -17.2238])\n",
      "  grad :  tensor([-0.0024,  0.0138])\n",
      "Epoch 3164, Loss 2.928219\n",
      "  params :  tensor([  5.3534, -17.2240])\n",
      "  grad :  tensor([-0.0024,  0.0138])\n",
      "Epoch 3165, Loss 2.928218\n",
      "  params :  tensor([  5.3535, -17.2241])\n",
      "  grad :  tensor([-0.0024,  0.0137])\n",
      "Epoch 3166, Loss 2.928216\n",
      "  params :  tensor([  5.3535, -17.2242])\n",
      "  grad :  tensor([-0.0024,  0.0137])\n",
      "Epoch 3167, Loss 2.928215\n",
      "  params :  tensor([  5.3535, -17.2244])\n",
      "  grad :  tensor([-0.0024,  0.0137])\n",
      "Epoch 3168, Loss 2.928212\n",
      "  params :  tensor([  5.3535, -17.2245])\n",
      "  grad :  tensor([-0.0024,  0.0137])\n",
      "Epoch 3169, Loss 2.928211\n",
      "  params :  tensor([  5.3536, -17.2246])\n",
      "  grad :  tensor([-0.0024,  0.0136])\n",
      "Epoch 3170, Loss 2.928209\n",
      "  params :  tensor([  5.3536, -17.2248])\n",
      "  grad :  tensor([-0.0024,  0.0136])\n",
      "Epoch 3171, Loss 2.928206\n",
      "  params :  tensor([  5.3536, -17.2249])\n",
      "  grad :  tensor([-0.0024,  0.0136])\n",
      "Epoch 3172, Loss 2.928205\n",
      "  params :  tensor([  5.3536, -17.2250])\n",
      "  grad :  tensor([-0.0024,  0.0136])\n",
      "Epoch 3173, Loss 2.928204\n",
      "  params :  tensor([  5.3537, -17.2252])\n",
      "  grad :  tensor([-0.0024,  0.0135])\n",
      "Epoch 3174, Loss 2.928202\n",
      "  params :  tensor([  5.3537, -17.2253])\n",
      "  grad :  tensor([-0.0024,  0.0135])\n",
      "Epoch 3175, Loss 2.928200\n",
      "  params :  tensor([  5.3537, -17.2255])\n",
      "  grad :  tensor([-0.0024,  0.0135])\n",
      "Epoch 3176, Loss 2.928196\n",
      "  params :  tensor([  5.3537, -17.2256])\n",
      "  grad :  tensor([-0.0024,  0.0135])\n",
      "Epoch 3177, Loss 2.928195\n",
      "  params :  tensor([  5.3538, -17.2257])\n",
      "  grad :  tensor([-0.0024,  0.0134])\n",
      "Epoch 3178, Loss 2.928195\n",
      "  params :  tensor([  5.3538, -17.2259])\n",
      "  grad :  tensor([-0.0024,  0.0134])\n",
      "Epoch 3179, Loss 2.928191\n",
      "  params :  tensor([  5.3538, -17.2260])\n",
      "  grad :  tensor([-0.0024,  0.0134])\n",
      "Epoch 3180, Loss 2.928190\n",
      "  params :  tensor([  5.3538, -17.2261])\n",
      "  grad :  tensor([-0.0024,  0.0134])\n",
      "Epoch 3181, Loss 2.928188\n",
      "  params :  tensor([  5.3538, -17.2263])\n",
      "  grad :  tensor([-0.0023,  0.0134])\n",
      "Epoch 3182, Loss 2.928186\n",
      "  params :  tensor([  5.3539, -17.2264])\n",
      "  grad :  tensor([-0.0023,  0.0133])\n",
      "Epoch 3183, Loss 2.928185\n",
      "  params :  tensor([  5.3539, -17.2265])\n",
      "  grad :  tensor([-0.0024,  0.0133])\n",
      "Epoch 3184, Loss 2.928184\n",
      "  params :  tensor([  5.3539, -17.2267])\n",
      "  grad :  tensor([-0.0023,  0.0133])\n",
      "Epoch 3185, Loss 2.928182\n",
      "  params :  tensor([  5.3539, -17.2268])\n",
      "  grad :  tensor([-0.0024,  0.0133])\n",
      "Epoch 3186, Loss 2.928180\n",
      "  params :  tensor([  5.3540, -17.2269])\n",
      "  grad :  tensor([-0.0024,  0.0132])\n",
      "Epoch 3187, Loss 2.928178\n",
      "  params :  tensor([  5.3540, -17.2271])\n",
      "  grad :  tensor([-0.0023,  0.0132])\n",
      "Epoch 3188, Loss 2.928175\n",
      "  params :  tensor([  5.3540, -17.2272])\n",
      "  grad :  tensor([-0.0023,  0.0132])\n",
      "Epoch 3189, Loss 2.928172\n",
      "  params :  tensor([  5.3540, -17.2273])\n",
      "  grad :  tensor([-0.0023,  0.0132])\n",
      "Epoch 3190, Loss 2.928171\n",
      "  params :  tensor([  5.3541, -17.2275])\n",
      "  grad :  tensor([-0.0023,  0.0132])\n",
      "Epoch 3191, Loss 2.928170\n",
      "  params :  tensor([  5.3541, -17.2276])\n",
      "  grad :  tensor([-0.0023,  0.0131])\n",
      "Epoch 3192, Loss 2.928169\n",
      "  params :  tensor([  5.3541, -17.2277])\n",
      "  grad :  tensor([-0.0023,  0.0131])\n",
      "Epoch 3193, Loss 2.928167\n",
      "  params :  tensor([  5.3541, -17.2278])\n",
      "  grad :  tensor([-0.0023,  0.0131])\n",
      "Epoch 3194, Loss 2.928164\n",
      "  params :  tensor([  5.3542, -17.2280])\n",
      "  grad :  tensor([-0.0023,  0.0131])\n",
      "Epoch 3195, Loss 2.928163\n",
      "  params :  tensor([  5.3542, -17.2281])\n",
      "  grad :  tensor([-0.0023,  0.0130])\n",
      "Epoch 3196, Loss 2.928162\n",
      "  params :  tensor([  5.3542, -17.2282])\n",
      "  grad :  tensor([-0.0023,  0.0130])\n",
      "Epoch 3197, Loss 2.928160\n",
      "  params :  tensor([  5.3542, -17.2284])\n",
      "  grad :  tensor([-0.0023,  0.0130])\n",
      "Epoch 3198, Loss 2.928158\n",
      "  params :  tensor([  5.3542, -17.2285])\n",
      "  grad :  tensor([-0.0023,  0.0130])\n",
      "Epoch 3199, Loss 2.928157\n",
      "  params :  tensor([  5.3543, -17.2286])\n",
      "  grad :  tensor([-0.0023,  0.0130])\n",
      "Epoch 3200, Loss 2.928154\n",
      "  params :  tensor([  5.3543, -17.2288])\n",
      "  grad :  tensor([-0.0023,  0.0129])\n",
      "Epoch 3201, Loss 2.928152\n",
      "  params :  tensor([  5.3543, -17.2289])\n",
      "  grad :  tensor([-0.0023,  0.0129])\n",
      "Epoch 3202, Loss 2.928149\n",
      "  params :  tensor([  5.3543, -17.2290])\n",
      "  grad :  tensor([-0.0023,  0.0129])\n",
      "Epoch 3203, Loss 2.928150\n",
      "  params :  tensor([  5.3544, -17.2291])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad :  tensor([-0.0023,  0.0129])\n",
      "Epoch 3204, Loss 2.928147\n",
      "  params :  tensor([  5.3544, -17.2293])\n",
      "  grad :  tensor([-0.0022,  0.0129])\n",
      "Epoch 3205, Loss 2.928146\n",
      "  params :  tensor([  5.3544, -17.2294])\n",
      "  grad :  tensor([-0.0023,  0.0128])\n",
      "Epoch 3206, Loss 2.928144\n",
      "  params :  tensor([  5.3544, -17.2295])\n",
      "  grad :  tensor([-0.0023,  0.0128])\n",
      "Epoch 3207, Loss 2.928142\n",
      "  params :  tensor([  5.3544, -17.2297])\n",
      "  grad :  tensor([-0.0023,  0.0128])\n",
      "Epoch 3208, Loss 2.928140\n",
      "  params :  tensor([  5.3545, -17.2298])\n",
      "  grad :  tensor([-0.0022,  0.0128])\n",
      "Epoch 3209, Loss 2.928138\n",
      "  params :  tensor([  5.3545, -17.2299])\n",
      "  grad :  tensor([-0.0022,  0.0127])\n",
      "Epoch 3210, Loss 2.928137\n",
      "  params :  tensor([  5.3545, -17.2300])\n",
      "  grad :  tensor([-0.0023,  0.0127])\n",
      "Epoch 3211, Loss 2.928135\n",
      "  params :  tensor([  5.3545, -17.2302])\n",
      "  grad :  tensor([-0.0023,  0.0127])\n",
      "Epoch 3212, Loss 2.928135\n",
      "  params :  tensor([  5.3546, -17.2303])\n",
      "  grad :  tensor([-0.0023,  0.0127])\n",
      "Epoch 3213, Loss 2.928133\n",
      "  params :  tensor([  5.3546, -17.2304])\n",
      "  grad :  tensor([-0.0022,  0.0127])\n",
      "Epoch 3214, Loss 2.928131\n",
      "  params :  tensor([  5.3546, -17.2305])\n",
      "  grad :  tensor([-0.0022,  0.0126])\n",
      "Epoch 3215, Loss 2.928130\n",
      "  params :  tensor([  5.3546, -17.2307])\n",
      "  grad :  tensor([-0.0022,  0.0126])\n",
      "Epoch 3216, Loss 2.928126\n",
      "  params :  tensor([  5.3546, -17.2308])\n",
      "  grad :  tensor([-0.0022,  0.0126])\n",
      "Epoch 3217, Loss 2.928125\n",
      "  params :  tensor([  5.3547, -17.2309])\n",
      "  grad :  tensor([-0.0022,  0.0126])\n",
      "Epoch 3218, Loss 2.928124\n",
      "  params :  tensor([  5.3547, -17.2310])\n",
      "  grad :  tensor([-0.0022,  0.0125])\n",
      "Epoch 3219, Loss 2.928121\n",
      "  params :  tensor([  5.3547, -17.2312])\n",
      "  grad :  tensor([-0.0022,  0.0125])\n",
      "Epoch 3220, Loss 2.928121\n",
      "  params :  tensor([  5.3547, -17.2313])\n",
      "  grad :  tensor([-0.0022,  0.0125])\n",
      "Epoch 3221, Loss 2.928120\n",
      "  params :  tensor([  5.3548, -17.2314])\n",
      "  grad :  tensor([-0.0022,  0.0125])\n",
      "Epoch 3222, Loss 2.928118\n",
      "  params :  tensor([  5.3548, -17.2315])\n",
      "  grad :  tensor([-0.0022,  0.0125])\n",
      "Epoch 3223, Loss 2.928117\n",
      "  params :  tensor([  5.3548, -17.2317])\n",
      "  grad :  tensor([-0.0022,  0.0124])\n",
      "Epoch 3224, Loss 2.928115\n",
      "  params :  tensor([  5.3548, -17.2318])\n",
      "  grad :  tensor([-0.0022,  0.0124])\n",
      "Epoch 3225, Loss 2.928113\n",
      "  params :  tensor([  5.3548, -17.2319])\n",
      "  grad :  tensor([-0.0022,  0.0124])\n",
      "Epoch 3226, Loss 2.928110\n",
      "  params :  tensor([  5.3549, -17.2320])\n",
      "  grad :  tensor([-0.0022,  0.0124])\n",
      "Epoch 3227, Loss 2.928109\n",
      "  params :  tensor([  5.3549, -17.2322])\n",
      "  grad :  tensor([-0.0022,  0.0124])\n",
      "Epoch 3228, Loss 2.928108\n",
      "  params :  tensor([  5.3549, -17.2323])\n",
      "  grad :  tensor([-0.0022,  0.0123])\n",
      "Epoch 3229, Loss 2.928105\n",
      "  params :  tensor([  5.3549, -17.2324])\n",
      "  grad :  tensor([-0.0022,  0.0123])\n",
      "Epoch 3230, Loss 2.928105\n",
      "  params :  tensor([  5.3550, -17.2325])\n",
      "  grad :  tensor([-0.0022,  0.0123])\n",
      "Epoch 3231, Loss 2.928104\n",
      "  params :  tensor([  5.3550, -17.2327])\n",
      "  grad :  tensor([-0.0022,  0.0123])\n",
      "Epoch 3232, Loss 2.928102\n",
      "  params :  tensor([  5.3550, -17.2328])\n",
      "  grad :  tensor([-0.0021,  0.0123])\n",
      "Epoch 3233, Loss 2.928101\n",
      "  params :  tensor([  5.3550, -17.2329])\n",
      "  grad :  tensor([-0.0022,  0.0122])\n",
      "Epoch 3234, Loss 2.928098\n",
      "  params :  tensor([  5.3550, -17.2330])\n",
      "  grad :  tensor([-0.0022,  0.0122])\n",
      "Epoch 3235, Loss 2.928097\n",
      "  params :  tensor([  5.3551, -17.2331])\n",
      "  grad :  tensor([-0.0022,  0.0122])\n",
      "Epoch 3236, Loss 2.928095\n",
      "  params :  tensor([  5.3551, -17.2333])\n",
      "  grad :  tensor([-0.0022,  0.0122])\n",
      "Epoch 3237, Loss 2.928094\n",
      "  params :  tensor([  5.3551, -17.2334])\n",
      "  grad :  tensor([-0.0022,  0.0121])\n",
      "Epoch 3238, Loss 2.928093\n",
      "  params :  tensor([  5.3551, -17.2335])\n",
      "  grad :  tensor([-0.0022,  0.0121])\n",
      "Epoch 3239, Loss 2.928091\n",
      "  params :  tensor([  5.3551, -17.2336])\n",
      "  grad :  tensor([-0.0022,  0.0121])\n",
      "Epoch 3240, Loss 2.928090\n",
      "  params :  tensor([  5.3552, -17.2338])\n",
      "  grad :  tensor([-0.0021,  0.0121])\n",
      "Epoch 3241, Loss 2.928088\n",
      "  params :  tensor([  5.3552, -17.2339])\n",
      "  grad :  tensor([-0.0021,  0.0121])\n",
      "Epoch 3242, Loss 2.928086\n",
      "  params :  tensor([  5.3552, -17.2340])\n",
      "  grad :  tensor([-0.0021,  0.0120])\n",
      "Epoch 3243, Loss 2.928085\n",
      "  params :  tensor([  5.3552, -17.2341])\n",
      "  grad :  tensor([-0.0021,  0.0120])\n",
      "Epoch 3244, Loss 2.928084\n",
      "  params :  tensor([  5.3553, -17.2342])\n",
      "  grad :  tensor([-0.0021,  0.0120])\n",
      "Epoch 3245, Loss 2.928082\n",
      "  params :  tensor([  5.3553, -17.2344])\n",
      "  grad :  tensor([-0.0021,  0.0120])\n",
      "Epoch 3246, Loss 2.928080\n",
      "  params :  tensor([  5.3553, -17.2345])\n",
      "  grad :  tensor([-0.0021,  0.0120])\n",
      "Epoch 3247, Loss 2.928079\n",
      "  params :  tensor([  5.3553, -17.2346])\n",
      "  grad :  tensor([-0.0021,  0.0119])\n",
      "Epoch 3248, Loss 2.928076\n",
      "  params :  tensor([  5.3553, -17.2347])\n",
      "  grad :  tensor([-0.0021,  0.0119])\n",
      "Epoch 3249, Loss 2.928077\n",
      "  params :  tensor([  5.3554, -17.2348])\n",
      "  grad :  tensor([-0.0021,  0.0119])\n",
      "Epoch 3250, Loss 2.928075\n",
      "  params :  tensor([  5.3554, -17.2350])\n",
      "  grad :  tensor([-0.0021,  0.0119])\n",
      "Epoch 3251, Loss 2.928072\n",
      "  params :  tensor([  5.3554, -17.2351])\n",
      "  grad :  tensor([-0.0021,  0.0119])\n",
      "Epoch 3252, Loss 2.928072\n",
      "  params :  tensor([  5.3554, -17.2352])\n",
      "  grad :  tensor([-0.0021,  0.0118])\n",
      "Epoch 3253, Loss 2.928071\n",
      "  params :  tensor([  5.3554, -17.2353])\n",
      "  grad :  tensor([-0.0021,  0.0118])\n",
      "Epoch 3254, Loss 2.928068\n",
      "  params :  tensor([  5.3555, -17.2354])\n",
      "  grad :  tensor([-0.0021,  0.0118])\n",
      "Epoch 3255, Loss 2.928069\n",
      "  params :  tensor([  5.3555, -17.2355])\n",
      "  grad :  tensor([-0.0021,  0.0118])\n",
      "Epoch 3256, Loss 2.928066\n",
      "  params :  tensor([  5.3555, -17.2357])\n",
      "  grad :  tensor([-0.0021,  0.0118])\n",
      "Epoch 3257, Loss 2.928065\n",
      "  params :  tensor([  5.3555, -17.2358])\n",
      "  grad :  tensor([-0.0021,  0.0117])\n",
      "Epoch 3258, Loss 2.928064\n",
      "  params :  tensor([  5.3555, -17.2359])\n",
      "  grad :  tensor([-0.0021,  0.0117])\n",
      "Epoch 3259, Loss 2.928061\n",
      "  params :  tensor([  5.3556, -17.2360])\n",
      "  grad :  tensor([-0.0021,  0.0117])\n",
      "Epoch 3260, Loss 2.928060\n",
      "  params :  tensor([  5.3556, -17.2361])\n",
      "  grad :  tensor([-0.0021,  0.0117])\n",
      "Epoch 3261, Loss 2.928057\n",
      "  params :  tensor([  5.3556, -17.2362])\n",
      "  grad :  tensor([-0.0021,  0.0117])\n",
      "Epoch 3262, Loss 2.928058\n",
      "  params :  tensor([  5.3556, -17.2364])\n",
      "  grad :  tensor([-0.0021,  0.0116])\n",
      "Epoch 3263, Loss 2.928056\n",
      "  params :  tensor([  5.3557, -17.2365])\n",
      "  grad :  tensor([-0.0021,  0.0116])\n",
      "Epoch 3264, Loss 2.928055\n",
      "  params :  tensor([  5.3557, -17.2366])\n",
      "  grad :  tensor([-0.0021,  0.0116])\n",
      "Epoch 3265, Loss 2.928052\n",
      "  params :  tensor([  5.3557, -17.2367])\n",
      "  grad :  tensor([-0.0021,  0.0116])\n",
      "Epoch 3266, Loss 2.928053\n",
      "  params :  tensor([  5.3557, -17.2368])\n",
      "  grad :  tensor([-0.0021,  0.0116])\n",
      "Epoch 3267, Loss 2.928051\n",
      "  params :  tensor([  5.3557, -17.2369])\n",
      "  grad :  tensor([-0.0021,  0.0115])\n",
      "Epoch 3268, Loss 2.928050\n",
      "  params :  tensor([  5.3558, -17.2371])\n",
      "  grad :  tensor([-0.0021,  0.0115])\n",
      "Epoch 3269, Loss 2.928047\n",
      "  params :  tensor([  5.3558, -17.2372])\n",
      "  grad :  tensor([-0.0020,  0.0115])\n",
      "Epoch 3270, Loss 2.928046\n",
      "  params :  tensor([  5.3558, -17.2373])\n",
      "  grad :  tensor([-0.0020,  0.0115])\n",
      "Epoch 3271, Loss 2.928046\n",
      "  params :  tensor([  5.3558, -17.2374])\n",
      "  grad :  tensor([-0.0020,  0.0115])\n",
      "Epoch 3272, Loss 2.928044\n",
      "  params :  tensor([  5.3558, -17.2375])\n",
      "  grad :  tensor([-0.0020,  0.0115])\n",
      "Epoch 3273, Loss 2.928042\n",
      "  params :  tensor([  5.3559, -17.2376])\n",
      "  grad :  tensor([-0.0020,  0.0114])\n",
      "Epoch 3274, Loss 2.928040\n",
      "  params :  tensor([  5.3559, -17.2377])\n",
      "  grad :  tensor([-0.0020,  0.0114])\n",
      "Epoch 3275, Loss 2.928040\n",
      "  params :  tensor([  5.3559, -17.2379])\n",
      "  grad :  tensor([-0.0020,  0.0114])\n",
      "Epoch 3276, Loss 2.928036\n",
      "  params :  tensor([  5.3559, -17.2380])\n",
      "  grad :  tensor([-0.0020,  0.0114])\n",
      "Epoch 3277, Loss 2.928036\n",
      "  params :  tensor([  5.3559, -17.2381])\n",
      "  grad :  tensor([-0.0020,  0.0113])\n",
      "Epoch 3278, Loss 2.928037\n",
      "  params :  tensor([  5.3560, -17.2382])\n",
      "  grad :  tensor([-0.0020,  0.0113])\n",
      "Epoch 3279, Loss 2.928034\n",
      "  params :  tensor([  5.3560, -17.2383])\n",
      "  grad :  tensor([-0.0020,  0.0113])\n",
      "Epoch 3280, Loss 2.928034\n",
      "  params :  tensor([  5.3560, -17.2384])\n",
      "  grad :  tensor([-0.0020,  0.0113])\n",
      "Epoch 3281, Loss 2.928031\n",
      "  params :  tensor([  5.3560, -17.2385])\n",
      "  grad :  tensor([-0.0020,  0.0113])\n",
      "Epoch 3282, Loss 2.928032\n",
      "  params :  tensor([  5.3560, -17.2386])\n",
      "  grad :  tensor([-0.0020,  0.0113])\n",
      "Epoch 3283, Loss 2.928028\n",
      "  params :  tensor([  5.3561, -17.2388])\n",
      "  grad :  tensor([-0.0020,  0.0112])\n",
      "Epoch 3284, Loss 2.928027\n",
      "  params :  tensor([  5.3561, -17.2389])\n",
      "  grad :  tensor([-0.0020,  0.0112])\n",
      "Epoch 3285, Loss 2.928026\n",
      "  params :  tensor([  5.3561, -17.2390])\n",
      "  grad :  tensor([-0.0020,  0.0112])\n",
      "Epoch 3286, Loss 2.928025\n",
      "  params :  tensor([  5.3561, -17.2391])\n",
      "  grad :  tensor([-0.0020,  0.0112])\n",
      "Epoch 3287, Loss 2.928024\n",
      "  params :  tensor([  5.3561, -17.2392])\n",
      "  grad :  tensor([-0.0020,  0.0112])\n",
      "Epoch 3288, Loss 2.928022\n",
      "  params :  tensor([  5.3562, -17.2393])\n",
      "  grad :  tensor([-0.0020,  0.0111])\n",
      "Epoch 3289, Loss 2.928023\n",
      "  params :  tensor([  5.3562, -17.2394])\n",
      "  grad :  tensor([-0.0020,  0.0111])\n",
      "Epoch 3290, Loss 2.928021\n",
      "  params :  tensor([  5.3562, -17.2395])\n",
      "  grad :  tensor([-0.0020,  0.0111])\n",
      "Epoch 3291, Loss 2.928019\n",
      "  params :  tensor([  5.3562, -17.2397])\n",
      "  grad :  tensor([-0.0020,  0.0111])\n",
      "Epoch 3292, Loss 2.928018\n",
      "  params :  tensor([  5.3562, -17.2398])\n",
      "  grad :  tensor([-0.0020,  0.0111])\n",
      "Epoch 3293, Loss 2.928017\n",
      "  params :  tensor([  5.3563, -17.2399])\n",
      "  grad :  tensor([-0.0020,  0.0110])\n",
      "Epoch 3294, Loss 2.928015\n",
      "  params :  tensor([  5.3563, -17.2400])\n",
      "  grad :  tensor([-0.0020,  0.0110])\n",
      "Epoch 3295, Loss 2.928013\n",
      "  params :  tensor([  5.3563, -17.2401])\n",
      "  grad :  tensor([-0.0020,  0.0110])\n",
      "Epoch 3296, Loss 2.928013\n",
      "  params :  tensor([  5.3563, -17.2402])\n",
      "  grad :  tensor([-0.0019,  0.0110])\n",
      "Epoch 3297, Loss 2.928011\n",
      "  params :  tensor([  5.3563, -17.2403])\n",
      "  grad :  tensor([-0.0019,  0.0110])\n",
      "Epoch 3298, Loss 2.928009\n",
      "  params :  tensor([  5.3563, -17.2404])\n",
      "  grad :  tensor([-0.0019,  0.0110])\n",
      "Epoch 3299, Loss 2.928008\n",
      "  params :  tensor([  5.3564, -17.2405])\n",
      "  grad :  tensor([-0.0019,  0.0109])\n",
      "Epoch 3300, Loss 2.928006\n",
      "  params :  tensor([  5.3564, -17.2406])\n",
      "  grad :  tensor([-0.0019,  0.0109])\n",
      "Epoch 3301, Loss 2.928007\n",
      "  params :  tensor([  5.3564, -17.2407])\n",
      "  grad :  tensor([-0.0019,  0.0109])\n",
      "Epoch 3302, Loss 2.928007\n",
      "  params :  tensor([  5.3564, -17.2409])\n",
      "  grad :  tensor([-0.0019,  0.0109])\n",
      "Epoch 3303, Loss 2.928004\n",
      "  params :  tensor([  5.3564, -17.2410])\n",
      "  grad :  tensor([-0.0019,  0.0109])\n",
      "Epoch 3304, Loss 2.928002\n",
      "  params :  tensor([  5.3565, -17.2411])\n",
      "  grad :  tensor([-0.0019,  0.0108])\n",
      "Epoch 3305, Loss 2.928002\n",
      "  params :  tensor([  5.3565, -17.2412])\n",
      "  grad :  tensor([-0.0019,  0.0108])\n",
      "Epoch 3306, Loss 2.928000\n",
      "  params :  tensor([  5.3565, -17.2413])\n",
      "  grad :  tensor([-0.0019,  0.0108])\n",
      "Epoch 3307, Loss 2.928000\n",
      "  params :  tensor([  5.3565, -17.2414])\n",
      "  grad :  tensor([-0.0019,  0.0108])\n",
      "Epoch 3308, Loss 2.927998\n",
      "  params :  tensor([  5.3565, -17.2415])\n",
      "  grad :  tensor([-0.0019,  0.0108])\n",
      "Epoch 3309, Loss 2.927995\n",
      "  params :  tensor([  5.3566, -17.2416])\n",
      "  grad :  tensor([-0.0019,  0.0107])\n",
      "Epoch 3310, Loss 2.927995\n",
      "  params :  tensor([  5.3566, -17.2417])\n",
      "  grad :  tensor([-0.0019,  0.0107])\n",
      "Epoch 3311, Loss 2.927994\n",
      "  params :  tensor([  5.3566, -17.2418])\n",
      "  grad :  tensor([-0.0019,  0.0107])\n",
      "Epoch 3312, Loss 2.927994\n",
      "  params :  tensor([  5.3566, -17.2419])\n",
      "  grad :  tensor([-0.0019,  0.0107])\n",
      "Epoch 3313, Loss 2.927991\n",
      "  params :  tensor([  5.3566, -17.2420])\n",
      "  grad :  tensor([-0.0019,  0.0107])\n",
      "Epoch 3314, Loss 2.927991\n",
      "  params :  tensor([  5.3567, -17.2421])\n",
      "  grad :  tensor([-0.0019,  0.0107])\n",
      "Epoch 3315, Loss 2.927990\n",
      "  params :  tensor([  5.3567, -17.2423])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad :  tensor([-0.0019,  0.0106])\n",
      "Epoch 3316, Loss 2.927989\n",
      "  params :  tensor([  5.3567, -17.2424])\n",
      "  grad :  tensor([-0.0019,  0.0106])\n",
      "Epoch 3317, Loss 2.927988\n",
      "  params :  tensor([  5.3567, -17.2425])\n",
      "  grad :  tensor([-0.0019,  0.0106])\n",
      "Epoch 3318, Loss 2.927986\n",
      "  params :  tensor([  5.3567, -17.2426])\n",
      "  grad :  tensor([-0.0019,  0.0106])\n",
      "Epoch 3319, Loss 2.927985\n",
      "  params :  tensor([  5.3567, -17.2427])\n",
      "  grad :  tensor([-0.0019,  0.0106])\n",
      "Epoch 3320, Loss 2.927983\n",
      "  params :  tensor([  5.3568, -17.2428])\n",
      "  grad :  tensor([-0.0018,  0.0106])\n",
      "Epoch 3321, Loss 2.927983\n",
      "  params :  tensor([  5.3568, -17.2429])\n",
      "  grad :  tensor([-0.0018,  0.0105])\n",
      "Epoch 3322, Loss 2.927981\n",
      "  params :  tensor([  5.3568, -17.2430])\n",
      "  grad :  tensor([-0.0018,  0.0105])\n",
      "Epoch 3323, Loss 2.927980\n",
      "  params :  tensor([  5.3568, -17.2431])\n",
      "  grad :  tensor([-0.0018,  0.0105])\n",
      "Epoch 3324, Loss 2.927979\n",
      "  params :  tensor([  5.3568, -17.2432])\n",
      "  grad :  tensor([-0.0018,  0.0105])\n",
      "Epoch 3325, Loss 2.927979\n",
      "  params :  tensor([  5.3569, -17.2433])\n",
      "  grad :  tensor([-0.0018,  0.0105])\n",
      "Epoch 3326, Loss 2.927977\n",
      "  params :  tensor([  5.3569, -17.2434])\n",
      "  grad :  tensor([-0.0018,  0.0104])\n",
      "Epoch 3327, Loss 2.927975\n",
      "  params :  tensor([  5.3569, -17.2435])\n",
      "  grad :  tensor([-0.0019,  0.0104])\n",
      "Epoch 3328, Loss 2.927973\n",
      "  params :  tensor([  5.3569, -17.2436])\n",
      "  grad :  tensor([-0.0018,  0.0104])\n",
      "Epoch 3329, Loss 2.927974\n",
      "  params :  tensor([  5.3569, -17.2437])\n",
      "  grad :  tensor([-0.0018,  0.0104])\n",
      "Epoch 3330, Loss 2.927974\n",
      "  params :  tensor([  5.3570, -17.2438])\n",
      "  grad :  tensor([-0.0018,  0.0104])\n",
      "Epoch 3331, Loss 2.927972\n",
      "  params :  tensor([  5.3570, -17.2439])\n",
      "  grad :  tensor([-0.0018,  0.0104])\n",
      "Epoch 3332, Loss 2.927972\n",
      "  params :  tensor([  5.3570, -17.2440])\n",
      "  grad :  tensor([-0.0018,  0.0103])\n",
      "Epoch 3333, Loss 2.927969\n",
      "  params :  tensor([  5.3570, -17.2441])\n",
      "  grad :  tensor([-0.0018,  0.0103])\n",
      "Epoch 3334, Loss 2.927969\n",
      "  params :  tensor([  5.3570, -17.2442])\n",
      "  grad :  tensor([-0.0018,  0.0103])\n",
      "Epoch 3335, Loss 2.927967\n",
      "  params :  tensor([  5.3570, -17.2443])\n",
      "  grad :  tensor([-0.0018,  0.0103])\n",
      "Epoch 3336, Loss 2.927967\n",
      "  params :  tensor([  5.3571, -17.2444])\n",
      "  grad :  tensor([-0.0018,  0.0103])\n",
      "Epoch 3337, Loss 2.927963\n",
      "  params :  tensor([  5.3571, -17.2446])\n",
      "  grad :  tensor([-0.0018,  0.0102])\n",
      "Epoch 3338, Loss 2.927963\n",
      "  params :  tensor([  5.3571, -17.2447])\n",
      "  grad :  tensor([-0.0018,  0.0102])\n",
      "Epoch 3339, Loss 2.927962\n",
      "  params :  tensor([  5.3571, -17.2448])\n",
      "  grad :  tensor([-0.0018,  0.0102])\n",
      "Epoch 3340, Loss 2.927962\n",
      "  params :  tensor([  5.3571, -17.2449])\n",
      "  grad :  tensor([-0.0018,  0.0102])\n",
      "Epoch 3341, Loss 2.927960\n",
      "  params :  tensor([  5.3572, -17.2450])\n",
      "  grad :  tensor([-0.0018,  0.0102])\n",
      "Epoch 3342, Loss 2.927960\n",
      "  params :  tensor([  5.3572, -17.2451])\n",
      "  grad :  tensor([-0.0018,  0.0102])\n",
      "Epoch 3343, Loss 2.927959\n",
      "  params :  tensor([  5.3572, -17.2452])\n",
      "  grad :  tensor([-0.0018,  0.0101])\n",
      "Epoch 3344, Loss 2.927958\n",
      "  params :  tensor([  5.3572, -17.2453])\n",
      "  grad :  tensor([-0.0018,  0.0101])\n",
      "Epoch 3345, Loss 2.927956\n",
      "  params :  tensor([  5.3572, -17.2454])\n",
      "  grad :  tensor([-0.0018,  0.0101])\n",
      "Epoch 3346, Loss 2.927956\n",
      "  params :  tensor([  5.3572, -17.2455])\n",
      "  grad :  tensor([-0.0018,  0.0101])\n",
      "Epoch 3347, Loss 2.927955\n",
      "  params :  tensor([  5.3573, -17.2456])\n",
      "  grad :  tensor([-0.0018,  0.0101])\n",
      "Epoch 3348, Loss 2.927953\n",
      "  params :  tensor([  5.3573, -17.2457])\n",
      "  grad :  tensor([-0.0018,  0.0101])\n",
      "Epoch 3349, Loss 2.927953\n",
      "  params :  tensor([  5.3573, -17.2458])\n",
      "  grad :  tensor([-0.0018,  0.0100])\n",
      "Epoch 3350, Loss 2.927951\n",
      "  params :  tensor([  5.3573, -17.2459])\n",
      "  grad :  tensor([-0.0018,  0.0100])\n",
      "Epoch 3351, Loss 2.927950\n",
      "  params :  tensor([  5.3573, -17.2460])\n",
      "  grad :  tensor([-0.0018,  0.0100])\n",
      "Epoch 3352, Loss 2.927948\n",
      "  params :  tensor([  5.3573, -17.2461])\n",
      "  grad :  tensor([-0.0018,  0.0100])\n",
      "Epoch 3353, Loss 2.927947\n",
      "  params :  tensor([  5.3574, -17.2462])\n",
      "  grad :  tensor([-0.0017,  0.0100])\n",
      "Epoch 3354, Loss 2.927948\n",
      "  params :  tensor([  5.3574, -17.2463])\n",
      "  grad :  tensor([-0.0018,  0.0100])\n",
      "Epoch 3355, Loss 2.927945\n",
      "  params :  tensor([  5.3574, -17.2464])\n",
      "  grad :  tensor([-0.0017,  0.0099])\n",
      "Epoch 3356, Loss 2.927944\n",
      "  params :  tensor([  5.3574, -17.2465])\n",
      "  grad :  tensor([-0.0017,  0.0099])\n",
      "Epoch 3357, Loss 2.927943\n",
      "  params :  tensor([  5.3574, -17.2466])\n",
      "  grad :  tensor([-0.0018,  0.0099])\n",
      "Epoch 3358, Loss 2.927944\n",
      "  params :  tensor([  5.3575, -17.2467])\n",
      "  grad :  tensor([-0.0017,  0.0099])\n",
      "Epoch 3359, Loss 2.927942\n",
      "  params :  tensor([  5.3575, -17.2468])\n",
      "  grad :  tensor([-0.0017,  0.0099])\n",
      "Epoch 3360, Loss 2.927941\n",
      "  params :  tensor([  5.3575, -17.2469])\n",
      "  grad :  tensor([-0.0018,  0.0099])\n",
      "Epoch 3361, Loss 2.927940\n",
      "  params :  tensor([  5.3575, -17.2470])\n",
      "  grad :  tensor([-0.0017,  0.0098])\n",
      "Epoch 3362, Loss 2.927938\n",
      "  params :  tensor([  5.3575, -17.2471])\n",
      "  grad :  tensor([-0.0017,  0.0098])\n",
      "Epoch 3363, Loss 2.927938\n",
      "  params :  tensor([  5.3575, -17.2472])\n",
      "  grad :  tensor([-0.0018,  0.0098])\n",
      "Epoch 3364, Loss 2.927936\n",
      "  params :  tensor([  5.3576, -17.2473])\n",
      "  grad :  tensor([-0.0017,  0.0098])\n",
      "Epoch 3365, Loss 2.927936\n",
      "  params :  tensor([  5.3576, -17.2474])\n",
      "  grad :  tensor([-0.0017,  0.0098])\n",
      "Epoch 3366, Loss 2.927937\n",
      "  params :  tensor([  5.3576, -17.2474])\n",
      "  grad :  tensor([-0.0017,  0.0098])\n",
      "Epoch 3367, Loss 2.927934\n",
      "  params :  tensor([  5.3576, -17.2475])\n",
      "  grad :  tensor([-0.0017,  0.0097])\n",
      "Epoch 3368, Loss 2.927933\n",
      "  params :  tensor([  5.3576, -17.2476])\n",
      "  grad :  tensor([-0.0017,  0.0097])\n",
      "Epoch 3369, Loss 2.927932\n",
      "  params :  tensor([  5.3576, -17.2477])\n",
      "  grad :  tensor([-0.0017,  0.0097])\n",
      "Epoch 3370, Loss 2.927930\n",
      "  params :  tensor([  5.3577, -17.2478])\n",
      "  grad :  tensor([-0.0017,  0.0097])\n",
      "Epoch 3371, Loss 2.927928\n",
      "  params :  tensor([  5.3577, -17.2479])\n",
      "  grad :  tensor([-0.0017,  0.0097])\n",
      "Epoch 3372, Loss 2.927931\n",
      "  params :  tensor([  5.3577, -17.2480])\n",
      "  grad :  tensor([-0.0017,  0.0097])\n",
      "Epoch 3373, Loss 2.927929\n",
      "  params :  tensor([  5.3577, -17.2481])\n",
      "  grad :  tensor([-0.0017,  0.0096])\n",
      "Epoch 3374, Loss 2.927927\n",
      "  params :  tensor([  5.3577, -17.2482])\n",
      "  grad :  tensor([-0.0017,  0.0096])\n",
      "Epoch 3375, Loss 2.927926\n",
      "  params :  tensor([  5.3577, -17.2483])\n",
      "  grad :  tensor([-0.0017,  0.0096])\n",
      "Epoch 3376, Loss 2.927925\n",
      "  params :  tensor([  5.3578, -17.2484])\n",
      "  grad :  tensor([-0.0017,  0.0096])\n",
      "Epoch 3377, Loss 2.927924\n",
      "  params :  tensor([  5.3578, -17.2485])\n",
      "  grad :  tensor([-0.0017,  0.0096])\n",
      "Epoch 3378, Loss 2.927923\n",
      "  params :  tensor([  5.3578, -17.2486])\n",
      "  grad :  tensor([-0.0017,  0.0096])\n",
      "Epoch 3379, Loss 2.927924\n",
      "  params :  tensor([  5.3578, -17.2487])\n",
      "  grad :  tensor([-0.0017,  0.0095])\n",
      "Epoch 3380, Loss 2.927922\n",
      "  params :  tensor([  5.3578, -17.2488])\n",
      "  grad :  tensor([-0.0017,  0.0095])\n",
      "Epoch 3381, Loss 2.927922\n",
      "  params :  tensor([  5.3578, -17.2489])\n",
      "  grad :  tensor([-0.0017,  0.0095])\n",
      "Epoch 3382, Loss 2.927920\n",
      "  params :  tensor([  5.3579, -17.2490])\n",
      "  grad :  tensor([-0.0017,  0.0095])\n",
      "Epoch 3383, Loss 2.927918\n",
      "  params :  tensor([  5.3579, -17.2491])\n",
      "  grad :  tensor([-0.0017,  0.0095])\n",
      "Epoch 3384, Loss 2.927917\n",
      "  params :  tensor([  5.3579, -17.2492])\n",
      "  grad :  tensor([-0.0017,  0.0095])\n",
      "Epoch 3385, Loss 2.927917\n",
      "  params :  tensor([  5.3579, -17.2493])\n",
      "  grad :  tensor([-0.0017,  0.0094])\n",
      "Epoch 3386, Loss 2.927915\n",
      "  params :  tensor([  5.3579, -17.2494])\n",
      "  grad :  tensor([-0.0017,  0.0094])\n",
      "Epoch 3387, Loss 2.927915\n",
      "  params :  tensor([  5.3579, -17.2495])\n",
      "  grad :  tensor([-0.0017,  0.0094])\n",
      "Epoch 3388, Loss 2.927914\n",
      "  params :  tensor([  5.3580, -17.2496])\n",
      "  grad :  tensor([-0.0016,  0.0094])\n",
      "Epoch 3389, Loss 2.927913\n",
      "  params :  tensor([  5.3580, -17.2496])\n",
      "  grad :  tensor([-0.0017,  0.0094])\n",
      "Epoch 3390, Loss 2.927911\n",
      "  params :  tensor([  5.3580, -17.2497])\n",
      "  grad :  tensor([-0.0016,  0.0094])\n",
      "Epoch 3391, Loss 2.927913\n",
      "  params :  tensor([  5.3580, -17.2498])\n",
      "  grad :  tensor([-0.0017,  0.0093])\n",
      "Epoch 3392, Loss 2.927911\n",
      "  params :  tensor([  5.3580, -17.2499])\n",
      "  grad :  tensor([-0.0016,  0.0093])\n",
      "Epoch 3393, Loss 2.927910\n",
      "  params :  tensor([  5.3580, -17.2500])\n",
      "  grad :  tensor([-0.0016,  0.0093])\n",
      "Epoch 3394, Loss 2.927909\n",
      "  params :  tensor([  5.3581, -17.2501])\n",
      "  grad :  tensor([-0.0016,  0.0093])\n",
      "Epoch 3395, Loss 2.927908\n",
      "  params :  tensor([  5.3581, -17.2502])\n",
      "  grad :  tensor([-0.0016,  0.0093])\n",
      "Epoch 3396, Loss 2.927907\n",
      "  params :  tensor([  5.3581, -17.2503])\n",
      "  grad :  tensor([-0.0017,  0.0093])\n",
      "Epoch 3397, Loss 2.927906\n",
      "  params :  tensor([  5.3581, -17.2504])\n",
      "  grad :  tensor([-0.0016,  0.0093])\n",
      "Epoch 3398, Loss 2.927905\n",
      "  params :  tensor([  5.3581, -17.2505])\n",
      "  grad :  tensor([-0.0017,  0.0092])\n",
      "Epoch 3399, Loss 2.927905\n",
      "  params :  tensor([  5.3581, -17.2506])\n",
      "  grad :  tensor([-0.0016,  0.0092])\n",
      "Epoch 3400, Loss 2.927904\n",
      "  params :  tensor([  5.3582, -17.2507])\n",
      "  grad :  tensor([-0.0016,  0.0092])\n",
      "Epoch 3401, Loss 2.927902\n",
      "  params :  tensor([  5.3582, -17.2508])\n",
      "  grad :  tensor([-0.0016,  0.0092])\n",
      "Epoch 3402, Loss 2.927902\n",
      "  params :  tensor([  5.3582, -17.2509])\n",
      "  grad :  tensor([-0.0016,  0.0092])\n",
      "Epoch 3403, Loss 2.927902\n",
      "  params :  tensor([  5.3582, -17.2509])\n",
      "  grad :  tensor([-0.0016,  0.0092])\n",
      "Epoch 3404, Loss 2.927899\n",
      "  params :  tensor([  5.3582, -17.2510])\n",
      "  grad :  tensor([-0.0016,  0.0091])\n",
      "Epoch 3405, Loss 2.927899\n",
      "  params :  tensor([  5.3582, -17.2511])\n",
      "  grad :  tensor([-0.0016,  0.0091])\n",
      "Epoch 3406, Loss 2.927898\n",
      "  params :  tensor([  5.3583, -17.2512])\n",
      "  grad :  tensor([-0.0016,  0.0091])\n",
      "Epoch 3407, Loss 2.927899\n",
      "  params :  tensor([  5.3583, -17.2513])\n",
      "  grad :  tensor([-0.0016,  0.0091])\n",
      "Epoch 3408, Loss 2.927896\n",
      "  params :  tensor([  5.3583, -17.2514])\n",
      "  grad :  tensor([-0.0016,  0.0091])\n",
      "Epoch 3409, Loss 2.927895\n",
      "  params :  tensor([  5.3583, -17.2515])\n",
      "  grad :  tensor([-0.0016,  0.0091])\n",
      "Epoch 3410, Loss 2.927896\n",
      "  params :  tensor([  5.3583, -17.2516])\n",
      "  grad :  tensor([-0.0016,  0.0091])\n",
      "Epoch 3411, Loss 2.927894\n",
      "  params :  tensor([  5.3583, -17.2517])\n",
      "  grad :  tensor([-0.0016,  0.0090])\n",
      "Epoch 3412, Loss 2.927892\n",
      "  params :  tensor([  5.3584, -17.2518])\n",
      "  grad :  tensor([-0.0016,  0.0090])\n",
      "Epoch 3413, Loss 2.927892\n",
      "  params :  tensor([  5.3584, -17.2519])\n",
      "  grad :  tensor([-0.0016,  0.0090])\n",
      "Epoch 3414, Loss 2.927891\n",
      "  params :  tensor([  5.3584, -17.2519])\n",
      "  grad :  tensor([-0.0016,  0.0090])\n",
      "Epoch 3415, Loss 2.927891\n",
      "  params :  tensor([  5.3584, -17.2520])\n",
      "  grad :  tensor([-0.0016,  0.0090])\n",
      "Epoch 3416, Loss 2.927890\n",
      "  params :  tensor([  5.3584, -17.2521])\n",
      "  grad :  tensor([-0.0016,  0.0090])\n",
      "Epoch 3417, Loss 2.927891\n",
      "  params :  tensor([  5.3584, -17.2522])\n",
      "  grad :  tensor([-0.0016,  0.0089])\n",
      "Epoch 3418, Loss 2.927888\n",
      "  params :  tensor([  5.3584, -17.2523])\n",
      "  grad :  tensor([-0.0016,  0.0089])\n",
      "Epoch 3419, Loss 2.927888\n",
      "  params :  tensor([  5.3585, -17.2524])\n",
      "  grad :  tensor([-0.0016,  0.0089])\n",
      "Epoch 3420, Loss 2.927886\n",
      "  params :  tensor([  5.3585, -17.2525])\n",
      "  grad :  tensor([-0.0016,  0.0089])\n",
      "Epoch 3421, Loss 2.927887\n",
      "  params :  tensor([  5.3585, -17.2526])\n",
      "  grad :  tensor([-0.0016,  0.0089])\n",
      "Epoch 3422, Loss 2.927886\n",
      "  params :  tensor([  5.3585, -17.2527])\n",
      "  grad :  tensor([-0.0016,  0.0089])\n",
      "Epoch 3423, Loss 2.927884\n",
      "  params :  tensor([  5.3585, -17.2527])\n",
      "  grad :  tensor([-0.0016,  0.0089])\n",
      "Epoch 3424, Loss 2.927883\n",
      "  params :  tensor([  5.3585, -17.2528])\n",
      "  grad :  tensor([-0.0015,  0.0088])\n",
      "Epoch 3425, Loss 2.927881\n",
      "  params :  tensor([  5.3586, -17.2529])\n",
      "  grad :  tensor([-0.0015,  0.0088])\n",
      "Epoch 3426, Loss 2.927881\n",
      "  params :  tensor([  5.3586, -17.2530])\n",
      "  grad :  tensor([-0.0016,  0.0088])\n",
      "Epoch 3427, Loss 2.927880\n",
      "  params :  tensor([  5.3586, -17.2531])\n",
      "  grad :  tensor([-0.0015,  0.0088])\n",
      "Epoch 3428, Loss 2.927880\n",
      "  params :  tensor([  5.3586, -17.2532])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad :  tensor([-0.0016,  0.0088])\n",
      "Epoch 3429, Loss 2.927879\n",
      "  params :  tensor([  5.3586, -17.2533])\n",
      "  grad :  tensor([-0.0015,  0.0088])\n",
      "Epoch 3430, Loss 2.927877\n",
      "  params :  tensor([  5.3586, -17.2534])\n",
      "  grad :  tensor([-0.0016,  0.0087])\n",
      "Epoch 3431, Loss 2.927876\n",
      "  params :  tensor([  5.3586, -17.2534])\n",
      "  grad :  tensor([-0.0015,  0.0087])\n",
      "Epoch 3432, Loss 2.927876\n",
      "  params :  tensor([  5.3587, -17.2535])\n",
      "  grad :  tensor([-0.0015,  0.0087])\n",
      "Epoch 3433, Loss 2.927876\n",
      "  params :  tensor([  5.3587, -17.2536])\n",
      "  grad :  tensor([-0.0015,  0.0087])\n",
      "Epoch 3434, Loss 2.927876\n",
      "  params :  tensor([  5.3587, -17.2537])\n",
      "  grad :  tensor([-0.0016,  0.0087])\n",
      "Epoch 3435, Loss 2.927876\n",
      "  params :  tensor([  5.3587, -17.2538])\n",
      "  grad :  tensor([-0.0016,  0.0087])\n",
      "Epoch 3436, Loss 2.927875\n",
      "  params :  tensor([  5.3587, -17.2539])\n",
      "  grad :  tensor([-0.0015,  0.0087])\n",
      "Epoch 3437, Loss 2.927873\n",
      "  params :  tensor([  5.3587, -17.2540])\n",
      "  grad :  tensor([-0.0015,  0.0087])\n",
      "Epoch 3438, Loss 2.927872\n",
      "  params :  tensor([  5.3588, -17.2541])\n",
      "  grad :  tensor([-0.0015,  0.0086])\n",
      "Epoch 3439, Loss 2.927871\n",
      "  params :  tensor([  5.3588, -17.2541])\n",
      "  grad :  tensor([-0.0015,  0.0086])\n",
      "Epoch 3440, Loss 2.927870\n",
      "  params :  tensor([  5.3588, -17.2542])\n",
      "  grad :  tensor([-0.0015,  0.0086])\n",
      "Epoch 3441, Loss 2.927871\n",
      "  params :  tensor([  5.3588, -17.2543])\n",
      "  grad :  tensor([-0.0015,  0.0086])\n",
      "Epoch 3442, Loss 2.927869\n",
      "  params :  tensor([  5.3588, -17.2544])\n",
      "  grad :  tensor([-0.0015,  0.0086])\n",
      "Epoch 3443, Loss 2.927869\n",
      "  params :  tensor([  5.3588, -17.2545])\n",
      "  grad :  tensor([-0.0015,  0.0086])\n",
      "Epoch 3444, Loss 2.927867\n",
      "  params :  tensor([  5.3588, -17.2546])\n",
      "  grad :  tensor([-0.0015,  0.0085])\n",
      "Epoch 3445, Loss 2.927866\n",
      "  params :  tensor([  5.3589, -17.2547])\n",
      "  grad :  tensor([-0.0015,  0.0085])\n",
      "Epoch 3446, Loss 2.927866\n",
      "  params :  tensor([  5.3589, -17.2547])\n",
      "  grad :  tensor([-0.0015,  0.0085])\n",
      "Epoch 3447, Loss 2.927866\n",
      "  params :  tensor([  5.3589, -17.2548])\n",
      "  grad :  tensor([-0.0015,  0.0085])\n",
      "Epoch 3448, Loss 2.927864\n",
      "  params :  tensor([  5.3589, -17.2549])\n",
      "  grad :  tensor([-0.0015,  0.0085])\n",
      "Epoch 3449, Loss 2.927863\n",
      "  params :  tensor([  5.3589, -17.2550])\n",
      "  grad :  tensor([-0.0015,  0.0085])\n",
      "Epoch 3450, Loss 2.927863\n",
      "  params :  tensor([  5.3589, -17.2551])\n",
      "  grad :  tensor([-0.0015,  0.0085])\n",
      "Epoch 3451, Loss 2.927862\n",
      "  params :  tensor([  5.3590, -17.2552])\n",
      "  grad :  tensor([-0.0015,  0.0084])\n",
      "Epoch 3452, Loss 2.927863\n",
      "  params :  tensor([  5.3590, -17.2552])\n",
      "  grad :  tensor([-0.0015,  0.0084])\n",
      "Epoch 3453, Loss 2.927860\n",
      "  params :  tensor([  5.3590, -17.2553])\n",
      "  grad :  tensor([-0.0015,  0.0084])\n",
      "Epoch 3454, Loss 2.927860\n",
      "  params :  tensor([  5.3590, -17.2554])\n",
      "  grad :  tensor([-0.0015,  0.0084])\n",
      "Epoch 3455, Loss 2.927860\n",
      "  params :  tensor([  5.3590, -17.2555])\n",
      "  grad :  tensor([-0.0015,  0.0084])\n",
      "Epoch 3456, Loss 2.927859\n",
      "  params :  tensor([  5.3590, -17.2556])\n",
      "  grad :  tensor([-0.0015,  0.0084])\n",
      "Epoch 3457, Loss 2.927858\n",
      "  params :  tensor([  5.3590, -17.2557])\n",
      "  grad :  tensor([-0.0015,  0.0084])\n",
      "Epoch 3458, Loss 2.927858\n",
      "  params :  tensor([  5.3591, -17.2557])\n",
      "  grad :  tensor([-0.0015,  0.0083])\n",
      "Epoch 3459, Loss 2.927856\n",
      "  params :  tensor([  5.3591, -17.2558])\n",
      "  grad :  tensor([-0.0015,  0.0083])\n",
      "Epoch 3460, Loss 2.927857\n",
      "  params :  tensor([  5.3591, -17.2559])\n",
      "  grad :  tensor([-0.0015,  0.0083])\n",
      "Epoch 3461, Loss 2.927854\n",
      "  params :  tensor([  5.3591, -17.2560])\n",
      "  grad :  tensor([-0.0015,  0.0083])\n",
      "Epoch 3462, Loss 2.927855\n",
      "  params :  tensor([  5.3591, -17.2561])\n",
      "  grad :  tensor([-0.0015,  0.0083])\n",
      "Epoch 3463, Loss 2.927854\n",
      "  params :  tensor([  5.3591, -17.2562])\n",
      "  grad :  tensor([-0.0015,  0.0083])\n",
      "Epoch 3464, Loss 2.927854\n",
      "  params :  tensor([  5.3591, -17.2562])\n",
      "  grad :  tensor([-0.0015,  0.0083])\n",
      "Epoch 3465, Loss 2.927851\n",
      "  params :  tensor([  5.3592, -17.2563])\n",
      "  grad :  tensor([-0.0014,  0.0082])\n",
      "Epoch 3466, Loss 2.927853\n",
      "  params :  tensor([  5.3592, -17.2564])\n",
      "  grad :  tensor([-0.0015,  0.0082])\n",
      "Epoch 3467, Loss 2.927852\n",
      "  params :  tensor([  5.3592, -17.2565])\n",
      "  grad :  tensor([-0.0014,  0.0082])\n",
      "Epoch 3468, Loss 2.927850\n",
      "  params :  tensor([  5.3592, -17.2566])\n",
      "  grad :  tensor([-0.0014,  0.0082])\n",
      "Epoch 3469, Loss 2.927849\n",
      "  params :  tensor([  5.3592, -17.2567])\n",
      "  grad :  tensor([-0.0015,  0.0082])\n",
      "Epoch 3470, Loss 2.927849\n",
      "  params :  tensor([  5.3592, -17.2567])\n",
      "  grad :  tensor([-0.0015,  0.0082])\n",
      "Epoch 3471, Loss 2.927848\n",
      "  params :  tensor([  5.3592, -17.2568])\n",
      "  grad :  tensor([-0.0014,  0.0082])\n",
      "Epoch 3472, Loss 2.927848\n",
      "  params :  tensor([  5.3593, -17.2569])\n",
      "  grad :  tensor([-0.0014,  0.0081])\n",
      "Epoch 3473, Loss 2.927846\n",
      "  params :  tensor([  5.3593, -17.2570])\n",
      "  grad :  tensor([-0.0015,  0.0081])\n",
      "Epoch 3474, Loss 2.927846\n",
      "  params :  tensor([  5.3593, -17.2571])\n",
      "  grad :  tensor([-0.0015,  0.0081])\n",
      "Epoch 3475, Loss 2.927845\n",
      "  params :  tensor([  5.3593, -17.2571])\n",
      "  grad :  tensor([-0.0014,  0.0081])\n",
      "Epoch 3476, Loss 2.927844\n",
      "  params :  tensor([  5.3593, -17.2572])\n",
      "  grad :  tensor([-0.0014,  0.0081])\n",
      "Epoch 3477, Loss 2.927844\n",
      "  params :  tensor([  5.3593, -17.2573])\n",
      "  grad :  tensor([-0.0014,  0.0081])\n",
      "Epoch 3478, Loss 2.927844\n",
      "  params :  tensor([  5.3593, -17.2574])\n",
      "  grad :  tensor([-0.0014,  0.0081])\n",
      "Epoch 3479, Loss 2.927843\n",
      "  params :  tensor([  5.3594, -17.2575])\n",
      "  grad :  tensor([-0.0014,  0.0081])\n",
      "Epoch 3480, Loss 2.927843\n",
      "  params :  tensor([  5.3594, -17.2575])\n",
      "  grad :  tensor([-0.0014,  0.0080])\n",
      "Epoch 3481, Loss 2.927842\n",
      "  params :  tensor([  5.3594, -17.2576])\n",
      "  grad :  tensor([-0.0014,  0.0080])\n",
      "Epoch 3482, Loss 2.927840\n",
      "  params :  tensor([  5.3594, -17.2577])\n",
      "  grad :  tensor([-0.0014,  0.0080])\n",
      "Epoch 3483, Loss 2.927842\n",
      "  params :  tensor([  5.3594, -17.2578])\n",
      "  grad :  tensor([-0.0014,  0.0080])\n",
      "Epoch 3484, Loss 2.927839\n",
      "  params :  tensor([  5.3594, -17.2579])\n",
      "  grad :  tensor([-0.0014,  0.0080])\n",
      "Epoch 3485, Loss 2.927838\n",
      "  params :  tensor([  5.3594, -17.2579])\n",
      "  grad :  tensor([-0.0014,  0.0080])\n",
      "Epoch 3486, Loss 2.927839\n",
      "  params :  tensor([  5.3595, -17.2580])\n",
      "  grad :  tensor([-0.0014,  0.0080])\n",
      "Epoch 3487, Loss 2.927838\n",
      "  params :  tensor([  5.3595, -17.2581])\n",
      "  grad :  tensor([-0.0014,  0.0079])\n",
      "Epoch 3488, Loss 2.927837\n",
      "  params :  tensor([  5.3595, -17.2582])\n",
      "  grad :  tensor([-0.0014,  0.0079])\n",
      "Epoch 3489, Loss 2.927835\n",
      "  params :  tensor([  5.3595, -17.2583])\n",
      "  grad :  tensor([-0.0014,  0.0079])\n",
      "Epoch 3490, Loss 2.927837\n",
      "  params :  tensor([  5.3595, -17.2583])\n",
      "  grad :  tensor([-0.0014,  0.0079])\n",
      "Epoch 3491, Loss 2.927836\n",
      "  params :  tensor([  5.3595, -17.2584])\n",
      "  grad :  tensor([-0.0014,  0.0079])\n",
      "Epoch 3492, Loss 2.927835\n",
      "  params :  tensor([  5.3595, -17.2585])\n",
      "  grad :  tensor([-0.0014,  0.0079])\n",
      "Epoch 3493, Loss 2.927833\n",
      "  params :  tensor([  5.3596, -17.2586])\n",
      "  grad :  tensor([-0.0014,  0.0079])\n",
      "Epoch 3494, Loss 2.927833\n",
      "  params :  tensor([  5.3596, -17.2587])\n",
      "  grad :  tensor([-0.0014,  0.0079])\n",
      "Epoch 3495, Loss 2.927833\n",
      "  params :  tensor([  5.3596, -17.2587])\n",
      "  grad :  tensor([-0.0014,  0.0078])\n",
      "Epoch 3496, Loss 2.927832\n",
      "  params :  tensor([  5.3596, -17.2588])\n",
      "  grad :  tensor([-0.0014,  0.0078])\n",
      "Epoch 3497, Loss 2.927831\n",
      "  params :  tensor([  5.3596, -17.2589])\n",
      "  grad :  tensor([-0.0014,  0.0078])\n",
      "Epoch 3498, Loss 2.927830\n",
      "  params :  tensor([  5.3596, -17.2590])\n",
      "  grad :  tensor([-0.0014,  0.0078])\n",
      "Epoch 3499, Loss 2.927830\n",
      "  params :  tensor([  5.3596, -17.2590])\n",
      "  grad :  tensor([-0.0014,  0.0078])\n",
      "Epoch 3500, Loss 2.927830\n",
      "  params :  tensor([  5.3597, -17.2591])\n",
      "  grad :  tensor([-0.0014,  0.0078])\n",
      "Epoch 3501, Loss 2.927829\n",
      "  params :  tensor([  5.3597, -17.2592])\n",
      "  grad :  tensor([-0.0014,  0.0078])\n",
      "Epoch 3502, Loss 2.927828\n",
      "  params :  tensor([  5.3597, -17.2593])\n",
      "  grad :  tensor([-0.0014,  0.0077])\n",
      "Epoch 3503, Loss 2.927828\n",
      "  params :  tensor([  5.3597, -17.2594])\n",
      "  grad :  tensor([-0.0014,  0.0077])\n",
      "Epoch 3504, Loss 2.927827\n",
      "  params :  tensor([  5.3597, -17.2594])\n",
      "  grad :  tensor([-0.0014,  0.0077])\n",
      "Epoch 3505, Loss 2.927825\n",
      "  params :  tensor([  5.3597, -17.2595])\n",
      "  grad :  tensor([-0.0014,  0.0077])\n",
      "Epoch 3506, Loss 2.927827\n",
      "  params :  tensor([  5.3597, -17.2596])\n",
      "  grad :  tensor([-0.0014,  0.0077])\n",
      "Epoch 3507, Loss 2.927825\n",
      "  params :  tensor([  5.3597, -17.2597])\n",
      "  grad :  tensor([-0.0014,  0.0077])\n",
      "Epoch 3508, Loss 2.927824\n",
      "  params :  tensor([  5.3598, -17.2597])\n",
      "  grad :  tensor([-0.0013,  0.0077])\n",
      "Epoch 3509, Loss 2.927824\n",
      "  params :  tensor([  5.3598, -17.2598])\n",
      "  grad :  tensor([-0.0013,  0.0077])\n",
      "Epoch 3510, Loss 2.927824\n",
      "  params :  tensor([  5.3598, -17.2599])\n",
      "  grad :  tensor([-0.0013,  0.0076])\n",
      "Epoch 3511, Loss 2.927822\n",
      "  params :  tensor([  5.3598, -17.2600])\n",
      "  grad :  tensor([-0.0014,  0.0076])\n",
      "Epoch 3512, Loss 2.927822\n",
      "  params :  tensor([  5.3598, -17.2600])\n",
      "  grad :  tensor([-0.0014,  0.0076])\n",
      "Epoch 3513, Loss 2.927821\n",
      "  params :  tensor([  5.3598, -17.2601])\n",
      "  grad :  tensor([-0.0013,  0.0076])\n",
      "Epoch 3514, Loss 2.927820\n",
      "  params :  tensor([  5.3598, -17.2602])\n",
      "  grad :  tensor([-0.0013,  0.0076])\n",
      "Epoch 3515, Loss 2.927820\n",
      "  params :  tensor([  5.3599, -17.2603])\n",
      "  grad :  tensor([-0.0014,  0.0076])\n",
      "Epoch 3516, Loss 2.927821\n",
      "  params :  tensor([  5.3599, -17.2604])\n",
      "  grad :  tensor([-0.0014,  0.0076])\n",
      "Epoch 3517, Loss 2.927819\n",
      "  params :  tensor([  5.3599, -17.2604])\n",
      "  grad :  tensor([-0.0014,  0.0075])\n",
      "Epoch 3518, Loss 2.927819\n",
      "  params :  tensor([  5.3599, -17.2605])\n",
      "  grad :  tensor([-0.0014,  0.0075])\n",
      "Epoch 3519, Loss 2.927819\n",
      "  params :  tensor([  5.3599, -17.2606])\n",
      "  grad :  tensor([-0.0013,  0.0075])\n",
      "Epoch 3520, Loss 2.927817\n",
      "  params :  tensor([  5.3599, -17.2607])\n",
      "  grad :  tensor([-0.0013,  0.0075])\n",
      "Epoch 3521, Loss 2.927817\n",
      "  params :  tensor([  5.3599, -17.2607])\n",
      "  grad :  tensor([-0.0013,  0.0075])\n",
      "Epoch 3522, Loss 2.927816\n",
      "  params :  tensor([  5.3599, -17.2608])\n",
      "  grad :  tensor([-0.0013,  0.0075])\n",
      "Epoch 3523, Loss 2.927815\n",
      "  params :  tensor([  5.3600, -17.2609])\n",
      "  grad :  tensor([-0.0013,  0.0075])\n",
      "Epoch 3524, Loss 2.927816\n",
      "  params :  tensor([  5.3600, -17.2610])\n",
      "  grad :  tensor([-0.0013,  0.0075])\n",
      "Epoch 3525, Loss 2.927815\n",
      "  params :  tensor([  5.3600, -17.2610])\n",
      "  grad :  tensor([-0.0013,  0.0074])\n",
      "Epoch 3526, Loss 2.927814\n",
      "  params :  tensor([  5.3600, -17.2611])\n",
      "  grad :  tensor([-0.0013,  0.0074])\n",
      "Epoch 3527, Loss 2.927813\n",
      "  params :  tensor([  5.3600, -17.2612])\n",
      "  grad :  tensor([-0.0013,  0.0074])\n",
      "Epoch 3528, Loss 2.927812\n",
      "  params :  tensor([  5.3600, -17.2612])\n",
      "  grad :  tensor([-0.0013,  0.0074])\n",
      "Epoch 3529, Loss 2.927811\n",
      "  params :  tensor([  5.3600, -17.2613])\n",
      "  grad :  tensor([-0.0013,  0.0074])\n",
      "Epoch 3530, Loss 2.927812\n",
      "  params :  tensor([  5.3601, -17.2614])\n",
      "  grad :  tensor([-0.0013,  0.0074])\n",
      "Epoch 3531, Loss 2.927812\n",
      "  params :  tensor([  5.3601, -17.2615])\n",
      "  grad :  tensor([-0.0013,  0.0074])\n",
      "Epoch 3532, Loss 2.927810\n",
      "  params :  tensor([  5.3601, -17.2615])\n",
      "  grad :  tensor([-0.0013,  0.0074])\n",
      "Epoch 3533, Loss 2.927809\n",
      "  params :  tensor([  5.3601, -17.2616])\n",
      "  grad :  tensor([-0.0013,  0.0073])\n",
      "Epoch 3534, Loss 2.927810\n",
      "  params :  tensor([  5.3601, -17.2617])\n",
      "  grad :  tensor([-0.0013,  0.0073])\n",
      "Epoch 3535, Loss 2.927809\n",
      "  params :  tensor([  5.3601, -17.2618])\n",
      "  grad :  tensor([-0.0013,  0.0073])\n",
      "Epoch 3536, Loss 2.927808\n",
      "  params :  tensor([  5.3601, -17.2618])\n",
      "  grad :  tensor([-0.0013,  0.0073])\n",
      "Epoch 3537, Loss 2.927808\n",
      "  params :  tensor([  5.3601, -17.2619])\n",
      "  grad :  tensor([-0.0013,  0.0073])\n",
      "Epoch 3538, Loss 2.927806\n",
      "  params :  tensor([  5.3602, -17.2620])\n",
      "  grad :  tensor([-0.0013,  0.0073])\n",
      "Epoch 3539, Loss 2.927806\n",
      "  params :  tensor([  5.3602, -17.2621])\n",
      "  grad :  tensor([-0.0013,  0.0073])\n",
      "Epoch 3540, Loss 2.927805\n",
      "  params :  tensor([  5.3602, -17.2621])\n",
      "  grad :  tensor([-0.0013,  0.0073])\n",
      "Epoch 3541, Loss 2.927804\n",
      "  params :  tensor([  5.3602, -17.2622])\n",
      "  grad :  tensor([-0.0013,  0.0073])\n",
      "Epoch 3542, Loss 2.927805\n",
      "  params :  tensor([  5.3602, -17.2623])\n",
      "  grad :  tensor([-0.0013,  0.0072])\n",
      "Epoch 3543, Loss 2.927804\n",
      "  params :  tensor([  5.3602, -17.2623])\n",
      "  grad :  tensor([-0.0013,  0.0072])\n",
      "Epoch 3544, Loss 2.927805\n",
      "  params :  tensor([  5.3602, -17.2624])\n",
      "  grad :  tensor([-0.0013,  0.0072])\n",
      "Epoch 3545, Loss 2.927804\n",
      "  params :  tensor([  5.3602, -17.2625])\n",
      "  grad :  tensor([-0.0013,  0.0072])\n",
      "Epoch 3546, Loss 2.927804\n",
      "  params :  tensor([  5.3603, -17.2626])\n",
      "  grad :  tensor([-0.0013,  0.0072])\n",
      "Epoch 3547, Loss 2.927803\n",
      "  params :  tensor([  5.3603, -17.2626])\n",
      "  grad :  tensor([-0.0013,  0.0072])\n",
      "Epoch 3548, Loss 2.927802\n",
      "  params :  tensor([  5.3603, -17.2627])\n",
      "  grad :  tensor([-0.0013,  0.0072])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3549, Loss 2.927801\n",
      "  params :  tensor([  5.3603, -17.2628])\n",
      "  grad :  tensor([-0.0013,  0.0071])\n",
      "Epoch 3550, Loss 2.927801\n",
      "  params :  tensor([  5.3603, -17.2628])\n",
      "  grad :  tensor([-0.0013,  0.0071])\n",
      "Epoch 3551, Loss 2.927799\n",
      "  params :  tensor([  5.3603, -17.2629])\n",
      "  grad :  tensor([-0.0012,  0.0071])\n",
      "Epoch 3552, Loss 2.927801\n",
      "  params :  tensor([  5.3603, -17.2630])\n",
      "  grad :  tensor([-0.0012,  0.0071])\n",
      "Epoch 3553, Loss 2.927798\n",
      "  params :  tensor([  5.3603, -17.2631])\n",
      "  grad :  tensor([-0.0012,  0.0071])\n",
      "Epoch 3554, Loss 2.927798\n",
      "  params :  tensor([  5.3604, -17.2631])\n",
      "  grad :  tensor([-0.0012,  0.0071])\n",
      "Epoch 3555, Loss 2.927798\n",
      "  params :  tensor([  5.3604, -17.2632])\n",
      "  grad :  tensor([-0.0013,  0.0071])\n",
      "Epoch 3556, Loss 2.927798\n",
      "  params :  tensor([  5.3604, -17.2633])\n",
      "  grad :  tensor([-0.0013,  0.0071])\n",
      "Epoch 3557, Loss 2.927798\n",
      "  params :  tensor([  5.3604, -17.2633])\n",
      "  grad :  tensor([-0.0013,  0.0071])\n",
      "Epoch 3558, Loss 2.927796\n",
      "  params :  tensor([  5.3604, -17.2634])\n",
      "  grad :  tensor([-0.0013,  0.0070])\n",
      "Epoch 3559, Loss 2.927795\n",
      "  params :  tensor([  5.3604, -17.2635])\n",
      "  grad :  tensor([-0.0013,  0.0070])\n",
      "Epoch 3560, Loss 2.927796\n",
      "  params :  tensor([  5.3604, -17.2636])\n",
      "  grad :  tensor([-0.0013,  0.0070])\n",
      "Epoch 3561, Loss 2.927794\n",
      "  params :  tensor([  5.3604, -17.2636])\n",
      "  grad :  tensor([-0.0013,  0.0070])\n",
      "Epoch 3562, Loss 2.927795\n",
      "  params :  tensor([  5.3605, -17.2637])\n",
      "  grad :  tensor([-0.0013,  0.0070])\n",
      "Epoch 3563, Loss 2.927795\n",
      "  params :  tensor([  5.3605, -17.2638])\n",
      "  grad :  tensor([-0.0013,  0.0070])\n",
      "Epoch 3564, Loss 2.927793\n",
      "  params :  tensor([  5.3605, -17.2638])\n",
      "  grad :  tensor([-0.0013,  0.0070])\n",
      "Epoch 3565, Loss 2.927795\n",
      "  params :  tensor([  5.3605, -17.2639])\n",
      "  grad :  tensor([-0.0012,  0.0070])\n",
      "Epoch 3566, Loss 2.927791\n",
      "  params :  tensor([  5.3605, -17.2640])\n",
      "  grad :  tensor([-0.0012,  0.0069])\n",
      "Epoch 3567, Loss 2.927791\n",
      "  params :  tensor([  5.3605, -17.2640])\n",
      "  grad :  tensor([-0.0012,  0.0069])\n",
      "Epoch 3568, Loss 2.927791\n",
      "  params :  tensor([  5.3605, -17.2641])\n",
      "  grad :  tensor([-0.0012,  0.0069])\n",
      "Epoch 3569, Loss 2.927790\n",
      "  params :  tensor([  5.3605, -17.2642])\n",
      "  grad :  tensor([-0.0012,  0.0069])\n",
      "Epoch 3570, Loss 2.927790\n",
      "  params :  tensor([  5.3606, -17.2642])\n",
      "  grad :  tensor([-0.0012,  0.0069])\n",
      "Epoch 3571, Loss 2.927789\n",
      "  params :  tensor([  5.3606, -17.2643])\n",
      "  grad :  tensor([-0.0012,  0.0069])\n",
      "Epoch 3572, Loss 2.927790\n",
      "  params :  tensor([  5.3606, -17.2644])\n",
      "  grad :  tensor([-0.0012,  0.0069])\n",
      "Epoch 3573, Loss 2.927789\n",
      "  params :  tensor([  5.3606, -17.2645])\n",
      "  grad :  tensor([-0.0012,  0.0069])\n",
      "Epoch 3574, Loss 2.927789\n",
      "  params :  tensor([  5.3606, -17.2645])\n",
      "  grad :  tensor([-0.0012,  0.0069])\n",
      "Epoch 3575, Loss 2.927789\n",
      "  params :  tensor([  5.3606, -17.2646])\n",
      "  grad :  tensor([-0.0012,  0.0068])\n",
      "Epoch 3576, Loss 2.927787\n",
      "  params :  tensor([  5.3606, -17.2647])\n",
      "  grad :  tensor([-0.0012,  0.0068])\n",
      "Epoch 3577, Loss 2.927786\n",
      "  params :  tensor([  5.3606, -17.2647])\n",
      "  grad :  tensor([-0.0012,  0.0068])\n",
      "Epoch 3578, Loss 2.927788\n",
      "  params :  tensor([  5.3607, -17.2648])\n",
      "  grad :  tensor([-0.0012,  0.0068])\n",
      "Epoch 3579, Loss 2.927785\n",
      "  params :  tensor([  5.3607, -17.2649])\n",
      "  grad :  tensor([-0.0012,  0.0068])\n",
      "Epoch 3580, Loss 2.927785\n",
      "  params :  tensor([  5.3607, -17.2649])\n",
      "  grad :  tensor([-0.0012,  0.0068])\n",
      "Epoch 3581, Loss 2.927786\n",
      "  params :  tensor([  5.3607, -17.2650])\n",
      "  grad :  tensor([-0.0012,  0.0068])\n",
      "Epoch 3582, Loss 2.927785\n",
      "  params :  tensor([  5.3607, -17.2651])\n",
      "  grad :  tensor([-0.0012,  0.0068])\n",
      "Epoch 3583, Loss 2.927784\n",
      "  params :  tensor([  5.3607, -17.2651])\n",
      "  grad :  tensor([-0.0012,  0.0067])\n",
      "Epoch 3584, Loss 2.927784\n",
      "  params :  tensor([  5.3607, -17.2652])\n",
      "  grad :  tensor([-0.0012,  0.0067])\n",
      "Epoch 3585, Loss 2.927783\n",
      "  params :  tensor([  5.3607, -17.2653])\n",
      "  grad :  tensor([-0.0012,  0.0067])\n",
      "Epoch 3586, Loss 2.927783\n",
      "  params :  tensor([  5.3607, -17.2653])\n",
      "  grad :  tensor([-0.0012,  0.0067])\n",
      "Epoch 3587, Loss 2.927781\n",
      "  params :  tensor([  5.3608, -17.2654])\n",
      "  grad :  tensor([-0.0012,  0.0067])\n",
      "Epoch 3588, Loss 2.927782\n",
      "  params :  tensor([  5.3608, -17.2655])\n",
      "  grad :  tensor([-0.0012,  0.0067])\n",
      "Epoch 3589, Loss 2.927781\n",
      "  params :  tensor([  5.3608, -17.2655])\n",
      "  grad :  tensor([-0.0012,  0.0067])\n",
      "Epoch 3590, Loss 2.927781\n",
      "  params :  tensor([  5.3608, -17.2656])\n",
      "  grad :  tensor([-0.0012,  0.0067])\n",
      "Epoch 3591, Loss 2.927781\n",
      "  params :  tensor([  5.3608, -17.2657])\n",
      "  grad :  tensor([-0.0012,  0.0067])\n",
      "Epoch 3592, Loss 2.927780\n",
      "  params :  tensor([  5.3608, -17.2657])\n",
      "  grad :  tensor([-0.0012,  0.0066])\n",
      "Epoch 3593, Loss 2.927780\n",
      "  params :  tensor([  5.3608, -17.2658])\n",
      "  grad :  tensor([-0.0012,  0.0066])\n",
      "Epoch 3594, Loss 2.927778\n",
      "  params :  tensor([  5.3608, -17.2659])\n",
      "  grad :  tensor([-0.0012,  0.0066])\n",
      "Epoch 3595, Loss 2.927779\n",
      "  params :  tensor([  5.3609, -17.2659])\n",
      "  grad :  tensor([-0.0012,  0.0066])\n",
      "Epoch 3596, Loss 2.927778\n",
      "  params :  tensor([  5.3609, -17.2660])\n",
      "  grad :  tensor([-0.0012,  0.0066])\n",
      "Epoch 3597, Loss 2.927778\n",
      "  params :  tensor([  5.3609, -17.2661])\n",
      "  grad :  tensor([-0.0012,  0.0066])\n",
      "Epoch 3598, Loss 2.927779\n",
      "  params :  tensor([  5.3609, -17.2661])\n",
      "  grad :  tensor([-0.0012,  0.0066])\n",
      "Epoch 3599, Loss 2.927777\n",
      "  params :  tensor([  5.3609, -17.2662])\n",
      "  grad :  tensor([-0.0012,  0.0066])\n",
      "Epoch 3600, Loss 2.927776\n",
      "  params :  tensor([  5.3609, -17.2663])\n",
      "  grad :  tensor([-0.0012,  0.0066])\n",
      "Epoch 3601, Loss 2.927775\n",
      "  params :  tensor([  5.3609, -17.2663])\n",
      "  grad :  tensor([-0.0012,  0.0065])\n",
      "Epoch 3602, Loss 2.927776\n",
      "  params :  tensor([  5.3609, -17.2664])\n",
      "  grad :  tensor([-0.0012,  0.0065])\n",
      "Epoch 3603, Loss 2.927773\n",
      "  params :  tensor([  5.3609, -17.2665])\n",
      "  grad :  tensor([-0.0012,  0.0065])\n",
      "Epoch 3604, Loss 2.927775\n",
      "  params :  tensor([  5.3610, -17.2665])\n",
      "  grad :  tensor([-0.0012,  0.0065])\n",
      "Epoch 3605, Loss 2.927775\n",
      "  params :  tensor([  5.3610, -17.2666])\n",
      "  grad :  tensor([-0.0011,  0.0065])\n",
      "Epoch 3606, Loss 2.927775\n",
      "  params :  tensor([  5.3610, -17.2667])\n",
      "  grad :  tensor([-0.0011,  0.0065])\n",
      "Epoch 3607, Loss 2.927773\n",
      "  params :  tensor([  5.3610, -17.2667])\n",
      "  grad :  tensor([-0.0011,  0.0065])\n",
      "Epoch 3608, Loss 2.927773\n",
      "  params :  tensor([  5.3610, -17.2668])\n",
      "  grad :  tensor([-0.0011,  0.0065])\n",
      "Epoch 3609, Loss 2.927773\n",
      "  params :  tensor([  5.3610, -17.2668])\n",
      "  grad :  tensor([-0.0011,  0.0065])\n",
      "Epoch 3610, Loss 2.927772\n",
      "  params :  tensor([  5.3610, -17.2669])\n",
      "  grad :  tensor([-0.0011,  0.0064])\n",
      "Epoch 3611, Loss 2.927772\n",
      "  params :  tensor([  5.3610, -17.2670])\n",
      "  grad :  tensor([-0.0011,  0.0064])\n",
      "Epoch 3612, Loss 2.927770\n",
      "  params :  tensor([  5.3611, -17.2670])\n",
      "  grad :  tensor([-0.0011,  0.0064])\n",
      "Epoch 3613, Loss 2.927772\n",
      "  params :  tensor([  5.3611, -17.2671])\n",
      "  grad :  tensor([-0.0011,  0.0064])\n",
      "Epoch 3614, Loss 2.927771\n",
      "  params :  tensor([  5.3611, -17.2672])\n",
      "  grad :  tensor([-0.0011,  0.0064])\n",
      "Epoch 3615, Loss 2.927770\n",
      "  params :  tensor([  5.3611, -17.2672])\n",
      "  grad :  tensor([-0.0011,  0.0064])\n",
      "Epoch 3616, Loss 2.927770\n",
      "  params :  tensor([  5.3611, -17.2673])\n",
      "  grad :  tensor([-0.0011,  0.0064])\n",
      "Epoch 3617, Loss 2.927769\n",
      "  params :  tensor([  5.3611, -17.2674])\n",
      "  grad :  tensor([-0.0011,  0.0064])\n",
      "Epoch 3618, Loss 2.927768\n",
      "  params :  tensor([  5.3611, -17.2674])\n",
      "  grad :  tensor([-0.0011,  0.0064])\n",
      "Epoch 3619, Loss 2.927769\n",
      "  params :  tensor([  5.3611, -17.2675])\n",
      "  grad :  tensor([-0.0011,  0.0064])\n",
      "Epoch 3620, Loss 2.927768\n",
      "  params :  tensor([  5.3611, -17.2675])\n",
      "  grad :  tensor([-0.0011,  0.0063])\n",
      "Epoch 3621, Loss 2.927767\n",
      "  params :  tensor([  5.3612, -17.2676])\n",
      "  grad :  tensor([-0.0011,  0.0063])\n",
      "Epoch 3622, Loss 2.927767\n",
      "  params :  tensor([  5.3612, -17.2677])\n",
      "  grad :  tensor([-0.0011,  0.0063])\n",
      "Epoch 3623, Loss 2.927767\n",
      "  params :  tensor([  5.3612, -17.2677])\n",
      "  grad :  tensor([-0.0011,  0.0063])\n",
      "Epoch 3624, Loss 2.927765\n",
      "  params :  tensor([  5.3612, -17.2678])\n",
      "  grad :  tensor([-0.0011,  0.0063])\n",
      "Epoch 3625, Loss 2.927766\n",
      "  params :  tensor([  5.3612, -17.2679])\n",
      "  grad :  tensor([-0.0011,  0.0063])\n",
      "Epoch 3626, Loss 2.927765\n",
      "  params :  tensor([  5.3612, -17.2679])\n",
      "  grad :  tensor([-0.0011,  0.0063])\n",
      "Epoch 3627, Loss 2.927765\n",
      "  params :  tensor([  5.3612, -17.2680])\n",
      "  grad :  tensor([-0.0011,  0.0063])\n",
      "Epoch 3628, Loss 2.927764\n",
      "  params :  tensor([  5.3612, -17.2681])\n",
      "  grad :  tensor([-0.0011,  0.0063])\n",
      "Epoch 3629, Loss 2.927764\n",
      "  params :  tensor([  5.3612, -17.2681])\n",
      "  grad :  tensor([-0.0011,  0.0062])\n",
      "Epoch 3630, Loss 2.927764\n",
      "  params :  tensor([  5.3613, -17.2682])\n",
      "  grad :  tensor([-0.0011,  0.0062])\n",
      "Epoch 3631, Loss 2.927762\n",
      "  params :  tensor([  5.3613, -17.2682])\n",
      "  grad :  tensor([-0.0011,  0.0062])\n",
      "Epoch 3632, Loss 2.927763\n",
      "  params :  tensor([  5.3613, -17.2683])\n",
      "  grad :  tensor([-0.0011,  0.0062])\n",
      "Epoch 3633, Loss 2.927763\n",
      "  params :  tensor([  5.3613, -17.2684])\n",
      "  grad :  tensor([-0.0011,  0.0062])\n",
      "Epoch 3634, Loss 2.927762\n",
      "  params :  tensor([  5.3613, -17.2684])\n",
      "  grad :  tensor([-0.0011,  0.0062])\n",
      "Epoch 3635, Loss 2.927761\n",
      "  params :  tensor([  5.3613, -17.2685])\n",
      "  grad :  tensor([-0.0011,  0.0062])\n",
      "Epoch 3636, Loss 2.927762\n",
      "  params :  tensor([  5.3613, -17.2685])\n",
      "  grad :  tensor([-0.0011,  0.0062])\n",
      "Epoch 3637, Loss 2.927759\n",
      "  params :  tensor([  5.3613, -17.2686])\n",
      "  grad :  tensor([-0.0011,  0.0062])\n",
      "Epoch 3638, Loss 2.927761\n",
      "  params :  tensor([  5.3613, -17.2687])\n",
      "  grad :  tensor([-0.0011,  0.0061])\n",
      "Epoch 3639, Loss 2.927761\n",
      "  params :  tensor([  5.3614, -17.2687])\n",
      "  grad :  tensor([-0.0011,  0.0061])\n",
      "Epoch 3640, Loss 2.927760\n",
      "  params :  tensor([  5.3614, -17.2688])\n",
      "  grad :  tensor([-0.0011,  0.0061])\n",
      "Epoch 3641, Loss 2.927759\n",
      "  params :  tensor([  5.3614, -17.2689])\n",
      "  grad :  tensor([-0.0011,  0.0061])\n",
      "Epoch 3642, Loss 2.927758\n",
      "  params :  tensor([  5.3614, -17.2689])\n",
      "  grad :  tensor([-0.0011,  0.0061])\n",
      "Epoch 3643, Loss 2.927759\n",
      "  params :  tensor([  5.3614, -17.2690])\n",
      "  grad :  tensor([-0.0011,  0.0061])\n",
      "Epoch 3644, Loss 2.927757\n",
      "  params :  tensor([  5.3614, -17.2690])\n",
      "  grad :  tensor([-0.0011,  0.0061])\n",
      "Epoch 3645, Loss 2.927758\n",
      "  params :  tensor([  5.3614, -17.2691])\n",
      "  grad :  tensor([-0.0011,  0.0061])\n",
      "Epoch 3646, Loss 2.927757\n",
      "  params :  tensor([  5.3614, -17.2692])\n",
      "  grad :  tensor([-0.0011,  0.0061])\n",
      "Epoch 3647, Loss 2.927757\n",
      "  params :  tensor([  5.3614, -17.2692])\n",
      "  grad :  tensor([-0.0011,  0.0061])\n",
      "Epoch 3648, Loss 2.927757\n",
      "  params :  tensor([  5.3614, -17.2693])\n",
      "  grad :  tensor([-0.0011,  0.0060])\n",
      "Epoch 3649, Loss 2.927756\n",
      "  params :  tensor([  5.3615, -17.2693])\n",
      "  grad :  tensor([-0.0011,  0.0060])\n",
      "Epoch 3650, Loss 2.927757\n",
      "  params :  tensor([  5.3615, -17.2694])\n",
      "  grad :  tensor([-0.0011,  0.0060])\n",
      "Epoch 3651, Loss 2.927756\n",
      "  params :  tensor([  5.3615, -17.2695])\n",
      "  grad :  tensor([-0.0011,  0.0060])\n",
      "Epoch 3652, Loss 2.927756\n",
      "  params :  tensor([  5.3615, -17.2695])\n",
      "  grad :  tensor([-0.0010,  0.0060])\n",
      "Epoch 3653, Loss 2.927755\n",
      "  params :  tensor([  5.3615, -17.2696])\n",
      "  grad :  tensor([-0.0010,  0.0060])\n",
      "Epoch 3654, Loss 2.927755\n",
      "  params :  tensor([  5.3615, -17.2696])\n",
      "  grad :  tensor([-0.0010,  0.0060])\n",
      "Epoch 3655, Loss 2.927754\n",
      "  params :  tensor([  5.3615, -17.2697])\n",
      "  grad :  tensor([-0.0010,  0.0060])\n",
      "Epoch 3656, Loss 2.927754\n",
      "  params :  tensor([  5.3615, -17.2698])\n",
      "  grad :  tensor([-0.0010,  0.0060])\n",
      "Epoch 3657, Loss 2.927755\n",
      "  params :  tensor([  5.3615, -17.2698])\n",
      "  grad :  tensor([-0.0010,  0.0060])\n",
      "Epoch 3658, Loss 2.927753\n",
      "  params :  tensor([  5.3616, -17.2699])\n",
      "  grad :  tensor([-0.0010,  0.0059])\n",
      "Epoch 3659, Loss 2.927752\n",
      "  params :  tensor([  5.3616, -17.2699])\n",
      "  grad :  tensor([-0.0010,  0.0059])\n",
      "Epoch 3660, Loss 2.927754\n",
      "  params :  tensor([  5.3616, -17.2700])\n",
      "  grad :  tensor([-0.0010,  0.0059])\n",
      "Epoch 3661, Loss 2.927752\n",
      "  params :  tensor([  5.3616, -17.2701])\n",
      "  grad :  tensor([-0.0010,  0.0059])\n",
      "Epoch 3662, Loss 2.927751\n",
      "  params :  tensor([  5.3616, -17.2701])\n",
      "  grad :  tensor([-0.0010,  0.0059])\n",
      "Epoch 3663, Loss 2.927752\n",
      "  params :  tensor([  5.3616, -17.2702])\n",
      "  grad :  tensor([-0.0010,  0.0059])\n",
      "Epoch 3664, Loss 2.927750\n",
      "  params :  tensor([  5.3616, -17.2702])\n",
      "  grad :  tensor([-0.0011,  0.0059])\n",
      "Epoch 3665, Loss 2.927749\n",
      "  params :  tensor([  5.3616, -17.2703])\n",
      "  grad :  tensor([-0.0010,  0.0059])\n",
      "Epoch 3666, Loss 2.927751\n",
      "  params :  tensor([  5.3616, -17.2703])\n",
      "  grad :  tensor([-0.0010,  0.0059])\n",
      "Epoch 3667, Loss 2.927750\n",
      "  params :  tensor([  5.3616, -17.2704])\n",
      "  grad :  tensor([-0.0010,  0.0058])\n",
      "Epoch 3668, Loss 2.927750\n",
      "  params :  tensor([  5.3617, -17.2705])\n",
      "  grad :  tensor([-0.0010,  0.0058])\n",
      "Epoch 3669, Loss 2.927747\n",
      "  params :  tensor([  5.3617, -17.2705])\n",
      "  grad :  tensor([-0.0010,  0.0058])\n",
      "Epoch 3670, Loss 2.927749\n",
      "  params :  tensor([  5.3617, -17.2706])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad :  tensor([-0.0010,  0.0058])\n",
      "Epoch 3671, Loss 2.927747\n",
      "  params :  tensor([  5.3617, -17.2706])\n",
      "  grad :  tensor([-0.0010,  0.0058])\n",
      "Epoch 3672, Loss 2.927748\n",
      "  params :  tensor([  5.3617, -17.2707])\n",
      "  grad :  tensor([-0.0010,  0.0058])\n",
      "Epoch 3673, Loss 2.927748\n",
      "  params :  tensor([  5.3617, -17.2708])\n",
      "  grad :  tensor([-0.0010,  0.0058])\n",
      "Epoch 3674, Loss 2.927747\n",
      "  params :  tensor([  5.3617, -17.2708])\n",
      "  grad :  tensor([-0.0010,  0.0058])\n",
      "Epoch 3675, Loss 2.927747\n",
      "  params :  tensor([  5.3617, -17.2709])\n",
      "  grad :  tensor([-0.0010,  0.0058])\n",
      "Epoch 3676, Loss 2.927748\n",
      "  params :  tensor([  5.3617, -17.2709])\n",
      "  grad :  tensor([-0.0010,  0.0058])\n",
      "Epoch 3677, Loss 2.927747\n",
      "  params :  tensor([  5.3617, -17.2710])\n",
      "  grad :  tensor([-0.0010,  0.0058])\n",
      "Epoch 3678, Loss 2.927747\n",
      "  params :  tensor([  5.3618, -17.2710])\n",
      "  grad :  tensor([-0.0010,  0.0057])\n",
      "Epoch 3679, Loss 2.927745\n",
      "  params :  tensor([  5.3618, -17.2711])\n",
      "  grad :  tensor([-0.0010,  0.0057])\n",
      "Epoch 3680, Loss 2.927745\n",
      "  params :  tensor([  5.3618, -17.2712])\n",
      "  grad :  tensor([-0.0010,  0.0057])\n",
      "Epoch 3681, Loss 2.927746\n",
      "  params :  tensor([  5.3618, -17.2712])\n",
      "  grad :  tensor([-0.0010,  0.0057])\n",
      "Epoch 3682, Loss 2.927744\n",
      "  params :  tensor([  5.3618, -17.2713])\n",
      "  grad :  tensor([-0.0010,  0.0057])\n",
      "Epoch 3683, Loss 2.927743\n",
      "  params :  tensor([  5.3618, -17.2713])\n",
      "  grad :  tensor([-0.0010,  0.0057])\n",
      "Epoch 3684, Loss 2.927743\n",
      "  params :  tensor([  5.3618, -17.2714])\n",
      "  grad :  tensor([-0.0010,  0.0057])\n",
      "Epoch 3685, Loss 2.927743\n",
      "  params :  tensor([  5.3618, -17.2714])\n",
      "  grad :  tensor([-0.0010,  0.0057])\n",
      "Epoch 3686, Loss 2.927743\n",
      "  params :  tensor([  5.3618, -17.2715])\n",
      "  grad :  tensor([-0.0010,  0.0057])\n",
      "Epoch 3687, Loss 2.927743\n",
      "  params :  tensor([  5.3618, -17.2716])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3688, Loss 2.927744\n",
      "  params :  tensor([  5.3619, -17.2716])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3689, Loss 2.927742\n",
      "  params :  tensor([  5.3619, -17.2717])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3690, Loss 2.927742\n",
      "  params :  tensor([  5.3619, -17.2717])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3691, Loss 2.927742\n",
      "  params :  tensor([  5.3619, -17.2718])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3692, Loss 2.927742\n",
      "  params :  tensor([  5.3619, -17.2718])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3693, Loss 2.927741\n",
      "  params :  tensor([  5.3619, -17.2719])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3694, Loss 2.927741\n",
      "  params :  tensor([  5.3619, -17.2719])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3695, Loss 2.927741\n",
      "  params :  tensor([  5.3619, -17.2720])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3696, Loss 2.927742\n",
      "  params :  tensor([  5.3619, -17.2721])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3697, Loss 2.927741\n",
      "  params :  tensor([  5.3619, -17.2721])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3698, Loss 2.927741\n",
      "  params :  tensor([  5.3620, -17.2722])\n",
      "  grad :  tensor([-0.0010,  0.0056])\n",
      "Epoch 3699, Loss 2.927740\n",
      "  params :  tensor([  5.3620, -17.2722])\n",
      "  grad :  tensor([-0.0010,  0.0055])\n",
      "Epoch 3700, Loss 2.927739\n",
      "  params :  tensor([  5.3620, -17.2723])\n",
      "  grad :  tensor([-0.0010,  0.0055])\n",
      "Epoch 3701, Loss 2.927738\n",
      "  params :  tensor([  5.3620, -17.2723])\n",
      "  grad :  tensor([-0.0010,  0.0055])\n",
      "Epoch 3702, Loss 2.927738\n",
      "  params :  tensor([  5.3620, -17.2724])\n",
      "  grad :  tensor([-0.0010,  0.0055])\n",
      "Epoch 3703, Loss 2.927737\n",
      "  params :  tensor([  5.3620, -17.2724])\n",
      "  grad :  tensor([-0.0010,  0.0055])\n",
      "Epoch 3704, Loss 2.927737\n",
      "  params :  tensor([  5.3620, -17.2725])\n",
      "  grad :  tensor([-0.0010,  0.0055])\n",
      "Epoch 3705, Loss 2.927738\n",
      "  params :  tensor([  5.3620, -17.2726])\n",
      "  grad :  tensor([-0.0010,  0.0055])\n",
      "Epoch 3706, Loss 2.927737\n",
      "  params :  tensor([  5.3620, -17.2726])\n",
      "  grad :  tensor([-0.0010,  0.0055])\n",
      "Epoch 3707, Loss 2.927736\n",
      "  params :  tensor([  5.3620, -17.2727])\n",
      "  grad :  tensor([-0.0010,  0.0055])\n",
      "Epoch 3708, Loss 2.927737\n",
      "  params :  tensor([  5.3621, -17.2727])\n",
      "  grad :  tensor([-0.0010,  0.0055])\n",
      "Epoch 3709, Loss 2.927737\n",
      "  params :  tensor([  5.3621, -17.2728])\n",
      "  grad :  tensor([-0.0010,  0.0054])\n",
      "Epoch 3710, Loss 2.927736\n",
      "  params :  tensor([  5.3621, -17.2728])\n",
      "  grad :  tensor([-0.0010,  0.0054])\n",
      "Epoch 3711, Loss 2.927734\n",
      "  params :  tensor([  5.3621, -17.2729])\n",
      "  grad :  tensor([-0.0010,  0.0054])\n",
      "Epoch 3712, Loss 2.927735\n",
      "  params :  tensor([  5.3621, -17.2729])\n",
      "  grad :  tensor([-0.0010,  0.0054])\n",
      "Epoch 3713, Loss 2.927735\n",
      "  params :  tensor([  5.3621, -17.2730])\n",
      "  grad :  tensor([-0.0009,  0.0054])\n",
      "Epoch 3714, Loss 2.927734\n",
      "  params :  tensor([  5.3621, -17.2730])\n",
      "  grad :  tensor([-0.0009,  0.0054])\n",
      "Epoch 3715, Loss 2.927734\n",
      "  params :  tensor([  5.3621, -17.2731])\n",
      "  grad :  tensor([-0.0009,  0.0054])\n",
      "Epoch 3716, Loss 2.927733\n",
      "  params :  tensor([  5.3621, -17.2732])\n",
      "  grad :  tensor([-0.0009,  0.0054])\n",
      "Epoch 3717, Loss 2.927734\n",
      "  params :  tensor([  5.3621, -17.2732])\n",
      "  grad :  tensor([-0.0009,  0.0054])\n",
      "Epoch 3718, Loss 2.927733\n",
      "  params :  tensor([  5.3622, -17.2733])\n",
      "  grad :  tensor([-0.0009,  0.0054])\n",
      "Epoch 3719, Loss 2.927733\n",
      "  params :  tensor([  5.3622, -17.2733])\n",
      "  grad :  tensor([-0.0009,  0.0054])\n",
      "Epoch 3720, Loss 2.927733\n",
      "  params :  tensor([  5.3622, -17.2734])\n",
      "  grad :  tensor([-0.0010,  0.0053])\n",
      "Epoch 3721, Loss 2.927732\n",
      "  params :  tensor([  5.3622, -17.2734])\n",
      "  grad :  tensor([-0.0009,  0.0053])\n",
      "Epoch 3722, Loss 2.927731\n",
      "  params :  tensor([  5.3622, -17.2735])\n",
      "  grad :  tensor([-0.0009,  0.0053])\n",
      "Epoch 3723, Loss 2.927731\n",
      "  params :  tensor([  5.3622, -17.2735])\n",
      "  grad :  tensor([-0.0009,  0.0053])\n",
      "Epoch 3724, Loss 2.927733\n",
      "  params :  tensor([  5.3622, -17.2736])\n",
      "  grad :  tensor([-0.0009,  0.0053])\n",
      "Epoch 3725, Loss 2.927730\n",
      "  params :  tensor([  5.3622, -17.2736])\n",
      "  grad :  tensor([-0.0009,  0.0053])\n",
      "Epoch 3726, Loss 2.927730\n",
      "  params :  tensor([  5.3622, -17.2737])\n",
      "  grad :  tensor([-0.0009,  0.0053])\n",
      "Epoch 3727, Loss 2.927732\n",
      "  params :  tensor([  5.3622, -17.2737])\n",
      "  grad :  tensor([-0.0009,  0.0053])\n",
      "Epoch 3728, Loss 2.927730\n",
      "  params :  tensor([  5.3622, -17.2738])\n",
      "  grad :  tensor([-0.0010,  0.0053])\n",
      "Epoch 3729, Loss 2.927732\n",
      "  params :  tensor([  5.3623, -17.2738])\n",
      "  grad :  tensor([-0.0009,  0.0053])\n",
      "Epoch 3730, Loss 2.927731\n",
      "  params :  tensor([  5.3623, -17.2739])\n",
      "  grad :  tensor([-0.0009,  0.0053])\n",
      "Epoch 3731, Loss 2.927730\n",
      "  params :  tensor([  5.3623, -17.2740])\n",
      "  grad :  tensor([-0.0009,  0.0052])\n",
      "Epoch 3732, Loss 2.927728\n",
      "  params :  tensor([  5.3623, -17.2740])\n",
      "  grad :  tensor([-0.0009,  0.0052])\n",
      "Epoch 3733, Loss 2.927729\n",
      "  params :  tensor([  5.3623, -17.2741])\n",
      "  grad :  tensor([-0.0009,  0.0052])\n",
      "Epoch 3734, Loss 2.927729\n",
      "  params :  tensor([  5.3623, -17.2741])\n",
      "  grad :  tensor([-0.0009,  0.0052])\n",
      "Epoch 3735, Loss 2.927728\n",
      "  params :  tensor([  5.3623, -17.2742])\n",
      "  grad :  tensor([-0.0009,  0.0052])\n",
      "Epoch 3736, Loss 2.927728\n",
      "  params :  tensor([  5.3623, -17.2742])\n",
      "  grad :  tensor([-0.0009,  0.0052])\n",
      "Epoch 3737, Loss 2.927728\n",
      "  params :  tensor([  5.3623, -17.2743])\n",
      "  grad :  tensor([-0.0009,  0.0052])\n",
      "Epoch 3738, Loss 2.927728\n",
      "  params :  tensor([  5.3623, -17.2743])\n",
      "  grad :  tensor([-0.0009,  0.0052])\n",
      "Epoch 3739, Loss 2.927727\n",
      "  params :  tensor([  5.3623, -17.2744])\n",
      "  grad :  tensor([-0.0009,  0.0052])\n",
      "Epoch 3740, Loss 2.927728\n",
      "  params :  tensor([  5.3624, -17.2744])\n",
      "  grad :  tensor([-0.0009,  0.0052])\n",
      "Epoch 3741, Loss 2.927728\n",
      "  params :  tensor([  5.3624, -17.2745])\n",
      "  grad :  tensor([-0.0009,  0.0052])\n",
      "Epoch 3742, Loss 2.927727\n",
      "  params :  tensor([  5.3624, -17.2745])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3743, Loss 2.927727\n",
      "  params :  tensor([  5.3624, -17.2746])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3744, Loss 2.927726\n",
      "  params :  tensor([  5.3624, -17.2746])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3745, Loss 2.927726\n",
      "  params :  tensor([  5.3624, -17.2747])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3746, Loss 2.927725\n",
      "  params :  tensor([  5.3624, -17.2747])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3747, Loss 2.927725\n",
      "  params :  tensor([  5.3624, -17.2748])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3748, Loss 2.927725\n",
      "  params :  tensor([  5.3624, -17.2748])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3749, Loss 2.927723\n",
      "  params :  tensor([  5.3624, -17.2749])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3750, Loss 2.927724\n",
      "  params :  tensor([  5.3624, -17.2749])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3751, Loss 2.927724\n",
      "  params :  tensor([  5.3625, -17.2750])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3752, Loss 2.927725\n",
      "  params :  tensor([  5.3625, -17.2750])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3753, Loss 2.927724\n",
      "  params :  tensor([  5.3625, -17.2751])\n",
      "  grad :  tensor([-0.0009,  0.0051])\n",
      "Epoch 3754, Loss 2.927724\n",
      "  params :  tensor([  5.3625, -17.2751])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3755, Loss 2.927723\n",
      "  params :  tensor([  5.3625, -17.2752])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3756, Loss 2.927723\n",
      "  params :  tensor([  5.3625, -17.2752])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3757, Loss 2.927723\n",
      "  params :  tensor([  5.3625, -17.2753])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3758, Loss 2.927722\n",
      "  params :  tensor([  5.3625, -17.2753])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3759, Loss 2.927723\n",
      "  params :  tensor([  5.3625, -17.2754])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3760, Loss 2.927722\n",
      "  params :  tensor([  5.3625, -17.2754])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3761, Loss 2.927723\n",
      "  params :  tensor([  5.3625, -17.2755])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3762, Loss 2.927721\n",
      "  params :  tensor([  5.3626, -17.2755])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3763, Loss 2.927722\n",
      "  params :  tensor([  5.3626, -17.2756])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3764, Loss 2.927720\n",
      "  params :  tensor([  5.3626, -17.2756])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3765, Loss 2.927720\n",
      "  params :  tensor([  5.3626, -17.2757])\n",
      "  grad :  tensor([-0.0009,  0.0050])\n",
      "Epoch 3766, Loss 2.927719\n",
      "  params :  tensor([  5.3626, -17.2757])\n",
      "  grad :  tensor([-0.0009,  0.0049])\n",
      "Epoch 3767, Loss 2.927721\n",
      "  params :  tensor([  5.3626, -17.2758])\n",
      "  grad :  tensor([-0.0009,  0.0049])\n",
      "Epoch 3768, Loss 2.927719\n",
      "  params :  tensor([  5.3626, -17.2758])\n",
      "  grad :  tensor([-0.0009,  0.0049])\n",
      "Epoch 3769, Loss 2.927719\n",
      "  params :  tensor([  5.3626, -17.2759])\n",
      "  grad :  tensor([-0.0009,  0.0049])\n",
      "Epoch 3770, Loss 2.927719\n",
      "  params :  tensor([  5.3626, -17.2759])\n",
      "  grad :  tensor([-0.0009,  0.0049])\n",
      "Epoch 3771, Loss 2.927719\n",
      "  params :  tensor([  5.3626, -17.2760])\n",
      "  grad :  tensor([-0.0009,  0.0049])\n",
      "Epoch 3772, Loss 2.927719\n",
      "  params :  tensor([  5.3626, -17.2760])\n",
      "  grad :  tensor([-0.0009,  0.0049])\n",
      "Epoch 3773, Loss 2.927720\n",
      "  params :  tensor([  5.3626, -17.2761])\n",
      "  grad :  tensor([-0.0009,  0.0049])\n",
      "Epoch 3774, Loss 2.927718\n",
      "  params :  tensor([  5.3627, -17.2761])\n",
      "  grad :  tensor([-0.0009,  0.0049])\n",
      "Epoch 3775, Loss 2.927718\n",
      "  params :  tensor([  5.3627, -17.2762])\n",
      "  grad :  tensor([-0.0009,  0.0049])\n",
      "Epoch 3776, Loss 2.927717\n",
      "  params :  tensor([  5.3627, -17.2762])\n",
      "  grad :  tensor([-0.0009,  0.0049])\n",
      "Epoch 3777, Loss 2.927718\n",
      "  params :  tensor([  5.3627, -17.2763])\n",
      "  grad :  tensor([-0.0008,  0.0049])\n",
      "Epoch 3778, Loss 2.927717\n",
      "  params :  tensor([  5.3627, -17.2763])\n",
      "  grad :  tensor([-0.0008,  0.0048])\n",
      "Epoch 3779, Loss 2.927717\n",
      "  params :  tensor([  5.3627, -17.2764])\n",
      "  grad :  tensor([-0.0008,  0.0048])\n",
      "Epoch 3780, Loss 2.927716\n",
      "  params :  tensor([  5.3627, -17.2764])\n",
      "  grad :  tensor([-0.0008,  0.0048])\n",
      "Epoch 3781, Loss 2.927716\n",
      "  params :  tensor([  5.3627, -17.2765])\n",
      "  grad :  tensor([-0.0008,  0.0048])\n",
      "Epoch 3782, Loss 2.927717\n",
      "  params :  tensor([  5.3627, -17.2765])\n",
      "  grad :  tensor([-0.0008,  0.0048])\n",
      "Epoch 3783, Loss 2.927717\n",
      "  params :  tensor([  5.3627, -17.2766])\n",
      "  grad :  tensor([-0.0008,  0.0048])\n",
      "Epoch 3784, Loss 2.927716\n",
      "  params :  tensor([  5.3627, -17.2766])\n",
      "  grad :  tensor([-0.0009,  0.0048])\n",
      "Epoch 3785, Loss 2.927715\n",
      "  params :  tensor([  5.3627, -17.2767])\n",
      "  grad :  tensor([-0.0008,  0.0048])\n",
      "Epoch 3786, Loss 2.927715\n",
      "  params :  tensor([  5.3628, -17.2767])\n",
      "  grad :  tensor([-0.0008,  0.0048])\n",
      "Epoch 3787, Loss 2.927715\n",
      "  params :  tensor([  5.3628, -17.2767])\n",
      "  grad :  tensor([-0.0008,  0.0048])\n",
      "Epoch 3788, Loss 2.927715\n",
      "  params :  tensor([  5.3628, -17.2768])\n",
      "  grad :  tensor([-0.0008,  0.0048])\n",
      "Epoch 3789, Loss 2.927715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3628, -17.2768])\n",
      "  grad :  tensor([-0.0008,  0.0048])\n",
      "Epoch 3790, Loss 2.927715\n",
      "  params :  tensor([  5.3628, -17.2769])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3791, Loss 2.927714\n",
      "  params :  tensor([  5.3628, -17.2769])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3792, Loss 2.927714\n",
      "  params :  tensor([  5.3628, -17.2770])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3793, Loss 2.927714\n",
      "  params :  tensor([  5.3628, -17.2770])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3794, Loss 2.927714\n",
      "  params :  tensor([  5.3628, -17.2771])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3795, Loss 2.927713\n",
      "  params :  tensor([  5.3628, -17.2771])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3796, Loss 2.927714\n",
      "  params :  tensor([  5.3628, -17.2772])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3797, Loss 2.927713\n",
      "  params :  tensor([  5.3629, -17.2772])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3798, Loss 2.927712\n",
      "  params :  tensor([  5.3629, -17.2773])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3799, Loss 2.927712\n",
      "  params :  tensor([  5.3629, -17.2773])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3800, Loss 2.927713\n",
      "  params :  tensor([  5.3629, -17.2774])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3801, Loss 2.927711\n",
      "  params :  tensor([  5.3629, -17.2774])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3802, Loss 2.927712\n",
      "  params :  tensor([  5.3629, -17.2775])\n",
      "  grad :  tensor([-0.0008,  0.0047])\n",
      "Epoch 3803, Loss 2.927712\n",
      "  params :  tensor([  5.3629, -17.2775])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3804, Loss 2.927711\n",
      "  params :  tensor([  5.3629, -17.2775])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3805, Loss 2.927712\n",
      "  params :  tensor([  5.3629, -17.2776])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3806, Loss 2.927711\n",
      "  params :  tensor([  5.3629, -17.2776])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3807, Loss 2.927711\n",
      "  params :  tensor([  5.3629, -17.2777])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3808, Loss 2.927711\n",
      "  params :  tensor([  5.3629, -17.2777])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3809, Loss 2.927709\n",
      "  params :  tensor([  5.3629, -17.2778])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3810, Loss 2.927710\n",
      "  params :  tensor([  5.3630, -17.2778])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3811, Loss 2.927710\n",
      "  params :  tensor([  5.3630, -17.2779])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3812, Loss 2.927708\n",
      "  params :  tensor([  5.3630, -17.2779])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3813, Loss 2.927708\n",
      "  params :  tensor([  5.3630, -17.2780])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3814, Loss 2.927709\n",
      "  params :  tensor([  5.3630, -17.2780])\n",
      "  grad :  tensor([-0.0008,  0.0046])\n",
      "Epoch 3815, Loss 2.927709\n",
      "  params :  tensor([  5.3630, -17.2781])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3816, Loss 2.927710\n",
      "  params :  tensor([  5.3630, -17.2781])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3817, Loss 2.927708\n",
      "  params :  tensor([  5.3630, -17.2781])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3818, Loss 2.927708\n",
      "  params :  tensor([  5.3630, -17.2782])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3819, Loss 2.927706\n",
      "  params :  tensor([  5.3630, -17.2782])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3820, Loss 2.927707\n",
      "  params :  tensor([  5.3630, -17.2783])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3821, Loss 2.927708\n",
      "  params :  tensor([  5.3630, -17.2783])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3822, Loss 2.927707\n",
      "  params :  tensor([  5.3631, -17.2784])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3823, Loss 2.927707\n",
      "  params :  tensor([  5.3631, -17.2784])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3824, Loss 2.927707\n",
      "  params :  tensor([  5.3631, -17.2785])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3825, Loss 2.927708\n",
      "  params :  tensor([  5.3631, -17.2785])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3826, Loss 2.927707\n",
      "  params :  tensor([  5.3631, -17.2786])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3827, Loss 2.927706\n",
      "  params :  tensor([  5.3631, -17.2786])\n",
      "  grad :  tensor([-0.0008,  0.0045])\n",
      "Epoch 3828, Loss 2.927707\n",
      "  params :  tensor([  5.3631, -17.2786])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3829, Loss 2.927705\n",
      "  params :  tensor([  5.3631, -17.2787])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3830, Loss 2.927706\n",
      "  params :  tensor([  5.3631, -17.2787])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3831, Loss 2.927706\n",
      "  params :  tensor([  5.3631, -17.2788])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3832, Loss 2.927705\n",
      "  params :  tensor([  5.3631, -17.2788])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3833, Loss 2.927705\n",
      "  params :  tensor([  5.3631, -17.2789])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3834, Loss 2.927705\n",
      "  params :  tensor([  5.3631, -17.2789])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3835, Loss 2.927705\n",
      "  params :  tensor([  5.3632, -17.2789])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3836, Loss 2.927705\n",
      "  params :  tensor([  5.3632, -17.2790])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3837, Loss 2.927705\n",
      "  params :  tensor([  5.3632, -17.2790])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3838, Loss 2.927704\n",
      "  params :  tensor([  5.3632, -17.2791])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3839, Loss 2.927704\n",
      "  params :  tensor([  5.3632, -17.2791])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3840, Loss 2.927704\n",
      "  params :  tensor([  5.3632, -17.2792])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3841, Loss 2.927703\n",
      "  params :  tensor([  5.3632, -17.2792])\n",
      "  grad :  tensor([-0.0008,  0.0044])\n",
      "Epoch 3842, Loss 2.927702\n",
      "  params :  tensor([  5.3632, -17.2793])\n",
      "  grad :  tensor([-0.0008,  0.0043])\n",
      "Epoch 3843, Loss 2.927703\n",
      "  params :  tensor([  5.3632, -17.2793])\n",
      "  grad :  tensor([-0.0008,  0.0043])\n",
      "Epoch 3844, Loss 2.927703\n",
      "  params :  tensor([  5.3632, -17.2793])\n",
      "  grad :  tensor([-0.0008,  0.0043])\n",
      "Epoch 3845, Loss 2.927704\n",
      "  params :  tensor([  5.3632, -17.2794])\n",
      "  grad :  tensor([-0.0008,  0.0043])\n",
      "Epoch 3846, Loss 2.927702\n",
      "  params :  tensor([  5.3632, -17.2794])\n",
      "  grad :  tensor([-0.0008,  0.0043])\n",
      "Epoch 3847, Loss 2.927701\n",
      "  params :  tensor([  5.3632, -17.2795])\n",
      "  grad :  tensor([-0.0008,  0.0043])\n",
      "Epoch 3848, Loss 2.927703\n",
      "  params :  tensor([  5.3633, -17.2795])\n",
      "  grad :  tensor([-0.0008,  0.0043])\n",
      "Epoch 3849, Loss 2.927702\n",
      "  params :  tensor([  5.3633, -17.2796])\n",
      "  grad :  tensor([-0.0008,  0.0043])\n",
      "Epoch 3850, Loss 2.927701\n",
      "  params :  tensor([  5.3633, -17.2796])\n",
      "  grad :  tensor([-0.0007,  0.0043])\n",
      "Epoch 3851, Loss 2.927701\n",
      "  params :  tensor([  5.3633, -17.2796])\n",
      "  grad :  tensor([-0.0007,  0.0043])\n",
      "Epoch 3852, Loss 2.927703\n",
      "  params :  tensor([  5.3633, -17.2797])\n",
      "  grad :  tensor([-0.0007,  0.0043])\n",
      "Epoch 3853, Loss 2.927700\n",
      "  params :  tensor([  5.3633, -17.2797])\n",
      "  grad :  tensor([-0.0007,  0.0043])\n",
      "Epoch 3854, Loss 2.927701\n",
      "  params :  tensor([  5.3633, -17.2798])\n",
      "  grad :  tensor([-0.0007,  0.0043])\n",
      "Epoch 3855, Loss 2.927701\n",
      "  params :  tensor([  5.3633, -17.2798])\n",
      "  grad :  tensor([-0.0007,  0.0043])\n",
      "Epoch 3856, Loss 2.927700\n",
      "  params :  tensor([  5.3633, -17.2799])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3857, Loss 2.927700\n",
      "  params :  tensor([  5.3633, -17.2799])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3858, Loss 2.927700\n",
      "  params :  tensor([  5.3633, -17.2799])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3859, Loss 2.927701\n",
      "  params :  tensor([  5.3633, -17.2800])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3860, Loss 2.927699\n",
      "  params :  tensor([  5.3633, -17.2800])\n",
      "  grad :  tensor([-0.0008,  0.0042])\n",
      "Epoch 3861, Loss 2.927699\n",
      "  params :  tensor([  5.3634, -17.2801])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3862, Loss 2.927700\n",
      "  params :  tensor([  5.3634, -17.2801])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3863, Loss 2.927699\n",
      "  params :  tensor([  5.3634, -17.2801])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3864, Loss 2.927698\n",
      "  params :  tensor([  5.3634, -17.2802])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3865, Loss 2.927699\n",
      "  params :  tensor([  5.3634, -17.2802])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3866, Loss 2.927697\n",
      "  params :  tensor([  5.3634, -17.2803])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3867, Loss 2.927700\n",
      "  params :  tensor([  5.3634, -17.2803])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3868, Loss 2.927699\n",
      "  params :  tensor([  5.3634, -17.2804])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3869, Loss 2.927698\n",
      "  params :  tensor([  5.3634, -17.2804])\n",
      "  grad :  tensor([-0.0007,  0.0042])\n",
      "Epoch 3870, Loss 2.927697\n",
      "  params :  tensor([  5.3634, -17.2804])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3871, Loss 2.927698\n",
      "  params :  tensor([  5.3634, -17.2805])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3872, Loss 2.927696\n",
      "  params :  tensor([  5.3634, -17.2805])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3873, Loss 2.927699\n",
      "  params :  tensor([  5.3634, -17.2806])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3874, Loss 2.927698\n",
      "  params :  tensor([  5.3634, -17.2806])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3875, Loss 2.927696\n",
      "  params :  tensor([  5.3635, -17.2806])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3876, Loss 2.927698\n",
      "  params :  tensor([  5.3635, -17.2807])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3877, Loss 2.927697\n",
      "  params :  tensor([  5.3635, -17.2807])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3878, Loss 2.927696\n",
      "  params :  tensor([  5.3635, -17.2808])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3879, Loss 2.927697\n",
      "  params :  tensor([  5.3635, -17.2808])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3880, Loss 2.927696\n",
      "  params :  tensor([  5.3635, -17.2808])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3881, Loss 2.927696\n",
      "  params :  tensor([  5.3635, -17.2809])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3882, Loss 2.927696\n",
      "  params :  tensor([  5.3635, -17.2809])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3883, Loss 2.927696\n",
      "  params :  tensor([  5.3635, -17.2810])\n",
      "  grad :  tensor([-0.0007,  0.0041])\n",
      "Epoch 3884, Loss 2.927696\n",
      "  params :  tensor([  5.3635, -17.2810])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3885, Loss 2.927695\n",
      "  params :  tensor([  5.3635, -17.2810])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3886, Loss 2.927696\n",
      "  params :  tensor([  5.3635, -17.2811])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3887, Loss 2.927696\n",
      "  params :  tensor([  5.3635, -17.2811])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3888, Loss 2.927695\n",
      "  params :  tensor([  5.3635, -17.2812])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3889, Loss 2.927695\n",
      "  params :  tensor([  5.3636, -17.2812])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3890, Loss 2.927694\n",
      "  params :  tensor([  5.3636, -17.2812])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3891, Loss 2.927693\n",
      "  params :  tensor([  5.3636, -17.2813])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3892, Loss 2.927693\n",
      "  params :  tensor([  5.3636, -17.2813])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3893, Loss 2.927695\n",
      "  params :  tensor([  5.3636, -17.2814])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3894, Loss 2.927695\n",
      "  params :  tensor([  5.3636, -17.2814])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3895, Loss 2.927694\n",
      "  params :  tensor([  5.3636, -17.2815])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3896, Loss 2.927696\n",
      "  params :  tensor([  5.3636, -17.2815])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3897, Loss 2.927693\n",
      "  params :  tensor([  5.3636, -17.2815])\n",
      "  grad :  tensor([-0.0007,  0.0040])\n",
      "Epoch 3898, Loss 2.927693\n",
      "  params :  tensor([  5.3636, -17.2816])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3899, Loss 2.927694\n",
      "  params :  tensor([  5.3636, -17.2816])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3900, Loss 2.927693\n",
      "  params :  tensor([  5.3636, -17.2817])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3901, Loss 2.927692\n",
      "  params :  tensor([  5.3636, -17.2817])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3902, Loss 2.927694\n",
      "  params :  tensor([  5.3636, -17.2817])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3903, Loss 2.927692\n",
      "  params :  tensor([  5.3637, -17.2818])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3904, Loss 2.927693\n",
      "  params :  tensor([  5.3637, -17.2818])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3905, Loss 2.927691\n",
      "  params :  tensor([  5.3637, -17.2818])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3906, Loss 2.927692\n",
      "  params :  tensor([  5.3637, -17.2819])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3907, Loss 2.927692\n",
      "  params :  tensor([  5.3637, -17.2819])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3908, Loss 2.927692\n",
      "  params :  tensor([  5.3637, -17.2820])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3909, Loss 2.927691\n",
      "  params :  tensor([  5.3637, -17.2820])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3910, Loss 2.927692\n",
      "  params :  tensor([  5.3637, -17.2820])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3911, Loss 2.927690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3637, -17.2821])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3912, Loss 2.927691\n",
      "  params :  tensor([  5.3637, -17.2821])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3913, Loss 2.927691\n",
      "  params :  tensor([  5.3637, -17.2822])\n",
      "  grad :  tensor([-0.0007,  0.0039])\n",
      "Epoch 3914, Loss 2.927691\n",
      "  params :  tensor([  5.3637, -17.2822])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3915, Loss 2.927690\n",
      "  params :  tensor([  5.3637, -17.2822])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3916, Loss 2.927691\n",
      "  params :  tensor([  5.3637, -17.2823])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3917, Loss 2.927691\n",
      "  params :  tensor([  5.3637, -17.2823])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3918, Loss 2.927689\n",
      "  params :  tensor([  5.3638, -17.2823])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3919, Loss 2.927690\n",
      "  params :  tensor([  5.3638, -17.2824])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3920, Loss 2.927690\n",
      "  params :  tensor([  5.3638, -17.2824])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3921, Loss 2.927690\n",
      "  params :  tensor([  5.3638, -17.2825])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3922, Loss 2.927690\n",
      "  params :  tensor([  5.3638, -17.2825])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3923, Loss 2.927689\n",
      "  params :  tensor([  5.3638, -17.2825])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3924, Loss 2.927689\n",
      "  params :  tensor([  5.3638, -17.2826])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3925, Loss 2.927689\n",
      "  params :  tensor([  5.3638, -17.2826])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3926, Loss 2.927688\n",
      "  params :  tensor([  5.3638, -17.2826])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3927, Loss 2.927689\n",
      "  params :  tensor([  5.3638, -17.2827])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3928, Loss 2.927689\n",
      "  params :  tensor([  5.3638, -17.2827])\n",
      "  grad :  tensor([-0.0007,  0.0038])\n",
      "Epoch 3929, Loss 2.927688\n",
      "  params :  tensor([  5.3638, -17.2828])\n",
      "  grad :  tensor([-0.0007,  0.0037])\n",
      "Epoch 3930, Loss 2.927688\n",
      "  params :  tensor([  5.3638, -17.2828])\n",
      "  grad :  tensor([-0.0007,  0.0037])\n",
      "Epoch 3931, Loss 2.927688\n",
      "  params :  tensor([  5.3638, -17.2828])\n",
      "  grad :  tensor([-0.0007,  0.0037])\n",
      "Epoch 3932, Loss 2.927688\n",
      "  params :  tensor([  5.3638, -17.2829])\n",
      "  grad :  tensor([-0.0007,  0.0037])\n",
      "Epoch 3933, Loss 2.927687\n",
      "  params :  tensor([  5.3639, -17.2829])\n",
      "  grad :  tensor([-0.0007,  0.0037])\n",
      "Epoch 3934, Loss 2.927689\n",
      "  params :  tensor([  5.3639, -17.2829])\n",
      "  grad :  tensor([-0.0006,  0.0037])\n",
      "Epoch 3935, Loss 2.927688\n",
      "  params :  tensor([  5.3639, -17.2830])\n",
      "  grad :  tensor([-0.0006,  0.0037])\n",
      "Epoch 3936, Loss 2.927687\n",
      "  params :  tensor([  5.3639, -17.2830])\n",
      "  grad :  tensor([-0.0006,  0.0037])\n",
      "Epoch 3937, Loss 2.927687\n",
      "  params :  tensor([  5.3639, -17.2831])\n",
      "  grad :  tensor([-0.0006,  0.0037])\n",
      "Epoch 3938, Loss 2.927687\n",
      "  params :  tensor([  5.3639, -17.2831])\n",
      "  grad :  tensor([-0.0006,  0.0037])\n",
      "Epoch 3939, Loss 2.927686\n",
      "  params :  tensor([  5.3639, -17.2831])\n",
      "  grad :  tensor([-0.0006,  0.0037])\n",
      "Epoch 3940, Loss 2.927686\n",
      "  params :  tensor([  5.3639, -17.2832])\n",
      "  grad :  tensor([-0.0007,  0.0037])\n",
      "Epoch 3941, Loss 2.927687\n",
      "  params :  tensor([  5.3639, -17.2832])\n",
      "  grad :  tensor([-0.0006,  0.0037])\n",
      "Epoch 3942, Loss 2.927686\n",
      "  params :  tensor([  5.3639, -17.2832])\n",
      "  grad :  tensor([-0.0006,  0.0037])\n",
      "Epoch 3943, Loss 2.927686\n",
      "  params :  tensor([  5.3639, -17.2833])\n",
      "  grad :  tensor([-0.0006,  0.0037])\n",
      "Epoch 3944, Loss 2.927686\n",
      "  params :  tensor([  5.3639, -17.2833])\n",
      "  grad :  tensor([-0.0006,  0.0037])\n",
      "Epoch 3945, Loss 2.927686\n",
      "  params :  tensor([  5.3639, -17.2833])\n",
      "  grad :  tensor([-0.0007,  0.0036])\n",
      "Epoch 3946, Loss 2.927685\n",
      "  params :  tensor([  5.3639, -17.2834])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3947, Loss 2.927685\n",
      "  params :  tensor([  5.3639, -17.2834])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3948, Loss 2.927686\n",
      "  params :  tensor([  5.3640, -17.2835])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3949, Loss 2.927685\n",
      "  params :  tensor([  5.3640, -17.2835])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3950, Loss 2.927686\n",
      "  params :  tensor([  5.3640, -17.2835])\n",
      "  grad :  tensor([-0.0007,  0.0036])\n",
      "Epoch 3951, Loss 2.927686\n",
      "  params :  tensor([  5.3640, -17.2836])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3952, Loss 2.927687\n",
      "  params :  tensor([  5.3640, -17.2836])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3953, Loss 2.927685\n",
      "  params :  tensor([  5.3640, -17.2836])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3954, Loss 2.927686\n",
      "  params :  tensor([  5.3640, -17.2837])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3955, Loss 2.927686\n",
      "  params :  tensor([  5.3640, -17.2837])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3956, Loss 2.927685\n",
      "  params :  tensor([  5.3640, -17.2837])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3957, Loss 2.927683\n",
      "  params :  tensor([  5.3640, -17.2838])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3958, Loss 2.927684\n",
      "  params :  tensor([  5.3640, -17.2838])\n",
      "  grad :  tensor([-0.0007,  0.0036])\n",
      "Epoch 3959, Loss 2.927685\n",
      "  params :  tensor([  5.3640, -17.2839])\n",
      "  grad :  tensor([-0.0006,  0.0036])\n",
      "Epoch 3960, Loss 2.927684\n",
      "  params :  tensor([  5.3640, -17.2839])\n",
      "  grad :  tensor([-0.0007,  0.0036])\n",
      "Epoch 3961, Loss 2.927684\n",
      "  params :  tensor([  5.3640, -17.2839])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3962, Loss 2.927684\n",
      "  params :  tensor([  5.3640, -17.2840])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3963, Loss 2.927685\n",
      "  params :  tensor([  5.3640, -17.2840])\n",
      "  grad :  tensor([-0.0007,  0.0035])\n",
      "Epoch 3964, Loss 2.927683\n",
      "  params :  tensor([  5.3641, -17.2840])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3965, Loss 2.927685\n",
      "  params :  tensor([  5.3641, -17.2841])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3966, Loss 2.927685\n",
      "  params :  tensor([  5.3641, -17.2841])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3967, Loss 2.927684\n",
      "  params :  tensor([  5.3641, -17.2841])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3968, Loss 2.927683\n",
      "  params :  tensor([  5.3641, -17.2842])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3969, Loss 2.927683\n",
      "  params :  tensor([  5.3641, -17.2842])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3970, Loss 2.927682\n",
      "  params :  tensor([  5.3641, -17.2842])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3971, Loss 2.927683\n",
      "  params :  tensor([  5.3641, -17.2843])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3972, Loss 2.927684\n",
      "  params :  tensor([  5.3641, -17.2843])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3973, Loss 2.927682\n",
      "  params :  tensor([  5.3641, -17.2843])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3974, Loss 2.927683\n",
      "  params :  tensor([  5.3641, -17.2844])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3975, Loss 2.927683\n",
      "  params :  tensor([  5.3641, -17.2844])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3976, Loss 2.927683\n",
      "  params :  tensor([  5.3641, -17.2844])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3977, Loss 2.927682\n",
      "  params :  tensor([  5.3641, -17.2845])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3978, Loss 2.927682\n",
      "  params :  tensor([  5.3641, -17.2845])\n",
      "  grad :  tensor([-0.0006,  0.0035])\n",
      "Epoch 3979, Loss 2.927682\n",
      "  params :  tensor([  5.3641, -17.2845])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3980, Loss 2.927681\n",
      "  params :  tensor([  5.3642, -17.2846])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3981, Loss 2.927682\n",
      "  params :  tensor([  5.3642, -17.2846])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3982, Loss 2.927682\n",
      "  params :  tensor([  5.3642, -17.2847])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3983, Loss 2.927682\n",
      "  params :  tensor([  5.3642, -17.2847])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3984, Loss 2.927682\n",
      "  params :  tensor([  5.3642, -17.2847])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3985, Loss 2.927682\n",
      "  params :  tensor([  5.3642, -17.2848])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3986, Loss 2.927682\n",
      "  params :  tensor([  5.3642, -17.2848])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3987, Loss 2.927680\n",
      "  params :  tensor([  5.3642, -17.2848])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3988, Loss 2.927682\n",
      "  params :  tensor([  5.3642, -17.2849])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3989, Loss 2.927681\n",
      "  params :  tensor([  5.3642, -17.2849])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3990, Loss 2.927681\n",
      "  params :  tensor([  5.3642, -17.2849])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3991, Loss 2.927680\n",
      "  params :  tensor([  5.3642, -17.2850])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3992, Loss 2.927681\n",
      "  params :  tensor([  5.3642, -17.2850])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3993, Loss 2.927680\n",
      "  params :  tensor([  5.3642, -17.2850])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3994, Loss 2.927680\n",
      "  params :  tensor([  5.3642, -17.2851])\n",
      "  grad :  tensor([-0.0006,  0.0034])\n",
      "Epoch 3995, Loss 2.927681\n",
      "  params :  tensor([  5.3642, -17.2851])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 3996, Loss 2.927681\n",
      "  params :  tensor([  5.3642, -17.2851])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 3997, Loss 2.927679\n",
      "  params :  tensor([  5.3643, -17.2852])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 3998, Loss 2.927680\n",
      "  params :  tensor([  5.3643, -17.2852])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 3999, Loss 2.927679\n",
      "  params :  tensor([  5.3643, -17.2852])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4000, Loss 2.927680\n",
      "  params :  tensor([  5.3643, -17.2853])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4001, Loss 2.927680\n",
      "  params :  tensor([  5.3643, -17.2853])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4002, Loss 2.927681\n",
      "  params :  tensor([  5.3643, -17.2853])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4003, Loss 2.927679\n",
      "  params :  tensor([  5.3643, -17.2854])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4004, Loss 2.927679\n",
      "  params :  tensor([  5.3643, -17.2854])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4005, Loss 2.927680\n",
      "  params :  tensor([  5.3643, -17.2854])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4006, Loss 2.927680\n",
      "  params :  tensor([  5.3643, -17.2855])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4007, Loss 2.927677\n",
      "  params :  tensor([  5.3643, -17.2855])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4008, Loss 2.927678\n",
      "  params :  tensor([  5.3643, -17.2855])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4009, Loss 2.927679\n",
      "  params :  tensor([  5.3643, -17.2856])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4010, Loss 2.927678\n",
      "  params :  tensor([  5.3643, -17.2856])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4011, Loss 2.927679\n",
      "  params :  tensor([  5.3643, -17.2856])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4012, Loss 2.927679\n",
      "  params :  tensor([  5.3643, -17.2857])\n",
      "  grad :  tensor([-0.0006,  0.0033])\n",
      "Epoch 4013, Loss 2.927679\n",
      "  params :  tensor([  5.3643, -17.2857])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4014, Loss 2.927677\n",
      "  params :  tensor([  5.3644, -17.2857])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4015, Loss 2.927677\n",
      "  params :  tensor([  5.3644, -17.2857])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4016, Loss 2.927677\n",
      "  params :  tensor([  5.3644, -17.2858])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4017, Loss 2.927679\n",
      "  params :  tensor([  5.3644, -17.2858])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4018, Loss 2.927677\n",
      "  params :  tensor([  5.3644, -17.2858])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4019, Loss 2.927678\n",
      "  params :  tensor([  5.3644, -17.2859])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4020, Loss 2.927678\n",
      "  params :  tensor([  5.3644, -17.2859])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4021, Loss 2.927677\n",
      "  params :  tensor([  5.3644, -17.2859])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4022, Loss 2.927677\n",
      "  params :  tensor([  5.3644, -17.2860])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4023, Loss 2.927678\n",
      "  params :  tensor([  5.3644, -17.2860])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4024, Loss 2.927677\n",
      "  params :  tensor([  5.3644, -17.2860])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4025, Loss 2.927677\n",
      "  params :  tensor([  5.3644, -17.2861])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4026, Loss 2.927676\n",
      "  params :  tensor([  5.3644, -17.2861])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4027, Loss 2.927676\n",
      "  params :  tensor([  5.3644, -17.2861])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4028, Loss 2.927675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3644, -17.2862])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4029, Loss 2.927677\n",
      "  params :  tensor([  5.3644, -17.2862])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4030, Loss 2.927674\n",
      "  params :  tensor([  5.3644, -17.2862])\n",
      "  grad :  tensor([-0.0006,  0.0032])\n",
      "Epoch 4031, Loss 2.927676\n",
      "  params :  tensor([  5.3644, -17.2863])\n",
      "  grad :  tensor([-0.0006,  0.0031])\n",
      "Epoch 4032, Loss 2.927675\n",
      "  params :  tensor([  5.3645, -17.2863])\n",
      "  grad :  tensor([-0.0006,  0.0031])\n",
      "Epoch 4033, Loss 2.927675\n",
      "  params :  tensor([  5.3645, -17.2863])\n",
      "  grad :  tensor([-0.0006,  0.0031])\n",
      "Epoch 4034, Loss 2.927675\n",
      "  params :  tensor([  5.3645, -17.2864])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4035, Loss 2.927675\n",
      "  params :  tensor([  5.3645, -17.2864])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4036, Loss 2.927674\n",
      "  params :  tensor([  5.3645, -17.2864])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4037, Loss 2.927674\n",
      "  params :  tensor([  5.3645, -17.2865])\n",
      "  grad :  tensor([-0.0006,  0.0031])\n",
      "Epoch 4038, Loss 2.927677\n",
      "  params :  tensor([  5.3645, -17.2865])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4039, Loss 2.927674\n",
      "  params :  tensor([  5.3645, -17.2865])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4040, Loss 2.927676\n",
      "  params :  tensor([  5.3645, -17.2865])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4041, Loss 2.927675\n",
      "  params :  tensor([  5.3645, -17.2866])\n",
      "  grad :  tensor([-0.0006,  0.0031])\n",
      "Epoch 4042, Loss 2.927675\n",
      "  params :  tensor([  5.3645, -17.2866])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4043, Loss 2.927675\n",
      "  params :  tensor([  5.3645, -17.2866])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4044, Loss 2.927674\n",
      "  params :  tensor([  5.3645, -17.2867])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4045, Loss 2.927674\n",
      "  params :  tensor([  5.3645, -17.2867])\n",
      "  grad :  tensor([-0.0006,  0.0031])\n",
      "Epoch 4046, Loss 2.927675\n",
      "  params :  tensor([  5.3645, -17.2867])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4047, Loss 2.927674\n",
      "  params :  tensor([  5.3645, -17.2868])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4048, Loss 2.927674\n",
      "  params :  tensor([  5.3645, -17.2868])\n",
      "  grad :  tensor([-0.0006,  0.0031])\n",
      "Epoch 4049, Loss 2.927675\n",
      "  params :  tensor([  5.3645, -17.2868])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4050, Loss 2.927672\n",
      "  params :  tensor([  5.3646, -17.2868])\n",
      "  grad :  tensor([-0.0005,  0.0031])\n",
      "Epoch 4051, Loss 2.927675\n",
      "  params :  tensor([  5.3646, -17.2869])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4052, Loss 2.927675\n",
      "  params :  tensor([  5.3646, -17.2869])\n",
      "  grad :  tensor([-0.0006,  0.0030])\n",
      "Epoch 4053, Loss 2.927673\n",
      "  params :  tensor([  5.3646, -17.2869])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4054, Loss 2.927673\n",
      "  params :  tensor([  5.3646, -17.2870])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4055, Loss 2.927674\n",
      "  params :  tensor([  5.3646, -17.2870])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4056, Loss 2.927673\n",
      "  params :  tensor([  5.3646, -17.2870])\n",
      "  grad :  tensor([-0.0006,  0.0030])\n",
      "Epoch 4057, Loss 2.927674\n",
      "  params :  tensor([  5.3646, -17.2871])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4058, Loss 2.927672\n",
      "  params :  tensor([  5.3646, -17.2871])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4059, Loss 2.927674\n",
      "  params :  tensor([  5.3646, -17.2871])\n",
      "  grad :  tensor([-0.0006,  0.0030])\n",
      "Epoch 4060, Loss 2.927675\n",
      "  params :  tensor([  5.3646, -17.2872])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4061, Loss 2.927672\n",
      "  params :  tensor([  5.3646, -17.2872])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4062, Loss 2.927673\n",
      "  params :  tensor([  5.3646, -17.2872])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4063, Loss 2.927675\n",
      "  params :  tensor([  5.3646, -17.2872])\n",
      "  grad :  tensor([-0.0006,  0.0030])\n",
      "Epoch 4064, Loss 2.927673\n",
      "  params :  tensor([  5.3646, -17.2873])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4065, Loss 2.927674\n",
      "  params :  tensor([  5.3646, -17.2873])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4066, Loss 2.927672\n",
      "  params :  tensor([  5.3646, -17.2873])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4067, Loss 2.927673\n",
      "  params :  tensor([  5.3646, -17.2874])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4068, Loss 2.927673\n",
      "  params :  tensor([  5.3646, -17.2874])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4069, Loss 2.927672\n",
      "  params :  tensor([  5.3647, -17.2874])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4070, Loss 2.927672\n",
      "  params :  tensor([  5.3647, -17.2875])\n",
      "  grad :  tensor([-0.0005,  0.0030])\n",
      "Epoch 4071, Loss 2.927673\n",
      "  params :  tensor([  5.3647, -17.2875])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4072, Loss 2.927673\n",
      "  params :  tensor([  5.3647, -17.2875])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4073, Loss 2.927671\n",
      "  params :  tensor([  5.3647, -17.2875])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4074, Loss 2.927673\n",
      "  params :  tensor([  5.3647, -17.2876])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4075, Loss 2.927672\n",
      "  params :  tensor([  5.3647, -17.2876])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4076, Loss 2.927672\n",
      "  params :  tensor([  5.3647, -17.2876])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4077, Loss 2.927672\n",
      "  params :  tensor([  5.3647, -17.2877])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4078, Loss 2.927670\n",
      "  params :  tensor([  5.3647, -17.2877])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4079, Loss 2.927672\n",
      "  params :  tensor([  5.3647, -17.2877])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4080, Loss 2.927672\n",
      "  params :  tensor([  5.3647, -17.2877])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4081, Loss 2.927671\n",
      "  params :  tensor([  5.3647, -17.2878])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4082, Loss 2.927670\n",
      "  params :  tensor([  5.3647, -17.2878])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4083, Loss 2.927673\n",
      "  params :  tensor([  5.3647, -17.2878])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4084, Loss 2.927672\n",
      "  params :  tensor([  5.3647, -17.2879])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4085, Loss 2.927670\n",
      "  params :  tensor([  5.3647, -17.2879])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4086, Loss 2.927670\n",
      "  params :  tensor([  5.3647, -17.2879])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4087, Loss 2.927670\n",
      "  params :  tensor([  5.3647, -17.2879])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4088, Loss 2.927672\n",
      "  params :  tensor([  5.3647, -17.2880])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4089, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2880])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4090, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2880])\n",
      "  grad :  tensor([-0.0005,  0.0029])\n",
      "Epoch 4091, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2881])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4092, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2881])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4093, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2881])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4094, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2881])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4095, Loss 2.927669\n",
      "  params :  tensor([  5.3648, -17.2882])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4096, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2882])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4097, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2882])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4098, Loss 2.927671\n",
      "  params :  tensor([  5.3648, -17.2883])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4099, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2883])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4100, Loss 2.927671\n",
      "  params :  tensor([  5.3648, -17.2883])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4101, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2883])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4102, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2884])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4103, Loss 2.927671\n",
      "  params :  tensor([  5.3648, -17.2884])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4104, Loss 2.927669\n",
      "  params :  tensor([  5.3648, -17.2884])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4105, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2885])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4106, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2885])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4107, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2885])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4108, Loss 2.927670\n",
      "  params :  tensor([  5.3648, -17.2885])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4109, Loss 2.927668\n",
      "  params :  tensor([  5.3649, -17.2886])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4110, Loss 2.927669\n",
      "  params :  tensor([  5.3649, -17.2886])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4111, Loss 2.927669\n",
      "  params :  tensor([  5.3649, -17.2886])\n",
      "  grad :  tensor([-0.0005,  0.0028])\n",
      "Epoch 4112, Loss 2.927670\n",
      "  params :  tensor([  5.3649, -17.2886])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4113, Loss 2.927670\n",
      "  params :  tensor([  5.3649, -17.2887])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4114, Loss 2.927670\n",
      "  params :  tensor([  5.3649, -17.2887])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4115, Loss 2.927670\n",
      "  params :  tensor([  5.3649, -17.2887])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4116, Loss 2.927669\n",
      "  params :  tensor([  5.3649, -17.2887])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4117, Loss 2.927669\n",
      "  params :  tensor([  5.3649, -17.2888])\n",
      "  grad :  tensor([-0.0004,  0.0027])\n",
      "Epoch 4118, Loss 2.927670\n",
      "  params :  tensor([  5.3649, -17.2888])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4119, Loss 2.927669\n",
      "  params :  tensor([  5.3649, -17.2888])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4120, Loss 2.927670\n",
      "  params :  tensor([  5.3649, -17.2889])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4121, Loss 2.927668\n",
      "  params :  tensor([  5.3649, -17.2889])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4122, Loss 2.927668\n",
      "  params :  tensor([  5.3649, -17.2889])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4123, Loss 2.927669\n",
      "  params :  tensor([  5.3649, -17.2889])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4124, Loss 2.927668\n",
      "  params :  tensor([  5.3649, -17.2890])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4125, Loss 2.927670\n",
      "  params :  tensor([  5.3649, -17.2890])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4126, Loss 2.927666\n",
      "  params :  tensor([  5.3649, -17.2890])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4127, Loss 2.927669\n",
      "  params :  tensor([  5.3649, -17.2890])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4128, Loss 2.927668\n",
      "  params :  tensor([  5.3649, -17.2891])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4129, Loss 2.927669\n",
      "  params :  tensor([  5.3649, -17.2891])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4130, Loss 2.927667\n",
      "  params :  tensor([  5.3650, -17.2891])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4131, Loss 2.927667\n",
      "  params :  tensor([  5.3650, -17.2892])\n",
      "  grad :  tensor([-0.0004,  0.0027])\n",
      "Epoch 4132, Loss 2.927668\n",
      "  params :  tensor([  5.3650, -17.2892])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4133, Loss 2.927667\n",
      "  params :  tensor([  5.3650, -17.2892])\n",
      "  grad :  tensor([-0.0005,  0.0027])\n",
      "Epoch 4134, Loss 2.927667\n",
      "  params :  tensor([  5.3650, -17.2892])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4135, Loss 2.927666\n",
      "  params :  tensor([  5.3650, -17.2893])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4136, Loss 2.927666\n",
      "  params :  tensor([  5.3650, -17.2893])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4137, Loss 2.927669\n",
      "  params :  tensor([  5.3650, -17.2893])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4138, Loss 2.927666\n",
      "  params :  tensor([  5.3650, -17.2893])\n",
      "  grad :  tensor([-0.0004,  0.0026])\n",
      "Epoch 4139, Loss 2.927668\n",
      "  params :  tensor([  5.3650, -17.2894])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4140, Loss 2.927666\n",
      "  params :  tensor([  5.3650, -17.2894])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4141, Loss 2.927667\n",
      "  params :  tensor([  5.3650, -17.2894])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4142, Loss 2.927667\n",
      "  params :  tensor([  5.3650, -17.2894])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4143, Loss 2.927666\n",
      "  params :  tensor([  5.3650, -17.2895])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4144, Loss 2.927667\n",
      "  params :  tensor([  5.3650, -17.2895])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4145, Loss 2.927666\n",
      "  params :  tensor([  5.3650, -17.2895])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4146, Loss 2.927667\n",
      "  params :  tensor([  5.3650, -17.2896])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4147, Loss 2.927667\n",
      "  params :  tensor([  5.3650, -17.2896])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4148, Loss 2.927667\n",
      "  params :  tensor([  5.3650, -17.2896])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4149, Loss 2.927667\n",
      "  params :  tensor([  5.3650, -17.2896])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4150, Loss 2.927665\n",
      "  params :  tensor([  5.3650, -17.2897])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4151, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2897])\n",
      "  grad :  tensor([-0.0004,  0.0026])\n",
      "Epoch 4152, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2897])\n",
      "  grad :  tensor([-0.0004,  0.0026])\n",
      "Epoch 4153, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2897])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4154, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2898])\n",
      "  grad :  tensor([-0.0005,  0.0026])\n",
      "Epoch 4155, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2898])\n",
      "  grad :  tensor([-0.0004,  0.0026])\n",
      "Epoch 4156, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2898])\n",
      "  grad :  tensor([-0.0004,  0.0026])\n",
      "Epoch 4157, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2898])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4158, Loss 2.927665\n",
      "  params :  tensor([  5.3651, -17.2899])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4159, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2899])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4160, Loss 2.927665\n",
      "  params :  tensor([  5.3651, -17.2899])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4161, Loss 2.927664\n",
      "  params :  tensor([  5.3651, -17.2899])\n",
      "  grad :  tensor([-0.0005,  0.0025])\n",
      "Epoch 4162, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2900])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4163, Loss 2.927665\n",
      "  params :  tensor([  5.3651, -17.2900])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4164, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2900])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4165, Loss 2.927664\n",
      "  params :  tensor([  5.3651, -17.2900])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4166, Loss 2.927665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3651, -17.2901])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4167, Loss 2.927665\n",
      "  params :  tensor([  5.3651, -17.2901])\n",
      "  grad :  tensor([-0.0005,  0.0025])\n",
      "Epoch 4168, Loss 2.927665\n",
      "  params :  tensor([  5.3651, -17.2901])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4169, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2901])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4170, Loss 2.927664\n",
      "  params :  tensor([  5.3651, -17.2902])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4171, Loss 2.927665\n",
      "  params :  tensor([  5.3651, -17.2902])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4172, Loss 2.927666\n",
      "  params :  tensor([  5.3651, -17.2902])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4173, Loss 2.927663\n",
      "  params :  tensor([  5.3651, -17.2902])\n",
      "  grad :  tensor([-0.0005,  0.0025])\n",
      "Epoch 4174, Loss 2.927664\n",
      "  params :  tensor([  5.3652, -17.2903])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4175, Loss 2.927664\n",
      "  params :  tensor([  5.3652, -17.2903])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4176, Loss 2.927665\n",
      "  params :  tensor([  5.3652, -17.2903])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4177, Loss 2.927663\n",
      "  params :  tensor([  5.3652, -17.2903])\n",
      "  grad :  tensor([-0.0004,  0.0025])\n",
      "Epoch 4178, Loss 2.927664\n",
      "  params :  tensor([  5.3652, -17.2903])\n",
      "  grad :  tensor([-0.0005,  0.0025])\n",
      "Epoch 4179, Loss 2.927664\n",
      "  params :  tensor([  5.3652, -17.2904])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4180, Loss 2.927663\n",
      "  params :  tensor([  5.3652, -17.2904])\n",
      "  grad :  tensor([-0.0005,  0.0024])\n",
      "Epoch 4181, Loss 2.927664\n",
      "  params :  tensor([  5.3652, -17.2904])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4182, Loss 2.927664\n",
      "  params :  tensor([  5.3652, -17.2904])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4183, Loss 2.927663\n",
      "  params :  tensor([  5.3652, -17.2905])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4184, Loss 2.927664\n",
      "  params :  tensor([  5.3652, -17.2905])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4185, Loss 2.927664\n",
      "  params :  tensor([  5.3652, -17.2905])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4186, Loss 2.927662\n",
      "  params :  tensor([  5.3652, -17.2905])\n",
      "  grad :  tensor([-0.0005,  0.0024])\n",
      "Epoch 4187, Loss 2.927665\n",
      "  params :  tensor([  5.3652, -17.2906])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4188, Loss 2.927663\n",
      "  params :  tensor([  5.3652, -17.2906])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4189, Loss 2.927662\n",
      "  params :  tensor([  5.3652, -17.2906])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4190, Loss 2.927663\n",
      "  params :  tensor([  5.3652, -17.2906])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4191, Loss 2.927664\n",
      "  params :  tensor([  5.3652, -17.2907])\n",
      "  grad :  tensor([-0.0005,  0.0024])\n",
      "Epoch 4192, Loss 2.927664\n",
      "  params :  tensor([  5.3652, -17.2907])\n",
      "  grad :  tensor([-0.0005,  0.0024])\n",
      "Epoch 4193, Loss 2.927662\n",
      "  params :  tensor([  5.3652, -17.2907])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4194, Loss 2.927663\n",
      "  params :  tensor([  5.3652, -17.2907])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4195, Loss 2.927663\n",
      "  params :  tensor([  5.3652, -17.2908])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4196, Loss 2.927665\n",
      "  params :  tensor([  5.3652, -17.2908])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4197, Loss 2.927664\n",
      "  params :  tensor([  5.3653, -17.2908])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4198, Loss 2.927663\n",
      "  params :  tensor([  5.3653, -17.2908])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4199, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2909])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4200, Loss 2.927664\n",
      "  params :  tensor([  5.3653, -17.2909])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4201, Loss 2.927663\n",
      "  params :  tensor([  5.3653, -17.2909])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4202, Loss 2.927661\n",
      "  params :  tensor([  5.3653, -17.2909])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4203, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2910])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4204, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2910])\n",
      "  grad :  tensor([-0.0004,  0.0024])\n",
      "Epoch 4205, Loss 2.927663\n",
      "  params :  tensor([  5.3653, -17.2910])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4206, Loss 2.927663\n",
      "  params :  tensor([  5.3653, -17.2910])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4207, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2910])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4208, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2911])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4209, Loss 2.927663\n",
      "  params :  tensor([  5.3653, -17.2911])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4210, Loss 2.927664\n",
      "  params :  tensor([  5.3653, -17.2911])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4211, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2911])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4212, Loss 2.927660\n",
      "  params :  tensor([  5.3653, -17.2912])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4213, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2912])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4214, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2912])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4215, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2912])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4216, Loss 2.927661\n",
      "  params :  tensor([  5.3653, -17.2913])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4217, Loss 2.927660\n",
      "  params :  tensor([  5.3653, -17.2913])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4218, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2913])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4219, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2913])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4220, Loss 2.927663\n",
      "  params :  tensor([  5.3653, -17.2913])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4221, Loss 2.927662\n",
      "  params :  tensor([  5.3653, -17.2914])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4222, Loss 2.927662\n",
      "  params :  tensor([  5.3654, -17.2914])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4223, Loss 2.927662\n",
      "  params :  tensor([  5.3654, -17.2914])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4224, Loss 2.927663\n",
      "  params :  tensor([  5.3654, -17.2914])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4225, Loss 2.927660\n",
      "  params :  tensor([  5.3654, -17.2915])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4226, Loss 2.927662\n",
      "  params :  tensor([  5.3654, -17.2915])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4227, Loss 2.927660\n",
      "  params :  tensor([  5.3654, -17.2915])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4228, Loss 2.927661\n",
      "  params :  tensor([  5.3654, -17.2915])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4229, Loss 2.927661\n",
      "  params :  tensor([  5.3654, -17.2915])\n",
      "  grad :  tensor([-0.0004,  0.0023])\n",
      "Epoch 4230, Loss 2.927660\n",
      "  params :  tensor([  5.3654, -17.2916])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4231, Loss 2.927662\n",
      "  params :  tensor([  5.3654, -17.2916])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4232, Loss 2.927662\n",
      "  params :  tensor([  5.3654, -17.2916])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4233, Loss 2.927660\n",
      "  params :  tensor([  5.3654, -17.2916])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4234, Loss 2.927662\n",
      "  params :  tensor([  5.3654, -17.2917])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4235, Loss 2.927661\n",
      "  params :  tensor([  5.3654, -17.2917])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4236, Loss 2.927662\n",
      "  params :  tensor([  5.3654, -17.2917])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4237, Loss 2.927661\n",
      "  params :  tensor([  5.3654, -17.2917])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4238, Loss 2.927660\n",
      "  params :  tensor([  5.3654, -17.2918])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4239, Loss 2.927661\n",
      "  params :  tensor([  5.3654, -17.2918])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4240, Loss 2.927660\n",
      "  params :  tensor([  5.3654, -17.2918])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4241, Loss 2.927662\n",
      "  params :  tensor([  5.3654, -17.2918])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4242, Loss 2.927660\n",
      "  params :  tensor([  5.3654, -17.2918])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4243, Loss 2.927659\n",
      "  params :  tensor([  5.3654, -17.2919])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4244, Loss 2.927661\n",
      "  params :  tensor([  5.3654, -17.2919])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4245, Loss 2.927661\n",
      "  params :  tensor([  5.3654, -17.2919])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4246, Loss 2.927662\n",
      "  params :  tensor([  5.3654, -17.2919])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4247, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2920])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4248, Loss 2.927659\n",
      "  params :  tensor([  5.3655, -17.2920])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4249, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2920])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4250, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2920])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4251, Loss 2.927662\n",
      "  params :  tensor([  5.3655, -17.2920])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4252, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2921])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4253, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2921])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4254, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2921])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4255, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2921])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4256, Loss 2.927660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3655, -17.2921])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4257, Loss 2.927661\n",
      "  params :  tensor([  5.3655, -17.2922])\n",
      "  grad :  tensor([-0.0004,  0.0022])\n",
      "Epoch 4258, Loss 2.927659\n",
      "  params :  tensor([  5.3655, -17.2922])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4259, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2922])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4260, Loss 2.927659\n",
      "  params :  tensor([  5.3655, -17.2922])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4261, Loss 2.927659\n",
      "  params :  tensor([  5.3655, -17.2922])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4262, Loss 2.927662\n",
      "  params :  tensor([  5.3655, -17.2923])\n",
      "  grad :  tensor([-0.0003,  0.0021])\n",
      "Epoch 4263, Loss 2.927658\n",
      "  params :  tensor([  5.3655, -17.2923])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4264, Loss 2.927659\n",
      "  params :  tensor([  5.3655, -17.2923])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4265, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2923])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4266, Loss 2.927659\n",
      "  params :  tensor([  5.3655, -17.2924])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4267, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2924])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4268, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2924])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4269, Loss 2.927659\n",
      "  params :  tensor([  5.3655, -17.2924])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4270, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2924])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4271, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2925])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4272, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2925])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4273, Loss 2.927660\n",
      "  params :  tensor([  5.3655, -17.2925])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4274, Loss 2.927658\n",
      "  params :  tensor([  5.3656, -17.2925])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4275, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2925])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4276, Loss 2.927660\n",
      "  params :  tensor([  5.3656, -17.2926])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4277, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2926])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4278, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2926])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4279, Loss 2.927658\n",
      "  params :  tensor([  5.3656, -17.2926])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4280, Loss 2.927658\n",
      "  params :  tensor([  5.3656, -17.2926])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4281, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2927])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4282, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2927])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4283, Loss 2.927660\n",
      "  params :  tensor([  5.3656, -17.2927])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4284, Loss 2.927660\n",
      "  params :  tensor([  5.3656, -17.2927])\n",
      "  grad :  tensor([-0.0004,  0.0021])\n",
      "Epoch 4285, Loss 2.927658\n",
      "  params :  tensor([  5.3656, -17.2927])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4286, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2928])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4287, Loss 2.927658\n",
      "  params :  tensor([  5.3656, -17.2928])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4288, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2928])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4289, Loss 2.927658\n",
      "  params :  tensor([  5.3656, -17.2928])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4290, Loss 2.927657\n",
      "  params :  tensor([  5.3656, -17.2929])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4291, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2929])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4292, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2929])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4293, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2929])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4294, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2929])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4295, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2930])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4296, Loss 2.927657\n",
      "  params :  tensor([  5.3656, -17.2930])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4297, Loss 2.927657\n",
      "  params :  tensor([  5.3656, -17.2930])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4298, Loss 2.927657\n",
      "  params :  tensor([  5.3656, -17.2930])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4299, Loss 2.927658\n",
      "  params :  tensor([  5.3656, -17.2930])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4300, Loss 2.927659\n",
      "  params :  tensor([  5.3656, -17.2931])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4301, Loss 2.927660\n",
      "  params :  tensor([  5.3657, -17.2931])\n",
      "  grad :  tensor([-0.0004,  0.0020])\n",
      "Epoch 4302, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2931])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4303, Loss 2.927658\n",
      "  params :  tensor([  5.3657, -17.2931])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4304, Loss 2.927658\n",
      "  params :  tensor([  5.3657, -17.2931])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4305, Loss 2.927658\n",
      "  params :  tensor([  5.3657, -17.2932])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4306, Loss 2.927658\n",
      "  params :  tensor([  5.3657, -17.2932])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4307, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2932])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4308, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2932])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4309, Loss 2.927658\n",
      "  params :  tensor([  5.3657, -17.2932])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4310, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2932])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4311, Loss 2.927660\n",
      "  params :  tensor([  5.3657, -17.2933])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4312, Loss 2.927659\n",
      "  params :  tensor([  5.3657, -17.2933])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4313, Loss 2.927658\n",
      "  params :  tensor([  5.3657, -17.2933])\n",
      "  grad :  tensor([-0.0003,  0.0020])\n",
      "Epoch 4314, Loss 2.927656\n",
      "  params :  tensor([  5.3657, -17.2933])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4315, Loss 2.927658\n",
      "  params :  tensor([  5.3657, -17.2933])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4316, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2934])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4317, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2934])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4318, Loss 2.927658\n",
      "  params :  tensor([  5.3657, -17.2934])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4319, Loss 2.927658\n",
      "  params :  tensor([  5.3657, -17.2934])\n",
      "  grad :  tensor([-0.0004,  0.0019])\n",
      "Epoch 4320, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2934])\n",
      "  grad :  tensor([-0.0004,  0.0019])\n",
      "Epoch 4321, Loss 2.927656\n",
      "  params :  tensor([  5.3657, -17.2935])\n",
      "  grad :  tensor([-0.0004,  0.0019])\n",
      "Epoch 4322, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2935])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4323, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2935])\n",
      "  grad :  tensor([-0.0004,  0.0019])\n",
      "Epoch 4324, Loss 2.927658\n",
      "  params :  tensor([  5.3657, -17.2935])\n",
      "  grad :  tensor([-0.0004,  0.0019])\n",
      "Epoch 4325, Loss 2.927658\n",
      "  params :  tensor([  5.3657, -17.2935])\n",
      "  grad :  tensor([-0.0004,  0.0019])\n",
      "Epoch 4326, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2936])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4327, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2936])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4328, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2936])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4329, Loss 2.927656\n",
      "  params :  tensor([  5.3657, -17.2936])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4330, Loss 2.927657\n",
      "  params :  tensor([  5.3657, -17.2936])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4331, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2936])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4332, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2937])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4333, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2937])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4334, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2937])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4335, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2937])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4336, Loss 2.927657\n",
      "  params :  tensor([  5.3658, -17.2937])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4337, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2938])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4338, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2938])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4339, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2938])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4340, Loss 2.927657\n",
      "  params :  tensor([  5.3658, -17.2938])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4341, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2938])\n",
      "  grad :  tensor([-0.0003,  0.0019])\n",
      "Epoch 4342, Loss 2.927657\n",
      "  params :  tensor([  5.3658, -17.2939])\n",
      "  grad :  tensor([-0.0004,  0.0019])\n",
      "Epoch 4343, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2939])\n",
      "  grad :  tensor([-0.0004,  0.0019])\n",
      "Epoch 4344, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2939])\n",
      "  grad :  tensor([-0.0004,  0.0018])\n",
      "Epoch 4345, Loss 2.927657\n",
      "  params :  tensor([  5.3658, -17.2939])\n",
      "  grad :  tensor([-0.0004,  0.0018])\n",
      "Epoch 4346, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2939])\n",
      "  grad :  tensor([-0.0004,  0.0018])\n",
      "Epoch 4347, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2940])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4348, Loss 2.927657\n",
      "  params :  tensor([  5.3658, -17.2940])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4349, Loss 2.927657\n",
      "  params :  tensor([  5.3658, -17.2940])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4350, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2940])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4351, Loss 2.927657\n",
      "  params :  tensor([  5.3658, -17.2940])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4352, Loss 2.927657\n",
      "  params :  tensor([  5.3658, -17.2941])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4353, Loss 2.927657\n",
      "  params :  tensor([  5.3658, -17.2941])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4354, Loss 2.927655\n",
      "  params :  tensor([  5.3658, -17.2941])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4355, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2941])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4356, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2941])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4357, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2941])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4358, Loss 2.927657\n",
      "  params :  tensor([  5.3658, -17.2942])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4359, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2942])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4360, Loss 2.927656\n",
      "  params :  tensor([  5.3658, -17.2942])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4361, Loss 2.927656\n",
      "  params :  tensor([  5.3659, -17.2942])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4362, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2942])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4363, Loss 2.927657\n",
      "  params :  tensor([  5.3659, -17.2942])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4364, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2943])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4365, Loss 2.927656\n",
      "  params :  tensor([  5.3659, -17.2943])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4366, Loss 2.927656\n",
      "  params :  tensor([  5.3659, -17.2943])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4367, Loss 2.927656\n",
      "  params :  tensor([  5.3659, -17.2943])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4368, Loss 2.927654\n",
      "  params :  tensor([  5.3659, -17.2943])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4369, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2943])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4370, Loss 2.927656\n",
      "  params :  tensor([  5.3659, -17.2944])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4371, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2944])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4372, Loss 2.927657\n",
      "  params :  tensor([  5.3659, -17.2944])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4373, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2944])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4374, Loss 2.927657\n",
      "  params :  tensor([  5.3659, -17.2944])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4375, Loss 2.927656\n",
      "  params :  tensor([  5.3659, -17.2945])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4376, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2945])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4377, Loss 2.927654\n",
      "  params :  tensor([  5.3659, -17.2945])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4378, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2945])\n",
      "  grad :  tensor([-0.0003,  0.0018])\n",
      "Epoch 4379, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2945])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4380, Loss 2.927654\n",
      "  params :  tensor([  5.3659, -17.2945])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4381, Loss 2.927656\n",
      "  params :  tensor([  5.3659, -17.2946])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4382, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2946])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4383, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2946])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4384, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2946])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4385, Loss 2.927656\n",
      "  params :  tensor([  5.3659, -17.2946])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4386, Loss 2.927656\n",
      "  params :  tensor([  5.3659, -17.2946])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4387, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2947])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4388, Loss 2.927653\n",
      "  params :  tensor([  5.3659, -17.2947])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4389, Loss 2.927654\n",
      "  params :  tensor([  5.3659, -17.2947])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4390, Loss 2.927654\n",
      "  params :  tensor([  5.3659, -17.2947])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4391, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2947])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4392, Loss 2.927656\n",
      "  params :  tensor([  5.3659, -17.2947])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4393, Loss 2.927655\n",
      "  params :  tensor([  5.3659, -17.2948])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4394, Loss 2.927656\n",
      "  params :  tensor([  5.3660, -17.2948])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4395, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2948])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4396, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2948])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4397, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2948])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4398, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2948])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4399, Loss 2.927656\n",
      "  params :  tensor([  5.3660, -17.2949])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4400, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2949])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4401, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2949])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4402, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2949])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4403, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2949])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4404, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2949])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4405, Loss 2.927656\n",
      "  params :  tensor([  5.3660, -17.2950])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4406, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2950])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4407, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2950])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4408, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2950])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4409, Loss 2.927653\n",
      "  params :  tensor([  5.3660, -17.2950])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4410, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2951])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4411, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2951])\n",
      "  grad :  tensor([-0.0003,  0.0017])\n",
      "Epoch 4412, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2951])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4413, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2951])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4414, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2951])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4415, Loss 2.927653\n",
      "  params :  tensor([  5.3660, -17.2951])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4416, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2952])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4417, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2952])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4418, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2952])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4419, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2952])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4420, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2952])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4421, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2952])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4422, Loss 2.927653\n",
      "  params :  tensor([  5.3660, -17.2953])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4423, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2953])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4424, Loss 2.927653\n",
      "  params :  tensor([  5.3660, -17.2953])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4425, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2953])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4426, Loss 2.927655\n",
      "  params :  tensor([  5.3660, -17.2953])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4427, Loss 2.927654\n",
      "  params :  tensor([  5.3660, -17.2953])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4428, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2953])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4429, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2954])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4430, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2954])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4431, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2954])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4432, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2954])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4433, Loss 2.927655\n",
      "  params :  tensor([  5.3661, -17.2954])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4434, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2954])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4435, Loss 2.927655\n",
      "  params :  tensor([  5.3661, -17.2955])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4436, Loss 2.927652\n",
      "  params :  tensor([  5.3661, -17.2955])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4437, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2955])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4438, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2955])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4439, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2955])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4440, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2955])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4441, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2955])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4442, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2956])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4443, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2956])\n",
      "  grad :  tensor([-0.0002,  0.0016])\n",
      "Epoch 4444, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2956])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4445, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2956])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4446, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2956])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4447, Loss 2.927652\n",
      "  params :  tensor([  5.3661, -17.2956])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4448, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2957])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4449, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2957])\n",
      "  grad :  tensor([-0.0003,  0.0016])\n",
      "Epoch 4450, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2957])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4451, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2957])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4452, Loss 2.927651\n",
      "  params :  tensor([  5.3661, -17.2957])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4453, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2957])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4454, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2957])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4455, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2958])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4456, Loss 2.927655\n",
      "  params :  tensor([  5.3661, -17.2958])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4457, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2958])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4458, Loss 2.927652\n",
      "  params :  tensor([  5.3661, -17.2958])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4459, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2958])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4460, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2958])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4461, Loss 2.927654\n",
      "  params :  tensor([  5.3661, -17.2959])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4462, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2959])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4463, Loss 2.927652\n",
      "  params :  tensor([  5.3661, -17.2959])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4464, Loss 2.927653\n",
      "  params :  tensor([  5.3661, -17.2959])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4465, Loss 2.927654\n",
      "  params :  tensor([  5.3662, -17.2959])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4466, Loss 2.927654\n",
      "  params :  tensor([  5.3662, -17.2959])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4467, Loss 2.927654\n",
      "  params :  tensor([  5.3662, -17.2959])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4468, Loss 2.927652\n",
      "  params :  tensor([  5.3662, -17.2960])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4469, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2960])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4470, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2960])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4471, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2960])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4472, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2960])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4473, Loss 2.927652\n",
      "  params :  tensor([  5.3662, -17.2960])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4474, Loss 2.927652\n",
      "  params :  tensor([  5.3662, -17.2960])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4475, Loss 2.927652\n",
      "  params :  tensor([  5.3662, -17.2961])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4476, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2961])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4477, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2961])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4478, Loss 2.927652\n",
      "  params :  tensor([  5.3662, -17.2961])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4479, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2961])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4480, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2961])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4481, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2962])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4482, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2962])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4483, Loss 2.927654\n",
      "  params :  tensor([  5.3662, -17.2962])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4484, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2962])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4485, Loss 2.927652\n",
      "  params :  tensor([  5.3662, -17.2962])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4486, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2962])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4487, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2962])\n",
      "  grad :  tensor([-0.0003,  0.0015])\n",
      "Epoch 4488, Loss 2.927652\n",
      "  params :  tensor([  5.3662, -17.2963])\n",
      "  grad :  tensor([-0.0003,  0.0014])\n",
      "Epoch 4489, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2963])\n",
      "  grad :  tensor([-0.0003,  0.0014])\n",
      "Epoch 4490, Loss 2.927652\n",
      "  params :  tensor([  5.3662, -17.2963])\n",
      "  grad :  tensor([-0.0003,  0.0014])\n",
      "Epoch 4491, Loss 2.927651\n",
      "  params :  tensor([  5.3662, -17.2963])\n",
      "  grad :  tensor([-0.0003,  0.0014])\n",
      "Epoch 4492, Loss 2.927651\n",
      "  params :  tensor([  5.3662, -17.2963])\n",
      "  grad :  tensor([-0.0003,  0.0014])\n",
      "Epoch 4493, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2963])\n",
      "  grad :  tensor([-0.0003,  0.0014])\n",
      "Epoch 4494, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2964])\n",
      "  grad :  tensor([-0.0003,  0.0014])\n",
      "Epoch 4495, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2964])\n",
      "  grad :  tensor([-0.0003,  0.0014])\n",
      "Epoch 4496, Loss 2.927651\n",
      "  params :  tensor([  5.3662, -17.2964])\n",
      "  grad :  tensor([-0.0003,  0.0014])\n",
      "Epoch 4497, Loss 2.927652\n",
      "  params :  tensor([  5.3662, -17.2964])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4498, Loss 2.927652\n",
      "  params :  tensor([  5.3662, -17.2964])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4499, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2964])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4500, Loss 2.927651\n",
      "  params :  tensor([  5.3662, -17.2964])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4501, Loss 2.927652\n",
      "  params :  tensor([  5.3662, -17.2964])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4502, Loss 2.927653\n",
      "  params :  tensor([  5.3662, -17.2965])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4503, Loss 2.927653\n",
      "  params :  tensor([  5.3663, -17.2965])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4504, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2965])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4505, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2965])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4506, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2965])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4507, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2965])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4508, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2965])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4509, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2966])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4510, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2966])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4511, Loss 2.927650\n",
      "  params :  tensor([  5.3663, -17.2966])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4512, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2966])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4513, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2966])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4514, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2966])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4515, Loss 2.927650\n",
      "  params :  tensor([  5.3663, -17.2966])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4516, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2966])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4517, Loss 2.927653\n",
      "  params :  tensor([  5.3663, -17.2967])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4518, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2967])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4519, Loss 2.927650\n",
      "  params :  tensor([  5.3663, -17.2967])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4520, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2967])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4521, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2967])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4522, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2967])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4523, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2967])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4524, Loss 2.927653\n",
      "  params :  tensor([  5.3663, -17.2968])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4525, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2968])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4526, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2968])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4527, Loss 2.927653\n",
      "  params :  tensor([  5.3663, -17.2968])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4528, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2968])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4529, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2968])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4530, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2968])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4531, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2969])\n",
      "  grad :  tensor([-0.0002,  0.0014])\n",
      "Epoch 4532, Loss 2.927650\n",
      "  params :  tensor([  5.3663, -17.2969])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4533, Loss 2.927652\n",
      "  params :  tensor([  5.3663, -17.2969])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4534, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2969])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4535, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2969])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4536, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2969])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4537, Loss 2.927653\n",
      "  params :  tensor([  5.3663, -17.2969])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4538, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2969])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4539, Loss 2.927650\n",
      "  params :  tensor([  5.3663, -17.2970])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4540, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2970])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4541, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2970])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4542, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2970])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4543, Loss 2.927651\n",
      "  params :  tensor([  5.3663, -17.2970])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4544, Loss 2.927650\n",
      "  params :  tensor([  5.3663, -17.2970])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4545, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2970])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4546, Loss 2.927652\n",
      "  params :  tensor([  5.3664, -17.2971])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4547, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2971])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4548, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2971])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4549, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2971])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4550, Loss 2.927652\n",
      "  params :  tensor([  5.3664, -17.2971])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4551, Loss 2.927652\n",
      "  params :  tensor([  5.3664, -17.2971])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4552, Loss 2.927653\n",
      "  params :  tensor([  5.3664, -17.2971])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4553, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2971])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4554, Loss 2.927652\n",
      "  params :  tensor([  5.3664, -17.2972])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4555, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2972])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4556, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2972])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4557, Loss 2.927652\n",
      "  params :  tensor([  5.3664, -17.2972])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4558, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2972])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4559, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2972])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4560, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2972])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4561, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2973])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4562, Loss 2.927652\n",
      "  params :  tensor([  5.3664, -17.2973])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4563, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2973])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4564, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2973])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4565, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2973])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4566, Loss 2.927652\n",
      "  params :  tensor([  5.3664, -17.2973])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4567, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2973])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4568, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2973])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4569, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2974])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4570, Loss 2.927652\n",
      "  params :  tensor([  5.3664, -17.2974])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4571, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2974])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4572, Loss 2.927649\n",
      "  params :  tensor([  5.3664, -17.2974])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4573, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2974])\n",
      "  grad :  tensor([-0.0002,  0.0013])\n",
      "Epoch 4574, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2974])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4575, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2974])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4576, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2975])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4577, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2975])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4578, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2975])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4579, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2975])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4580, Loss 2.927652\n",
      "  params :  tensor([  5.3664, -17.2975])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4581, Loss 2.927649\n",
      "  params :  tensor([  5.3664, -17.2975])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4582, Loss 2.927653\n",
      "  params :  tensor([  5.3664, -17.2975])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4583, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2975])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4584, Loss 2.927649\n",
      "  params :  tensor([  5.3664, -17.2976])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4585, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2976])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4586, Loss 2.927650\n",
      "  params :  tensor([  5.3664, -17.2976])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4587, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2976])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4588, Loss 2.927651\n",
      "  params :  tensor([  5.3664, -17.2976])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4589, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2976])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4590, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2976])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4591, Loss 2.927652\n",
      "  params :  tensor([  5.3665, -17.2976])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4592, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2976])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4593, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2977])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4594, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2977])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4595, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2977])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4596, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2977])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4597, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2977])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4598, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2977])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4599, Loss 2.927652\n",
      "  params :  tensor([  5.3665, -17.2977])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4600, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2977])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4601, Loss 2.927651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3665, -17.2977])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4602, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2978])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4603, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2978])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4604, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2978])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4605, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2978])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4606, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2978])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4607, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2978])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4608, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2978])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4609, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2978])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4610, Loss 2.927649\n",
      "  params :  tensor([  5.3665, -17.2978])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4611, Loss 2.927649\n",
      "  params :  tensor([  5.3665, -17.2979])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4612, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2979])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4613, Loss 2.927649\n",
      "  params :  tensor([  5.3665, -17.2979])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4614, Loss 2.927649\n",
      "  params :  tensor([  5.3665, -17.2979])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4615, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2979])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4616, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2979])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4617, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2979])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4618, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2979])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4619, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2980])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4620, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2980])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4621, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2980])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4622, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2980])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4623, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2980])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4624, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2980])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4625, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2980])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4626, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2980])\n",
      "  grad :  tensor([-0.0002,  0.0012])\n",
      "Epoch 4627, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2980])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4628, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2981])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4629, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2981])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4630, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2981])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4631, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2981])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4632, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2981])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4633, Loss 2.927651\n",
      "  params :  tensor([  5.3665, -17.2981])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4634, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2981])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4635, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2981])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4636, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2981])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4637, Loss 2.927649\n",
      "  params :  tensor([  5.3665, -17.2982])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4638, Loss 2.927650\n",
      "  params :  tensor([  5.3665, -17.2982])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4639, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2982])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4640, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2982])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4641, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2982])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4642, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2982])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4643, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2982])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4644, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2982])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4645, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2982])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4646, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2983])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4647, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2983])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4648, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2983])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4649, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2983])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4650, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2983])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4651, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2983])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4652, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2983])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4653, Loss 2.927651\n",
      "  params :  tensor([  5.3666, -17.2983])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4654, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2984])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4655, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2984])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4656, Loss 2.927651\n",
      "  params :  tensor([  5.3666, -17.2984])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4657, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2984])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4658, Loss 2.927651\n",
      "  params :  tensor([  5.3666, -17.2984])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4659, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2984])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4660, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2984])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4661, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2984])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4662, Loss 2.927648\n",
      "  params :  tensor([  5.3666, -17.2984])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4663, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2985])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4664, Loss 2.927648\n",
      "  params :  tensor([  5.3666, -17.2985])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4665, Loss 2.927648\n",
      "  params :  tensor([  5.3666, -17.2985])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4666, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2985])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4667, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2985])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4668, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2985])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4669, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2985])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4670, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2985])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4671, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2985])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4672, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2986])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4673, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2986])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4674, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2986])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4675, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2986])\n",
      "  grad :  tensor([-0.0002,  0.0011])\n",
      "Epoch 4676, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2986])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4677, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2986])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4678, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2986])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4679, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2986])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4680, Loss 2.927648\n",
      "  params :  tensor([  5.3666, -17.2986])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4681, Loss 2.927648\n",
      "  params :  tensor([  5.3666, -17.2987])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4682, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2987])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4683, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2987])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4684, Loss 2.927648\n",
      "  params :  tensor([  5.3666, -17.2987])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4685, Loss 2.927650\n",
      "  params :  tensor([  5.3666, -17.2987])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4686, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2987])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4687, Loss 2.927651\n",
      "  params :  tensor([  5.3666, -17.2987])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4688, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2987])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4689, Loss 2.927649\n",
      "  params :  tensor([  5.3666, -17.2987])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4690, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2987])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4691, Loss 2.927650\n",
      "  params :  tensor([  5.3667, -17.2987])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4692, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2988])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4693, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2988])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4694, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2988])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4695, Loss 2.927650\n",
      "  params :  tensor([  5.3667, -17.2988])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4696, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2988])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4697, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2988])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4698, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2988])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4699, Loss 2.927650\n",
      "  params :  tensor([  5.3667, -17.2988])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4700, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2988])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4701, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2988])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4702, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2989])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4703, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2989])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4704, Loss 2.927647\n",
      "  params :  tensor([  5.3667, -17.2989])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4705, Loss 2.927650\n",
      "  params :  tensor([  5.3667, -17.2989])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4706, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2989])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4707, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2989])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4708, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2989])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4709, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2989])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4710, Loss 2.927650\n",
      "  params :  tensor([  5.3667, -17.2989])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4711, Loss 2.927650\n",
      "  params :  tensor([  5.3667, -17.2989])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4712, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2989])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4713, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2990])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4714, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2990])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4715, Loss 2.927650\n",
      "  params :  tensor([  5.3667, -17.2990])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4716, Loss 2.927647\n",
      "  params :  tensor([  5.3667, -17.2990])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4717, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2990])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4718, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2990])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4719, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2990])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4720, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2990])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4721, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2990])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4722, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2990])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4723, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2991])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4724, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2991])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4725, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2991])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4726, Loss 2.927650\n",
      "  params :  tensor([  5.3667, -17.2991])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4727, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2991])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4728, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2991])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4729, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2991])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4730, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2991])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4731, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2991])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4732, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2991])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4733, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2991])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4734, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2992])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4735, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2992])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4736, Loss 2.927650\n",
      "  params :  tensor([  5.3667, -17.2992])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4737, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2992])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4738, Loss 2.927650\n",
      "  params :  tensor([  5.3667, -17.2992])\n",
      "  grad :  tensor([-0.0002,  0.0010])\n",
      "Epoch 4739, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2992])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4740, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2992])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4741, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2992])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4742, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2992])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4743, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2992])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4744, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2993])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4745, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2993])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4746, Loss 2.927648\n",
      "  params :  tensor([  5.3667, -17.2993])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4747, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2993])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4748, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2993])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4749, Loss 2.927649\n",
      "  params :  tensor([  5.3667, -17.2993])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4750, Loss 2.927650\n",
      "  params :  tensor([  5.3668, -17.2993])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4751, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2993])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4752, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2993])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4753, Loss 2.927647\n",
      "  params :  tensor([  5.3668, -17.2993])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4754, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2993])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4755, Loss 2.927647\n",
      "  params :  tensor([  5.3668, -17.2994])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4756, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2994])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4757, Loss 2.927647\n",
      "  params :  tensor([  5.3668, -17.2994])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4758, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2994])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4759, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2994])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4760, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2994])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4761, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2994])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4762, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2994])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4763, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2994])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4764, Loss 2.927647\n",
      "  params :  tensor([  5.3668, -17.2994])\n",
      "  grad :  tensor([-0.0001,  0.0009])\n",
      "Epoch 4765, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2995])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4766, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2995])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4767, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2995])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4768, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2995])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4769, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2995])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4770, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2995])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4771, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2995])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4772, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2995])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4773, Loss 2.927650\n",
      "  params :  tensor([  5.3668, -17.2995])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4774, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2995])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4775, Loss 2.927650\n",
      "  params :  tensor([  5.3668, -17.2995])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4776, Loss 2.927647\n",
      "  params :  tensor([  5.3668, -17.2996])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4777, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2996])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4778, Loss 2.927647\n",
      "  params :  tensor([  5.3668, -17.2996])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4779, Loss 2.927650\n",
      "  params :  tensor([  5.3668, -17.2996])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4780, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2996])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4781, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2996])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4782, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2996])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4783, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2996])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4784, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2996])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4785, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2996])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4786, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4787, Loss 2.927650\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4788, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4789, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4790, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4791, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4792, Loss 2.927647\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4793, Loss 2.927650\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4794, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0001,  0.0009])\n",
      "Epoch 4795, Loss 2.927650\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4796, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4797, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2997])\n",
      "  grad :  tensor([-0.0002,  0.0009])\n",
      "Epoch 4798, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0009])\n",
      "Epoch 4799, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0009])\n",
      "Epoch 4800, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0009])\n",
      "Epoch 4801, Loss 2.927646\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0009])\n",
      "Epoch 4802, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0009])\n",
      "Epoch 4803, Loss 2.927647\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0009])\n",
      "Epoch 4804, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4805, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4806, Loss 2.927647\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4807, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4808, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4809, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4810, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2998])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4811, Loss 2.927648\n",
      "  params :  tensor([  5.3668, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4812, Loss 2.927649\n",
      "  params :  tensor([  5.3668, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4813, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4814, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4815, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4816, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4817, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4818, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4819, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4820, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4821, Loss 2.927649\n",
      "  params :  tensor([  5.3669, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4822, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4823, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.2999])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4824, Loss 2.927649\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4825, Loss 2.927649\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4826, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4827, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4828, Loss 2.927649\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4829, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4830, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4831, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4832, Loss 2.927649\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4833, Loss 2.927646\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4834, Loss 2.927649\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4835, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4836, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3000])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4837, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4838, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4839, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4840, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4841, Loss 2.927650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4842, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4843, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4844, Loss 2.927649\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4845, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4846, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4847, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4848, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4849, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3001])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4850, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4851, Loss 2.927649\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4852, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4853, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4854, Loss 2.927649\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4855, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4856, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4857, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4858, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4859, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4860, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4861, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4862, Loss 2.927645\n",
      "  params :  tensor([  5.3669, -17.3002])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4863, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4864, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4865, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4866, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4867, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4868, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4869, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4870, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4871, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4872, Loss 2.927646\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4873, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4874, Loss 2.927649\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4875, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3003])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4876, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4877, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4878, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4879, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4880, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4881, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0008])\n",
      "Epoch 4882, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4883, Loss 2.927648\n",
      "  params :  tensor([  5.3669, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4884, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4885, Loss 2.927647\n",
      "  params :  tensor([  5.3669, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4886, Loss 2.927649\n",
      "  params :  tensor([  5.3669, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4887, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4888, Loss 2.927649\n",
      "  params :  tensor([  5.3670, -17.3004])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4889, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4890, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4891, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4892, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4893, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4894, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4895, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4896, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4897, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4898, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4899, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4900, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4901, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3005])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4902, Loss 2.927649\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4903, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4904, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4905, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4906, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4907, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4908, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4909, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4910, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4911, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4912, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4913, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4914, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4915, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3006])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4916, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4917, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4918, Loss 2.927649\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4919, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4920, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4921, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4922, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4923, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4924, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4925, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4926, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4927, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4928, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3007])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4929, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4930, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4931, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4932, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4933, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4934, Loss 2.927649\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4935, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4936, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4937, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4938, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4939, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4940, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4941, Loss 2.927649\n",
      "  params :  tensor([  5.3670, -17.3008])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4942, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4943, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4944, Loss 2.927649\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4945, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4946, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4947, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4948, Loss 2.927649\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4949, Loss 2.927646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4950, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-9.9361e-05,  6.6355e-04])\n",
      "Epoch 4951, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-9.5367e-05,  6.6292e-04])\n",
      "Epoch 4952, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-9.6858e-05,  6.6188e-04])\n",
      "Epoch 4953, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4954, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4955, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4956, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4957, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4958, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3009])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4959, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3010])\n",
      "  grad :  tensor([-9.8228e-05,  6.5479e-04])\n",
      "Epoch 4960, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4961, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3010])\n",
      "  grad :  tensor([-9.8288e-05,  6.5270e-04])\n",
      "Epoch 4962, Loss 2.927646\n",
      "  params :  tensor([  5.3670, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0007])\n",
      "Epoch 4963, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4964, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4965, Loss 2.927647\n",
      "  params :  tensor([  5.3670, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4966, Loss 2.927648\n",
      "  params :  tensor([  5.3670, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4967, Loss 2.927646\n",
      "  params :  tensor([  5.3671, -17.3010])\n",
      "  grad :  tensor([-9.7990e-05,  6.4683e-04])\n",
      "Epoch 4968, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3010])\n",
      "  grad :  tensor([-9.7573e-05,  6.4588e-04])\n",
      "Epoch 4969, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4970, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4971, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4972, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4973, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4974, Loss 2.927646\n",
      "  params :  tensor([  5.3671, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4975, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3010])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4976, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4977, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-9.5606e-05,  6.3694e-04])\n",
      "Epoch 4978, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-9.8169e-05,  6.3545e-04])\n",
      "Epoch 4979, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4980, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4981, Loss 2.927646\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4982, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4983, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4984, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4985, Loss 2.927646\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4986, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-9.6440e-05,  6.2808e-04])\n",
      "Epoch 4987, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4988, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4989, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4990, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4991, Loss 2.927646\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4992, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4993, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3011])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4994, Loss 2.927646\n",
      "  params :  tensor([  5.3671, -17.3012])\n",
      "  grad :  tensor([-9.2626e-05,  6.2042e-04])\n",
      "Epoch 4995, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3012])\n",
      "  grad :  tensor([-9.8884e-05,  6.1837e-04])\n",
      "Epoch 4996, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3012])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4997, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3012])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4998, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3012])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 4999, Loss 2.927647\n",
      "  params :  tensor([  5.3671, -17.3012])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n",
      "Epoch 5000, Loss 2.927648\n",
      "  params :  tensor([  5.3671, -17.3012])\n",
      "  grad :  tensor([-0.0001,  0.0006])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于 t_p = w* t_u + b ,并且我们可以发现 对于grad 的权重比例 W的梯度是 b的梯度下降的50 倍，这表示w 和 b 不在同一比例空间之中。\n",
    "# 我们可以对t_u 进行归一化输入，来降低w 所占的梯度比例。（如果学习参数有明显的比例区别的话，参数输入会对这个参数十分敏感，对其他的不敏感）\n",
    "#因为现在是线性训练，W可以乘以系数成回去，但是b 基本保持不变，可以理解为下方行数训练而来。\n",
    "#t_p = w*（0.1*t_u）+b\n",
    "params = torch.tensor([1.0, 0.0])\n",
    "t_un = 0.1 * t_u\n",
    "training_loop( \n",
    "    n_epochs = 5000, \n",
    "    learning_rate = 1e-2, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_un, \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25d8adfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18a49ac5da0>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. 进行可视化数据\n",
    "%matplotlib inline \n",
    "params = torch.tensor([  5.3671, -17.3012]) #采用训练之后的数据\n",
    "from matplotlib import pyplot as plt\n",
    "t_p = model(t_un, *params)  # 进行一次权重计算\n",
    "fig = plt.figure(dpi=300) \n",
    "plt.xlabel(\"Temperature (°Fahrenheit)\") \n",
    "plt.ylabel(\"Temperature (°Celsius)\") \n",
    "#绘制一条曲线，X轴数据是t_u（转换为NumPy数组），Y轴数据是t_p（转换为NumPy数组）。\n",
    "#t_p.detach().numpy()将PyTorch张量t_p从计算图中分离并转换为NumPy数组，以便用于绘图。\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy()) \n",
    "\n",
    "#绘制散点图，X轴数据是t_u（转换为NumPy数组），Y轴数据是t_c（转换为NumPy数组）。\n",
    "#'o'表示使用圆圈作为标记。\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843af976",
   "metadata": {},
   "source": [
    "**上面已经完成了一次线性模型的训练**，其中我们是自己手写了 求导、梯度下降和反向传播、输入归一化处理、模型更新、等内容。\n",
    "其实，pytorch已经为我们提供了反向传播的接口了，我们可以利用pytorch为我们做这部分工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ab5a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4517.2969,   82.6000])\n"
     ]
    }
   ],
   "source": [
    "def model(t_u, w, b): \n",
    "    return w * t_u + b \n",
    "\n",
    "def loss_fn(t_p, t_c): \n",
    "    squared_diffs = (t_p - t_c)**2 \n",
    "    return squared_diffs.mean() \n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "params.grad is None \n",
    "\n",
    "loss = loss_fn(model(t_u,*params), t_c)\n",
    "loss.backward()\n",
    "print(params.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e07b72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c): \n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "            \n",
    "        t_p = model(t_u,*params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        \n",
    "#在这个上下文中，所有的操作都不会被 PyTorch 的自动微分引擎跟踪，因此不会计算或存储梯度。\n",
    "#这通常用于模型推理或手动更新参数时，避免不必要的梯度计算和内存消耗\n",
    "#         with torch.no_grad():\n",
    "#             params = params - learning_rate * params.grad  不能使用这种写法，这种写法会再开辟一个变量，并且导致梯度失败\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            params -= learning_rate * params.grad # 在本地修改\n",
    " \n",
    "        if epoch % 500 == 0: \n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss))) \n",
    " \n",
    "    return params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c97e010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0], requires_grad=True),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9b4bab46",
   "metadata": {},
   "source": [
    "**优化器** 上面我们相当于自己写了一个梯度下降的优化器，接下来我们将用pytorch的优化器来更新参数。根据下图一个模型优化过程是四个步骤：\n",
    "1. 定义模型\n",
    "2. 模型计算损失\n",
    "3. 损失进行backword\n",
    "4. 优化器根据backword 更新参数 params\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd9ed6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim \n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True) \n",
    "learning_rate = 1e-5 \n",
    "optimizer = optim.SGD([params],lr=learning_rate) # 优化器输入是参数和学习率\n",
    "\n",
    "# 利用优化器进行一次计算\n",
    "t_p = model(t_u, *params) \n",
    "loss = loss_fn(t_p, t_c) \n",
    "loss.backward()\n",
    "optimizer.step() \n",
    "print(params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "457c5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c): \n",
    "    for epochs in range(1,n_epochs+1):\n",
    "        t_p = model(t_u, *params) \n",
    "        loss = loss_fn(t_p, t_c) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  # 会自动更新params\n",
    "        \n",
    "        if(epochs % 500) == 0:\n",
    "            print(\"epochs %d,loss: %f\" %(epochs,loss))\n",
    "        \n",
    "    return params        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5523552d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 500,loss: 7.860115\n",
      "epochs 1000,loss: 3.828538\n",
      "epochs 1500,loss: 3.092191\n",
      "epochs 2000,loss: 2.957698\n",
      "epochs 2500,loss: 2.933134\n",
      "epochs 3000,loss: 2.928648\n",
      "epochs 3500,loss: 2.927830\n",
      "epochs 4000,loss: 2.927679\n",
      "epochs 4500,loss: 2.927652\n",
      "epochs 5000,loss: 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "\n",
    "#optimizer = optim.Adam([params],lr=learning_rate)\n",
    "optimizer = optim.SGD([params],lr=learning_rate)\n",
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a7973",
   "metadata": {},
   "source": [
    " **数据集和验证集：** 下面我们将继续讨论对于已有数据拆分为数据集和验证集来处理方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "09ca88a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  9, 10,  8,  1,  5,  7,  3,  6]) tensor([4, 2])\n"
     ]
    }
   ],
   "source": [
    "n_samples = t_u.shape[0] #11\n",
    "n_val = int(0.2 * n_samples) # 20%\n",
    "shuffled_indices = torch.randperm(n_samples) # randperm() 重新排列\n",
    "train_indices = shuffled_indices[:-n_val] \n",
    "val_indices = shuffled_indices[-n_val:] \n",
    "print(train_indices,val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0cbb36e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices] \n",
    "train_t_c = t_c[train_indices] \n",
    "\n",
    "val_t_u = t_u[val_indices] \n",
    "val_t_c = t_c[val_indices] \n",
    "train_t_un = 0.1 * train_t_u \n",
    "val_t_un = 0.1 * val_t_u \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36743dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, \n",
    "                  train_t_c, val_t_c): \n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        train_t_p = model(train_t_u, *params)  \n",
    "        train_loss = loss_fn(train_t_p, train_t_c) \n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            val_t_p = model(val_t_u, *params) # 这里会记录计算图？->利用torch.no_grad() 来消除影响\n",
    "            val_loss = loss_fn(val_t_p, val_t_c) \n",
    "            assert val_loss.requires_grad == False \n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward() \n",
    "        optimizer.step() \n",
    "        \n",
    "        if(epochs % 500) == 0:\n",
    "            print(\"epochs %d,loss: %f\" %(epochs,loss))\n",
    "    return params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bac899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
